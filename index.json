
[{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":" Docker 基础 # 配置镜像加速 # # 编辑 Docker 配置文件 $ sudo vim /etc/docker/daemon.json # 加入以下配置项 { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://dockerproxy.com\u0026#34;, \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://mirror.baidubce.com\u0026#34;, \u0026#34;https://ccr.ccs.tencentyun.com\u0026#34; ] } # 查看 Docker 信息 $ sudo docker info # 出现以下字段代表配置成功 Registry Mirrors: https://dockerproxy.com/ https://hub-mirror.c.163.com/ https://mirror.baidubce.com/ https://ccr.ccs.tencentyun.com/ 0.全局指令 # 1.login 、logout 登陆、登出 # **-u :**登陆的用户名 **-p :**登陆的密码 2.pull、push 拉取、上传 # 3.ps 列出容器 # **-a :**显示所有的容器，包括未运行的 **-f :**根据条件过滤显示的内容 **\u0026ndash;format :**指定返回值的模板文件 **-l :**显示最近创建的容器 **-n :**列出最近创建的n个容器 **\u0026ndash;no-trunc :**不截断输出 **-q :**静默模式，只显示容器编号 **-s :**显示总的文件大小 不常用指令 # 1）search 查找 # docker search -f stars=10 java 查找所有镜像名包含 java，并且收藏数大于 10 的镜像 2）info 显示 Docker 系统信息，包括镜像和容器数 version 版本信息 # 3）stats 显示容器资源的使用情况 # docker stats [OPTIONS] [CONTAINER...]\t# CPU、内存、网络 I/O 等 **\u0026ndash;all , -a :**显示所有的容器，包括未运行的。 **\u0026ndash;format :**指定返回值的模板文件。 **\u0026ndash;no-stream :**展示当前状态就直接退出了，不再实时更新。 **\u0026ndash;no-trunc :**不截断输出。 1.镜像操作 # 1.images # **-a:**列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层） **\u0026ndash;digests :**显示镜像的摘要信息 **-f :**显示满足条件的镜像 **\u0026ndash;format :**指定返回值的模板文件 **\u0026ndash;no-trunc :**显示完整的镜像信息 **-q :**只显示镜像ID 2.rmi 删除本地一个或多个镜像 # **-f :**强制删除 **\u0026ndash;no-prune :**不移除该镜像的过程镜像，默认移除 3.build 使用 Dockerfile 创建镜像 # **\u0026ndash;build-arg=[] :**设置镜像创建时的变量； **\u0026ndash;cpu-shares :**设置 cpu 使用权重； **\u0026ndash;cpu-period :**限制 CPU CFS周期； **\u0026ndash;cpu-quota :**限制 CPU CFS配额； **\u0026ndash;cpuset-cpus :**指定使用的CPU id； **\u0026ndash;cpuset-mems :**指定使用的内存 id； **\u0026ndash;disable-content-trust :**忽略校验，默认开启； **-f :**指定要使用的Dockerfile路径； **\u0026ndash;force-rm :**设置镜像过程中删除中间容器； **\u0026ndash;isolation :**使用容器隔离技术； **\u0026ndash;label=[] :**设置镜像使用的元数据； **-m :**设置内存最大值； **\u0026ndash;memory-swap :**设置Swap的最大值为内存+swap，\u0026quot;-1\u0026quot;表示不限swap； **\u0026ndash;no-cache :**创建镜像的过程不使用缓存； **\u0026ndash;pull :**尝试去更新镜像的新版本； **\u0026ndash;quiet, -q :**安静模式，成功后只输出镜像 ID； **\u0026ndash;rm :**设置镜像成功后删除中间容器； **\u0026ndash;shm-size :**设置/dev/shm的大小，默认值是64M； **\u0026ndash;ulimit :**Ulimit配置。 **\u0026ndash;squash :**将 Dockerfile 中所有的操作压缩为一层。 \u0026ndash;tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。\u0026lt; \u0026ndash;network: 默认 default。在构建期间设置RUN指令的网络模式 docker build -t runoob/ubuntu:v1 . 不常用指令 # 1.tag 标记本地镜像，将其归入某一仓库 # docker tag [OPTIONS] IMAGE[:TAG] [REGISTRYHOST/][USERNAME/]NAME[:TAG] docker tag ubuntu:15.10 runoob/ubuntu:v3\t# 将镜像ubuntu:15.10标记为 runoob/ubuntu:v3 镜像 2.history 查看指定镜像的创建历史 # **-H :**以可读的格式打印镜像大小和日期，默认为true **\u0026ndash;no-trunc :**显示完整的提交记录 **-q :**仅列出提交记录ID 3.save、load 将指定镜像保存成 tar 归档文件、导出为镜像 # docker save -o A.tar A:v3 # 将镜像 A:v3 生成A.tar 文档 docker load --i A.tar\t# 导入镜像A 4.import 从归档文件中创建镜像 # **-c :**应用docker 指令创建镜像 **-m :**提交时的说明文字 docker import A.tar B:v4 # 从镜像归档文件A.tar创建镜像，命名为B:v4 2.容器操作 # 1.create 创建一个新的容器但不启动它 # 2.run 创建一个新的容器并运行一个命令 # docker run [OPTIONS] IMAGE [COMMAND] [ARG...] \u0026ndash;name=\u0026ldquo;nginx-lb\u0026rdquo;: 为容器指定一个名称； -d: 后台运行容器，并返回容器ID； -i: 以交互模式运行容器，通常与 -t 同时使用； -t: 为容器重新分配一个伪输入终端，通常与 -i 同时使用； -P: 随机端口映射，容器内部端口随机映射到主机的端口 -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 \u0026ndash;volume , -v: 绑定一个卷 \u0026ndash;link=[]: 添加链接到另一个容器 \u0026ndash;net=\u0026ldquo;bridge\u0026rdquo;: 指定容器的网络连接类型，支持 bridge/host/none/container: 四种类型； **-m :**设置容器使用内存最大值； \u0026ndash;dns 8.8.8.8: 指定容器使用的DNS服务器，默认和宿主一致； \u0026ndash;dns-search example.com: 指定容器DNS搜索域名，默认和宿主一致； -h \u0026ldquo;mars\u0026rdquo;: 指定容器的hostname； -e username=\u0026ldquo;ritchie\u0026rdquo;: 设置环境变量； \u0026ndash;env-file=[]: 从指定文件读入环境变量； \u0026ndash;cpuset=\u0026ldquo;0-2\u0026rdquo; or \u0026ndash;cpuset=\u0026ldquo;0,1,2\u0026rdquo;: 绑定容器到指定CPU运行； -a stdin: 指定标准输入输出内容类型，可选 STDIN/STDOUT/STDERR 三项； \u0026ndash;expose=[]: 开放一个端口或一组端口； docker run --name mynginx -d nginx:latest docker run -p 80:80 -v /data:/data -d nginx:latest docker run -it nginx:latest /bin/bash 3.start、stop、restart、kill 启动、停止、重启、杀死运行中容器 # 4.rm 删除一个或多个容器 # **-f :**通过 SIGKILL 信号强制删除一个运行中的容器 **-l :**移除容器间的网络连接，而非容器本身 **-v :**同时删除与容器关联的卷 5.exec 在运行的容器中执行命令 # **-d :**分离模式: 在后台运行 **-i :**即使没有附加也保持STDIN 打开 **-t :**分配一个伪终端 docker exec [OPTIONS] CONTAINER COMMAND [ARG...] docker exec -it mynginx /bin/bash\t# 进入交互模式bash 6.commit 从容器创建一个新的镜像 # **-a :**提交的镜像作者 **-c :**使用Dockerfile指令来创建镜像 **-m :**提交时的说明文字 **-p :**在commit时，将容器暂停 7.cp 容器与主机之间的数据拷贝 # docker cp [-L] 源 目标\t# /www/runoob\t96f7f14e99ab:/www/ 8.inspect 获取容器/镜像的元数据 # **-f :**指定返回值的模板文件 **-s :**显示总的文件大小 **\u0026ndash;type :**为指定类型返回JSON 9.top 查看容器中运行的进程信息，支持 ps 命令参数 # 10.attach 连接到正在运行中的容器 # 11.logs 容器日志 # -f : 跟踪日志输出 **\u0026ndash;since :**显示某个开始时间的所有日志 -t : 显示时间戳 **\u0026ndash;tail :**仅列出最新N条容器日志 12.port 列出指定的容器的端口映射 # 不常用指令 # 1.pause、unpause 暂停、恢复一个或多个容器的所有进程 # 2.events 从服务器获取实时事件 # **-f ：**根据条件过滤事件 **\u0026ndash;since ：**从指定的时间戳后显示所有事件 **\u0026ndash;until ：**流水时间显示到指定的时间为止 3.wait 阻塞运行直到容器停止，然后打印出它的退出代码 # 4.export 将文件系统作为一个tar归档文件导出到STDOUT # **-o :**将输入内容写到文件 3.容器网络 # 1.docker network create 创建网络： # docker network create \u0026lt;network_name\u0026gt; 2.docker network ls 列出网络 # 3.network inspect 查看网络详情： # docker network inspect \u0026lt;network_name\u0026gt; 4.network connect 连接容器到网络： # docker network connect \u0026lt;network_name\u0026gt; \u0026lt;container_name\u0026gt; 5.network disconnect 断开容器与网络的连接： # docker network disconnect \u0026lt;network_name\u0026gt; \u0026lt;container_name\u0026gt; 6.network rm 删除网络： # docker network rm \u0026lt;network_name\u0026gt; 4.数据卷 # docker volume prune 删除未使用的所有数据卷 # 5.Dockerfile命令 # FROM # 指定基础镜像\nENV # 设置环境变量：ENV key value\nCOPY # 拷贝本地文件到镜像的指定目录\nRUN # 执行shell指令\nEXPOSE # 指定监听端口，给使用者看\nENTERPOINT # 镜像的启动命令，容器运行时调用\n6.DockerCompose # 基于Compose文件快速部署分布式应用，无需手动创建一个个创建和运行容器，会自动创建docker网络\n安装后编写 docker-compose.yaml 文件：docker-compose up -d\nversion: \u0026#39;3\u0026#39; services: nacos: image: nacos/naco-server evironment: MODE: standalone ports: - \u0026#34;8848:8848\u0026#34; mysql: image: mysql:8 container_name: mysql8 restart: always volumes: - \u0026#34;$PWD/mysql/data:/var/lib/mysql\u0026#34; - \u0026#34;$PWD/mysql/conf:/etc/mysql/conf.d/\u0026#34; environment: - MYSQL_ROOT_PASSWORD: 123456 - key2: value2 ports: - \u0026#34;3306:3306\u0026#34; userservice: build: ./user-service orderservice: build: ./order-service gateway: build: ./gateway ports: - \u0026#34;10010:10010\u0026#34; Docker 实战 # 1.记录第一次部署 nginx 服务器-403 # 文件结构 Docker ----nginx ---------global.conf ---------nginx.conf ----website ---------index.html ----Dockerfile 各文件内容\nnginx.conf\nuser www-data; worker_processes 4; pid /run/nginx.pid; daemon off; events { } http { sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 2048; include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/access.log; error_log /var/log/nginx/error.log; gzip on; gzip_disable \u0026#34;msie6\u0026#34;; include /etc/nginx/conf.d/*.conf; } global.conf\nserver { listen 0.0.0.0:80; server_name test; location / { root /var/www/html/website; index index.html index.htm; } access_log /var/log/nginx/default_access.log; error_log /var/log/nginx/default_error.log; } Dockerfile\nFrom ubuntu:18.04 LABEL name=\u0026#34;TCmatj\u0026#34; ENV REFRESHED_AT 2021-02-01 RUN apt-get -yqq update \u0026amp;\u0026amp; apt-get -yqq install nginx RUN mkdir -p /var/www/html/website ADD nginx/global.conf /etc/nginx/conf.d/ ADD nginx/nginx.conf /etc/nginx/nginx.conf EXPOSE 80 过程：\n构建镜像\ndocker build -t nginx . . 表示在当前目录寻找 Dockerfile -t: 镜像的名字及标签\n运行容器\ndocker run -d -p 6131:80 \\ --name website --privileged=true \\ -v D:/code_py/Docker/website:/var/www/html/website nginx nginx -p 绑定端口，-v 挂载目录，-d 后台运行\n倒数第二个为镜像名称，倒数第一个为执行的指令\n打开界面\n浏览器输入localhost:6131\n报错403，解决思路\n一开始以为是权限问题，捣鼓半天权限，更换nginx.conf 的 user 为 root，保证主进程和四个工作进程都是 root 用户，但是并没有解决 最后发现挂载的目录不正确，需要使用 / 而不是 \\。写错的路径不会正确挂载到 var 之下，会变为一个新的目录\\var\\www\\html\\website ","externalUrl":null,"permalink":"/docs/%E5%B7%A5%E5%85%B7/docker/","section":"Docs","summary":"Docker 基础 # 配置镜像加速 # # 编辑 Docker 配置文件 $ sudo vim /etc/docker/daemon.json # 加入以下配置","title":"Docker","type":"docs"},{"content":"","externalUrl":null,"permalink":"/docs/","section":"Docs","summary":"","title":"Docs","type":"docs"},{"content":"分布式版本控制系统\n1. 命令 # 1.1 安装后配置 # git -v 查看git版本\ngit config [--global/system] user.name \u0026quot;TCmatj\u0026quot; 配置用户名\n缺省: 本地配置，只对本地仓库有效\n--global: 全局配置，对所有仓库生效\n--system: 系统配置，对所有用户生效\ngit config --global user.email tcmatj@gmail.com 配置邮箱\ngit config -- global credential.helper store 保存用户名和密码\ngit config --global --list 查看配置信息\n1.2 新建仓库 # git init 在当前目录创建一个仓库 git clone 仓库地址 克隆远程仓库 git status 查看仓库状态 1.3 文件提交 # git add . 将所有未跟踪【Untracked】的文件添加到暂存区\n*.txt 所有以.txt 结尾的文件\ngit rm --cached \u0026lt;file\u0026gt; ... 将暂存的文件拿回来\ngit commit -m [注释] 将暂存区文件提交到本地仓库\ngit log [--oneline] 查看历史提交记录，oneline 简洁的提交记录\ngit reflog 查看操作历史记录，可以使用误操作之前的版本号回退\n1.4 git reset 版本回退 # git reset --soft \u0026lt;版本号\u0026gt; 保存工作区（ls 显示的当前目录文件）和暂存区内容\ngit reset --hard \u0026lt;版本号\u0026gt; 丢弃工作区和暂存区内容\ngit reset [--mixed] \u0026lt;版本号\u0026gt; 默认参数。保存工作区，丢弃暂存区内容\n1.5 git diff 查看差异 # git diff 查看工作区、暂存区之间的差异\ngit diff HEAD 查看工作区、仓库之间的差异\ngit diff --cached 查看暂存区、仓库之间的差异\ngit diff \u0026lt;版本1\u0026gt; \u0026lt;版本2\u0026gt; 比较版本之间的差异，可在后面加上某个文件，只比这个文件差异\nHEAD~ HEAD^ HEAD~1 表示上一个版本 HEAD 表示当前版本\n1.6 git rm 删除文件 # 删除文件后提交\ngit rm \u0026lt;file\u0026gt; 把文件从工作区和暂存区同时删除，也需压再进行提交\n1.7 SSH配置、关联本地仓库 # 生成SSH密钥 ssh-keygen -t rsa -b 4096 会生成 用户目录/.ssh/id_rsa 密钥文件\n没有 .pub 拓展名的文件是私钥文件，有的是公钥文件。将公钥文件复制到 Github 里\ngit remote add origin \u0026lt;远程仓库地址\u0026gt; origin 是远程仓库的别名，可以自己指定\ngit remote -v 查看远程仓库的别名和地址\ngit branch -M main 指定分支名称为 main\ngit push -u origin main[:main] 将本地仓库的main与远程仓库main关联\ngit pull \u0026lt;远程仓库名\u0026gt; \u0026lt;远程分支名\u0026gt;:\u0026lt;本地分支名[相同可以省略]\u0026gt; 拉取远程仓库\n1.8 分支操作 # git branch 查看所有分支，带 * 的是当前分支\ngit branch \u0026lt;新分支\u0026gt; 创建新分支\ngit branch -d \u0026lt;分支名称\u0026gt; 删除分支（已被合并的分支）\ngit branch -D \u0026lt;分支名称\u0026gt; 强制删除分支（可以删除未合并分支）\ngit checkout \u0026lt;分支名称\u0026gt; git switch \u0026lt;分支名称\u0026gt; 切换分支\ngit merge \u0026lt;分支名称\u0026gt; 将分支合并到当前分支\ngit log --graph --oneline --decorate --all 显示分支图\n1.9 解决冲突 # merge 发生冲突后：\ngit merge --abort 终止合并\ngit status 查看冲突文件列表\ngit diff 查看冲突文件内容\n解决冲突：\nvim 冲突文件 修改为目标文件\ngit add .\ngit commit -m \u0026lt;注释\u0026gt;\n1.10 Rebase # git switch dev 切换到dev分支\ngit rebase main 变基到main：将公共祖先开始的dev分支的链添加在main的后面\nmerge：不会破坏原分支的提交历史，方便回溯和查看；会产生额外的提交节点\nrebase：不会新增额外的提交历史；会改变提交历史\n1.11 工作流 # GitFlow：\ngit tag 可以添加版本号\nmain/master 分支： 项目的核心分支，最新稳定版本的代码，保证代码是可发布的。不允许直接修改，只允许merge\nhotfix 分支：线上版本 bug 热修复分支，用于解决线上问题，修复完成后合并回主分支\ndevelop 分支： 开发分支\nfeature 分支：下一个版本发布主要功能分支\nfeature 分支：未来版本的功能开发和管理\nrelease 分支：预发布分支，代码稳定后合并到主分支和开发分支\nGitHub Flow：\n主分支 + 功能分支\n","externalUrl":null,"permalink":"/docs/%E5%B7%A5%E5%85%B7/git/","section":"Docs","summary":"分布式版本控制系统 1. 命令 # 1.1 安装后配置 # git -v 查看git版本 git","title":"Git","type":"docs"},{"content":" 1.类的初始化执行顺序 # 对所有字段赋默认值 静态语句、静态代码块按声明顺序执行 初始化块按声明顺序执行 执行构造函数 2.类的回收阶段执行finalize方法 # finalize方法会在对象被回收之前执行，不推荐使用，不好把握时间。\n可用用它来实现自救【不被GC回收】（也只有这一次机会，任何对象的finalize方法只能执行一次）\n3.访问修饰符 # 可修饰：类、成员变量、方法，内部类\n**1）public ：**公共权限，可以被任意类访问，不同包不同类依然可以访问，\n**3）default：**默认的(无)，同包权限，只能被同包的类访问\n可修饰：成员变量、方法、内部类\n**2）protected：**受保护的权限，可以被同包类访问，如果不是同包类，必须是该类的子类才可以访问\n**4）private：**私有权限，只能在本类中访问，同包其他类也不能访问\n4.构造函数 # 子类的构造函数会隐式调用父类默认构造（无参构造）函数 调用构造函数语句只能在其它构造函数的第一句 5.抽象类不能实例化，但可以作为非抽象子类的引用(调用子类的方法) # 6.toString 方法在与字符串相加、打印时会被隐式调用 # 7.Arrays.deepToString 方法可以转化多维数组 # 8.反射机制 # static Class forName(Strinf className) 返回Class对象 Object newInstance() 返回这个类的新实例 Object newInstance(Object[] args) 使用构造器构造一个新实例 9.接口 # 接口不能实例化，但可以声明。使用时，必须引用实现了接口的类对象 接口中的字段和方法自动设置为public、public static final 与抽象类的区别：接口提供了多重继承的大多数好处，还能避免多重继承的复杂性和低效性 接口可以定义静态方法、默认方法（default 关键字） 默认方法冲突规则： 超类优先 一个接口提供默认方法，其它接口存在同名同参的方法时，必须在实现这些接口时主动覆盖这个方法（可以直接返回：return 接口名.super.方法名(); ） 继承优先于接口 10.clone # clone是Object类的 protected 方法，是浅拷贝（字段是对象时，只复制引用，只有在引用的对象是不可变对象时才会正常工作）\n实现Cloneable接口【标记类，不包含任何方法，便于使用 instanceof】：重新定义clone方法，并声明为 public\n11.lambda表达式 # lambda表达式是一个代码块以及必须传入的变量类型声明。\n形式：\n(int a, ...) -\u0026gt; { // ... return xxx; } 没有参数仍需要提供括号 只有一个参数，且类型可以推出时，可以省略括号 若可以推出类型，则可以省略类型 单个语句可以省略 return、大括号 12.重载（同名不同参）、重写（同名同参、同返回类型） # 不能重写被标示为 final 的方法\n重载遵循“编译期绑定”，即在编译时根据参数变量的类型判断应该调用哪个方法。\n重写遵循“运行期绑定”，即在运行的时候，根据引用变量所指向的实际对象的类型来调用方法。\n因为在编译期已经确定调用哪个方法，所以重载并不是多态。而重写是多态。重载只是一种语言特性，是一种语法规则，与多态无关，与面向对象也无关。（注：严格来说，重载是编译时多态，即静态多态。但是，Java 中提到的多态，在不特别说明的情况下都指动态多态）\n13.自动装箱、拆箱 # 装箱过程：通过调用包装器的 valueOf 方法实现\n拆箱过程：通过调用包装器的 xxxValue 方法实现\n包装类的 == 运算符不遇到算术运算的情况下不会自动拆箱，此时比的是对象的地址。如 Integer 对象两个128使用 == 比较为 false\nequals 方法不处理数据类型转型。如 g.equals(a + b) ，其中 Long g = 3L; Integer a = 1, b = 2;，结果为 false\n14.内部类 # 内部类也是语法糖，是因为它仅仅是一个编译时的概念，outer.java 里面定义了一个内部类 inner，一旦编译成功，就会生成两个完全不同的.class 文件了，分别是outer.class 和 outer$inner.class。所以内部类的名字完全可以和它的外部类名字相同\n15.断言——底层实现就是 if 语言 # 如果断言结果为true，则什么都不做，程序继续执行，如果断言结果为 false，则程序抛出 AssertError 来打断程序的执行\n16.数字字面量——允许在数字之间插入任意多个下划线 # 不管是整数还是浮点数。这些下划线不会对字面量的数值产生影响，目的就是方便阅读\n17.for-each 实现原理——使用了普通的 for 循环和迭代器 # for(int i : list){ sum += i; } // 反编译结果： for(Iterator localIterator = list.iterator(); localIterator.hasnext(); ){ int i = ((Integer) localIterator.next()).intValue(); sum += i; } 18.try语句块的finally语句块： # 执行try中所有语句 若存在finally块，则执行之 若finally块存在return，则舍弃try中的return 若不存在return，则将之前的return返回 private int x = 0; public int checkReturn() { try { // x 等于 1，此处不返回 return ++x; } finally { // 返回的结果是 2 return ++x; } } private int extracted() { int x = 0; try{ // 返回 0 return x++; }finally{ x++; } } 19.NullPointerException # 防止 NPE，是程序员的基本修养，注意 NPE 产生的场景：\n1） 返回类型为基本数据类型，return 包装数据类型的对象时，自动拆箱有可能产生 NPE\n反例：public int f() { return Integer 对象}， 如果为 null，自动解箱抛 NPE\n2） 数据库的查询结果可能为 null\n3） 集合里的元素即使 isNotEmpty，取出的数据元素也可能为 null\n4） 远程调用返回对象时，一律要求进行空指针判断，防止 NPE\n5） 对于 Session 中获取的数据，建议进行 NPE 检查，避免空指针\n6） 级联调用 obj.getA().getB().getC()；一连串调用，易产生 NPE\n20.泛型 # javac会对泛型进行类型擦除，得到原始类型。在使用时，再对类型进行强制类型转换。\n导致对原始类型数据的支持麻烦，因为不支持 基本数据类型 与 Object 之间的强转，所以干脆不支持这种基本数据类型的泛型\n运行期无法获取到泛型类型信息，不支持下面的用法：\nif (item instanceof E){\t// 无法对泛型进行实例判断 ... } E newItem = new E();\t// 无法使用泛型创建对象 E[] itemArray = new E[10];\t// 无法使用泛型创建数组 两个泛型方法重载时，返回值不同可以重载，否则不能重载\n返回值不参与重载选择，但Class文件格式中，只要描述符不完全一致的两个方法就可以共存\n21.String 相加 # JDK5 之前，加法会转为 StringBuffer 对象的连续 append() 操作（方法都被synchronized修饰，会对sb对象加锁）\nJDK5 之后，加法转为 StringBuilder 对象的连续 append() 操作\n22.JSON深拷贝 # Msg msg2 = JSON.parseObject(JSON.toJSONString(msg1), Msg.class); 23.对象属性拷贝问题 BeanCopier.create()\\BeanUtils.copy() # 属性拷贝时由于类型擦除可能导致 List\u0026lt;A\u0026gt; 赋值给 List\u0026lt;B\u0026gt; ，导致出现问题 在业务层面上PO(persistence object) -\u0026gt; DO、VO -\u0026gt; DTO的转换对应不同的业务含义和用途，不能一一对应映射。不推荐使用属性拷贝，应该手动 convert() 或 transfer() 转换 24.冲突解决流程 # 创建冲突分支（scm默认创建 scm/conflict/xxx） 本地拉取冲突分支，将自己的开发分支 merge 进去。发生冲突时，与对应代码块开发一起对齐合并内容 解决冲突后 push 到 remote scm/conflict/xxx 远程自动完成合并 25.DDD代码分层规范 # 用户接口层、应用层、领域层、基础设施层\nDomain 领域层 # Domain 层 依赖 Infrastructure 层\n含 Aggregate 聚合模型，每个模型包含：\nentity 包：领域实体类 valueobject 包：值对象 respository 包：定义对外操作数据库的方法 service 包：领域对象相关的方法 dto 包：定义对外访问数据库的入参数 跨领域出参为 bo，定义在 app 层，绕过 domain 直接访问 Infrastructure 层 repositoryImpl Infrastructure 层 # api 包：定义对外部服务接口的封装调用 request 包：定义 xxDto 对象作为入参，调用外部接口时，可将 xxDto 转为实际外部接口的入参对象 response 包：定义 xxBo 对象作为出参，调外部接口时，可将实际外部接口出参转为 Bo 对象 common 包：存放常量、枚举 config 包：存放 ark 配置 intergration 包：拆分 db 和 es dao.imp 包：dao 封装了对 Mapper 的调用， xxxDao 需要继承 ServiceImpl【封装了批量保存、更新等方法】 mapper 包： 需要继承 BaseMapper 接口【封装了大部分 crud 】 dataobject 包：定义对外访问数据的出参，DO 结尾 model 包： xxxTemp 类 plugin 包：切面、缓存框架、全局异常处理器、重试组件、自定义线程池、告警工具类 utils 包：工具类 Application 层 # 依赖 Infrastructure、api 层\napi包：对外提供api层接口的实现类，以ServiceImpl结尾，很薄的一层，不涉及业务逻辑，里面调用query包或者command包 \u0026hellip; Interfaces 层 # 对前端展示的路由和适配 仅依赖application层\nassembler包：针对cmd相关方法，将xxxRequest转为xxxCmd，或者将xxxResult转为xxxResponse， 类名以RequestTranslate结尾或ResponseTranslate结尾 facade包： base包：定义dubbo接口，类名以ServiceBase结尾，用户通过业务网关调用dubbo接口，可以不用给用户再包一层http接口进行访问。 impl包：base包中接口的实现类，类名以Adaptor结尾，内部继续调用query或cmd包的方法做服务编排。 request包：定义interfaces层面向用户的接口的入参，类名以Request结尾 response包：定义interfaces层面向用户的接口的出参，类名以Reponse结尾 Api 层 # 封装 sdk 供外部调用 【 rpc api 】 纯粹的参数、接口定义，不依赖DDD的其余层。所有的pom依赖强制加上provided\ncommon包：公共的对象定义，例如公共的分页参数PageRequest, PageResponse enums包：定义对外的枚举，类名以Enum结尾 request包：定义rpc接口入参数，类名以Request结尾 response包：定义prc接口出参数，类名以Reponse结尾 service包：定义dubbo的RPC接口，类名以ServiceBase结尾 DTO 数据传输模型 # 针对前端的业务目标、过程需要、开发使用习惯调整，模型定义贴近业务，更能反应业务的特性\nxxxRequestDTO xxxResponseDTO xxxDTO 领域模型 # 分为实体对象、值对象，每个字段都是对业务中某一特征的完整描述\n实体对象 xxxEntity 值对象 无特殊后缀 DO 数据模型 # 为了更好的实现数据存储，对于mysql，DO的字段可以一一映射数据表的字段。与领域模型的区别是，缺少业务含义，单独的字段属性无法完整描述业务特征\nxxxDO 模型关系 # 26. Map 集合能不能存储 null 值情况 # 集合类 Key Value 并发安全 Hashtable 不允许为 null 不允许为 null 线程安全 ConcurrentHashMap 不允许为 null 不允许为 null 锁分段技术 TreeMap 不允许为 null 允许为 null 线程不安全 HashMap 允许为 null 允许为 null 线程不安全 27. 线程资源必须通过线程池提供，不允许显式创建线程 # 28. 线程池不允许使用 Executors 创建，通过ThreadPoolExecutor创建 # Executors返回的线程弊端：\nFixedThreadPool 和 SingleThreadPool 允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量请求，导致 OOM CachedThreadPool 允许创建线程的数量为 Integer.MAX_VALUE，可能会创建大量线程，导致 OOM 29. 高并发场景中，避免使用 == 判断作为中断或退出条件 # 30. varchar 超过 5000 字符时，应定义为text，独立一张表出来，避免影响其他字段的索引效率 # 31. 字段允许适当冗余，提高查询性能，但必须考虑数据的一致性 # 不是频繁修改的字段 不是 varchar 超长字段，更不是 text 字段 不是唯一索引的字段 32. 分库分表时机：单表超过 500 万行或超过 2GB # 33. 分布式锁使用：业务代码必须写在try内，finally内进行锁的释放 # String activity_create_lock = \u0026#34;activity_create_lock\u0026#34;; Lock lock = lockManager.acquire(activity_create_lock); if (lock.tryLock()) { try { // 业务逻辑 do something }catch (Exception e) { }finally { lock.unlock(); } } 34. 异常分类 # ParamException application 服务接口实现参数校验的地方抛出 BizException 业务异常，application \u0026amp; domain 内部抛出 DependencyException 依赖异常，调用二方接口，接口调用超时、异常或响应不符合业务要求的场景 dependency包中抛出 ExtensionException 业务逻辑的异常场景在开发阶段未被处理或者异常需要被关注和升级（可理解为业务异常的升级版本，抛出位置也一样） 35. 使用枚举类产生单例对象 # public enum ExampleObjectFactory { /** * 单例对象 */ INSTANCES; // 对象属性 ... // 对象方法部分： ... } 无论在什么地方调 ExampleObjectFactory.INSTANCES 得到的都是一个单例对象。JVM 加载类时，保证只会初始化枚举类一次，从而保证了线程安全\n36. MapStruct 的 Mapper # MapStruct 解决的问题：手动创建 bean 映射器非常耗时。但该库可以自动生成 bean 映射器类\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;mapstruct.version\u0026gt;1.3.1.Final\u0026lt;/mapstruct.version\u0026gt; @Mapper(componentModel = \u0026#34;spring\u0026#34;, nullValueCheckStrategy = ALWAYS, nullValueMappingStrategy = RETURN_NULL, nullValuePropertyMappingStrategy = IGNORE) 与lombok共存时需要添加：https://juejin.cn/post/7183207387078066237\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lombok.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.10.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;8\u0026lt;/target\u0026gt; \u0026lt;annotationProcessorPaths\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.mapstruct\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mapstruct-processor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${org.mapstruct.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;path\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${lombok.version}\u0026lt;/version\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;/annotationProcessorPaths\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 37. 幂等、ABA # 幂等的意思是，针对同一个接口，多次发起同一个业务请求，必须保证业务只执行一次。 如何判断两次接口请求是重试关系?\n需要给同一业务请求一个唯一标识，也就是“幂等号”，幂等号需要保证全局唯一性：通过某种算法来随机生成没有业务含义的幂等 号 接口调用方生成幂等号，并且跟随接口请求，将幂等号传递给接口实现方。接口实现方接收到接口请求之后，按照约定，从 HTTP Header 或者接口参数中，解析出幂等号。如果幂等号已经存在，说明业务已经执行或正在执行，直接返回。否则记录幂等号，继续执行业务 使用分布式锁对唯一id，来保证接口幂等\nABA：\n线程 T1 读取共享数据的值 A 线程 T2 修改共享数据的值为 B，然后再修改回 A（也就是经历了 A-\u0026gt;B-\u0026gt;A 的过程） 线程 T1 再次读取共享数据的值，发现和之前读取的值相同，认为数据没有被修改过，继续执行后续操作 在这种情况下，线程 T1 可能会在没有意识到共享数据经历了 B 的中间状态后，错误地进行操作\n解决方法： 添加一列版本号，每次更新数据时，比较传来的版本号是否正确，更新数据时更新版本号\nUPDATE orders SET tracking_number = 666, version = version + 1 WHERE version = 8; 增：可以使用唯一索引保证幂等 删：ABA 问题 改：update x = x + delta 不幂等; update x = y 幂等，也存在 ABA 问题 查：天然幂等\n38. 斐波那契散列法 # 使用斐波那契数列的值作为乘数的散列方法 $$ h(k) = [a * k \\ mod \\ 2^w] \u0026raquo; (w - r) $$ a：实数 ; k：哈希码 =0x61c88647（32位的黄金分割） ; w：计算机字位大小 =32 ; r：=7（2^r = 哈希表大小128）\n所以实际上就是 $$ h(k) = a * 0x61c88647 \u0026raquo; 25 $$\n39. 两个单列索引导致的更新死锁 # 问题 ：并发执行的update语句，where字段不同发生死锁： idx1 与 idx2 为独立的索引字段\nupdate table_name set ... where idx1 = a and idx2 = b; update table_name set ... where idx1 = c and idx2 = d; 原因 ： update时，如果where条件里面涉及多个字段，区分度都比较高且字段都分别建了索引的话，mysql会多个索引各走一遍，然后结果取个交集；\n单条记录更新不会引发问题；\n多条记录并发更新时，如果索引行数有重叠，因加锁顺序可能不同，互相等待可能会导致死锁，为什么加锁顺序会不同呢？where条件的顺序是一定的，那么加锁顺序也应该一定，为什么会有加锁顺序不同情况?\n因为我们使用的是两个单值索引，where条件中是复合条件，那么mysql会使用index merge进行优化，优化过程是mysql会先用索引1进行扫表，在用索引2进行扫表，然后求交集形成一个合并索引。 这个使用索引扫表的过程和我们本身的sql使用索引的顺序可能存在互斥，所以造成了死锁。\n解决方案：删除一个索引、使用联合索引 idx1, idx2\n40. 数据量太大导致 order by id 时索引失效 # 类似\nselect * from tableName where id \u0026gt; 99 and item in (12, 34, 56, 65) order by id desc limit 100; 其中 item 是索引项，当数据巨大时（案例接近一亿），若使用索引，mysql 的执行过程：\n执行 where 语句段 因为二级索引包含主键，继续执行 order by id desc （Using filesort） 执行 limit 回表查询得到完整字段信息 问题：\n二级索引数据过多，需要在临时磁盘排序，性能很差，mysql 会把这种方式判定为不好的方式 执行器实际执行的方式：\n顺序扫描主键索引 判断是否满足 where 条件 拿到满足的 limit 就结束 为何之前没有发现问题？ where 中字段为新增字段，之前没有数据，会发生全表扫描，因此变慢\n解决方案：\n强制使用索引 force(idx_item) 写到 sql 中 order by 时使用 order by id + 0 去排除使用主键索引的可能 41. MySQL 行锁 # update table_name set a = a + 1 where id = 1;\nset a = a + 1 时会锁住 id = 1 这一行\n如果在事务内执行，则其他使用此行的会被阻塞\n","externalUrl":null,"permalink":"/docs/java/java%E5%9F%BA%E7%A1%80/","section":"Docs","summary":"1.类的初始化执行顺序 # 对所有字段赋默认值 静态语句、静态代码","title":"Java","type":"docs"},{"content":" 1.线程的基本操作 # Java 线程主流实现是使用内核线程 1:1 实现，每个内核线程都可以支持一个轻量级线程。由内核完成线程切换，内核通过操纵调度器对线程调度\n1—创建、启动 # 方式一、继承Thread、重写 run 方法\nT1 t1 = new T1();\t// T1 类继承Tread t1.start(); // 匿名类形式 Thread t1 = new Thread(){ @Override public void run(){ // 代码 } }; t1.start(); // 简化的匿名类 Thread t1 = new Thread(() -\u0026gt; { // 代码 }); t1.start(); start() 后，会创建一个新的线程并让这个线程执行 run() 方法。在当前线程使用 t1.run() 也会执行（串行），不会启动新线程。\n方式二、实现 Runnable 接口。\nThread有一个构造方法，可以传入实现这个接口的类。\npublic interface Runnable{ public abstract void run(); } public class ThreadDemo implements Runnable{ public static void main(String[] args){ Thread t = new Thread(new ThreadDemo()); t.start(); } @Override public void run(){ System.out.println(\u0026#34;Test\u0026#34;); } } 方法三、基于线程池的execute()，创建临时线程\npublic class EThread { public static void main(String[] args) { //创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for(int i = 0;i\u0026lt;10;i++) { //调用execute()方法创建线程 //采用匿名内部类的方法，创建Runnable对象，并重写run()方法 threadPool.execute(new Runnable() { @Override public void run() { System.out.println(Thread.currentThread().getName()); } }); // lambda 简化 threadPool.execute(() -\u0026gt; { System.out.println(Thread.currentThread().getName() + \u0026#34;执行\u0026#34;); }); } } } 方法四、使用Callable和Future：使用Callable接口创建一个带有返回值的任务，并通过ExecutorService的submit()方法提交任务并获取Future对象，调用Future的get()方法获取任务的返回结果\nCallable\u0026lt;Integer\u0026gt; callable = new Callable\u0026lt;Integer\u0026gt;() { public Integer call() throws Exception { // 线程执行的代码 // ... return result; // 返回结果 } }; // lambda 简化 Callable\u0026lt;Integer\u0026gt; callable = () -\u0026gt; { // 线程执行的代码 return result; }; ExecutorService executor = Executors.newSingleThreadExecutor(); Future\u0026lt;Integer\u0026gt; future = executor.submit(callable); Integer result = future.get(); // 获取返回结果 executor.shutdown();\t// 关闭线程池 2—终止线程、sleep方法 # stop() 方法会直接终止线程，并立即释放所持有的锁。过于暴力\ninterrupt() 方法，告诉线程被中断了（只是设置了中断标志位），需要自行处理中断(增加相应的中断处理代码)\nisInsterrupted() 判断线程是否被中断\ninterrupted() 判断线程是否被中断，并清除当前中断状态\nThread t1 = new Thread(){ @Override public void run(){ while(true){ if(Thread.currentThread().isInterrupted()){ break; } Thread.yield();\t//使当前线程从运行状态变为就绪状态(线程让步) } } }; t1.start(); Thread.sleep() 方法会让当前线程休眠若干时间。若线程休眠时被中断，它会抛出一个InterruptedException 中断异常（不是运行时异常，需要捕获并处理）\ntry { Thread.sleep(2000); } catch (InterruptedException e) { System.out.println(\u0026#34;Interrupted when sleep\u0026#34;); // 设置中断状态，因为抛出异常，会清除中断标志位 Thread.currentThead().interrupt(); } 3—等待（wait）和通知(notify)（必须在 synchronized 语句中） # wait 和 notify 均为 Object 类的方法 因此调用时需要对这个类进行加锁 线程调用 object.wait() 方法后会等待其它线程调用 object.notify() 方法随机唤醒一个线程，且 wait 会释放锁\nobject.notifyAll() 唤醒这个等待队列中的所有线程\n4—挂起（suspend）和继续执行（resume） # 不推荐使用，如果resume在suspend之前执行，会发生错误\nsuspend() 方法会暂停线程，但是不会释放锁。直到对应的线程执行resume方法\nresume() 方法\n5—join方法 # 一个线程的输入可能非常依赖另一个或多个线程的输出，这个线程需要等待依赖的线程执行完毕。\npublic final void join() 无期限等待（阻塞） public final synchronized void join(long millis) 有期限等待 使用：在当前线程使用 t.join() 可以让当前线程等待 线程t 结束\n6—volatile 关键字 # 保证可见性，不保证原子性\n一条线程修改了变量值后，新值会被立即刷新到主内存，其它线程获取主内存的变量时立即可见。普通变量不会立即刷新到主内存，而是存在工作内存中\n禁止指令重排\n双锁检测单例模式：\npublic class Singleton { private volatile static Singleton instance; public static Singleton getInstance() { if (instance == null) {\t// 未创建单例才需要创建 synchronized (Singleton.class) {\t// 加锁 if (instance == null) {\t// 若A创建后释放，被B获取锁，则还需要进行判断是否为空 instance = new Singleton(); } } } return instance(); } } synchronized 和 final 也能实现可见性 7—线程组 # ThreadGroup tg = new ThreadGroup(\u0026#34;threadName\u0026#34;);\t//创建一个线程组，名为threadName Thread t1 = new Thread(tg, new aThread(), \u0026#34;T1\u0026#34;);//新建一个线程放入线程组tg，名称位T1 tg.activeCount() 可以获取活动的线程总数（估计的） tg.list() 可以打印所有线程的信息 8—Daemon守护线程、线程优先级 # 如果用户线程全部结束，守护线程也会结束\nt.setDaemon(true); // 设置t为守护线程 t.setPriority(Thread.MAX_PRIORITY)\t// 设置线程优先级为最高级 10 // Thread.MIN_PRIORITY = 1 // Thread.NORM_PRIORITY = 5 9—synchronized # 指定加锁对象，进入同步代码之前要获得给定对象的锁\n在new一个线程时，传入的对象要是同一个，而不是都去new一个新的对象（多把不同的锁）\n作为方法签名，相当于对当前实例加锁，进入同步代码前需要获得当前实例的锁\n注意new线程时，需要把实例传进去，这样才能关注同一个实例的锁\n作为静态方法签名，相当于对当前类加锁，需要获得当前类的锁\n使用这种方法可以都去new一个新对象，因为类都是一致的，是一把锁\n10—ArrayList线程不安全\u0026ndash;使用Vector代替 # 可能正常结束\n可能在扩容时，内部一致性被破坏，越界报错\n更可能多线程对同一个位置赋值，导致添加的数量少\n11—HashMap线程不安全\u0026ndash;使用ConcurrentHashMap代替 # 可能正常结束 正常结束不符合预期（也是因为没有锁） 死循环（jdk1.7扩容时put头插法导致数据结构异常形成环） 2.JDK并发包 # 1—可完全代替synchronized的重入锁ReentrantLock # 必须手动指定何时加锁 lock()，释放锁 unlock() 。\nlock()\nunlock()\nlockInterruptibly()\ntryLock()\ntryLock(long time, TimeUnit unit)\nJDK6 优化后，两者的性能差不太多，可以使用 synchronized 的情况下就使用它，更简单、更易优化\n可重入：可以加上多重锁，但释放也需要多次释放锁\n公平锁：默认是非公平，可以通过带布尔参数的构造函数使用公平锁。（严格按照申请锁的顺序依次获得锁，但是性能会急剧下降）\npublic class ReentrantLockTest implements Runnable{ public static ReentrantLock lock = new ReentrantLock(); public static int i = 0; @Override public void run() { for (int j = 0; j \u0026lt; 1000; j++) { lock.lock(); try { i++; }finally { lock.unlock(); } } } public static void main(String[] args) throws InterruptedException { ReentrantLockTest t = new ReentrantLockTest(); Thread t1 = new Thread(t); Thread t2 = new Thread(t); t1.start();t2.start(); t1.join();t2.join(); System.out.println(i); } } 中断响应 # lock.lockInterruptibly();\t// 请求锁lock，但是可以响应中断 lock.isHeldByCurrentThread();\t// 当前线程是否持有锁 a.interrupt();\t// 中断线程a 限时等待 # lock.tryLock(5, TimeUnit.SECONDS);\t// 请求锁lock，若超过5秒没有得到锁，返回false。得到锁，返回true // 不带参数时，不等待，无锁返回false 公平锁 # 可以在构造重入锁时传入fair参数，表示是否公平（排队）\n公平锁需要维护有序线程队列，性能低下\nCondition对象——重入锁的搭档 # public static Condition condition = lock.newCondition(); 获取可重入锁对应的Condition对象\nvoid await() 等待并释放锁，直到其它线程调用 signal 方法唤醒后，重新申请锁并继续运行 void awaitUninterruptibly() 与上一致，但不会在等待过程中响应中断 long awaitNanos(long nanoTimeout) boolean await(long time, TimeUnit unit) boolean awaitUntil(Date deadline) void signal() 唤醒一个等待中的线程，调用时，需要先获取锁，调用后，需要释放锁 void signalAll() 唤醒等待中的所有线程 public class ConditionDemo { private static final int BUFFER_SIZE = 5; private static final Queue\u0026lt;Integer\u0026gt; buffer = new LinkedList\u0026lt;\u0026gt;(); private static final Lock lock = new ReentrantLock(); private static final Condition notFull = lock.newCondition(); private static final Condition notEmpty = lock.newCondition(); public static void main(String[] args) { for (int i = 0; i \u0026lt; 5; i++) { Thread producerThread = new Thread(ConditionDemo::producer); producerThread.setName(\u0026#34;Producer-\u0026#34; + i); producerThread.start(); } for (int i = 0; i \u0026lt; 3; i++) { Thread consumerThread = new Thread(ConditionDemo::consumer); consumerThread.setName(\u0026#34;Consumer-\u0026#34; + i); consumerThread.start(); } } private static void producer() { while(true){ try { lock.lock(); while (buffer.size() == BUFFER_SIZE) {\t// 不能使用if，否则会越出临界区 notFull.await(); // 缓冲区已满，生产者等待 } // 生产数据 int data = produceData(); buffer.offer(data); System.out.println(Thread.currentThread().getName() + \u0026#34; Produced: \u0026#34; + data); notEmpty.signal(); // 唤醒等待的消费者 } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } } private static void consumer() { while(true){ try { lock.lock(); while (buffer.isEmpty()) { notEmpty.await(); // 缓冲区为空，消费者等待 } // 消费数据 int data = buffer.poll(); System.out.println(Thread.currentThread().getName() +\u0026#34; Consumed: \u0026#34; + data); notFull.signal(); // 唤醒等待的生产者 } catch (InterruptedException e) { e.printStackTrace(); } finally { lock.unlock(); } } } private static int produceData() throws InterruptedException { // 生产数据的逻辑 Thread.sleep(200); Random random = new Random(); return random.nextInt(); } } 2—信号量Semaphore # 排他锁、共享锁 # 排他锁，只允许一个线程可以访问临界资源。eg.:重入锁\n共享锁，允许多个线程访问临界资源。eg.: 信号量\n信号量 # 构造函数（必须指定可用临界资源数目）：\npublic Semaphore(int permits) public Semaphore(int permits, boolean fair) 方法：\npublic void acquire() 尝试获取临界资源，直到获取或被中断 public void acquireUninterruptibly() public boolean tryAcquire() 尝试获取临界资源，成功返回 true，失败立即返回 false public boolean tryAcquire(long timeout, TimeUnit unit) public void release() 释放临界资源 import java.util.concurrent.Semaphore; public class SemaphoreExample { private static final int MAX_CONCURRENT_THREADS = 3; private static final int TOTAL_TASKS = 10; public static void main(String[] args) { Semaphore semaphore = new Semaphore(MAX_CONCURRENT_THREADS); for (int i = 0; i \u0026lt; TOTAL_TASKS; i++) { Thread task = new Thread(() -\u0026gt; { try { semaphore.acquire(); // 获取信号量许可 System.out.println(\u0026#34;Task started: \u0026#34; + Thread.currentThread().getName()); // 模拟任务执行时间 Thread.sleep(2000); System.out.println(\u0026#34;Task completed: \u0026#34; + Thread.currentThread().getName()); } catch (InterruptedException e) { e.printStackTrace(); } finally { semaphore.release(); // 释放信号量许可 } }); task.start(); } } } 3—读写锁ReentrantReadWriteLock # 使用：\nprivate static ReentrantReadWriteLock readWriteLock = new ReentrantReadWriteLock(); private static Lock readLock = readWriteLock.readLock(); private static Lock writeLock = readWriteLock.writeLock(); 读-读：不互斥 读-写：互斥 写-写：互斥 import java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantReadWriteLock; public class ReadWriteLockDemo { public static final ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); private static final Lock readLock = lock.readLock(); private static final ReentrantReadWriteLock.WriteLock writeLock = lock.writeLock(); private static int state = 0; public static void main(String[] args) { Runnable read = () -\u0026gt; { try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } readLock.lock(); try { System.out.println(Thread.currentThread().getName() + \u0026#34;Read state:\u0026#34; + state); } finally { readLock.unlock(); } }; Runnable write = new Runnable() { @Override public void run() { writeLock.lock(); try { state++; System.out.println(Thread.currentThread().getName() + \u0026#34;State increment 1\u0026#34;); } finally { writeLock.unlock(); } } }; for (int i = 0; i \u0026lt; 5; i++) { Thread readThread = new Thread(read); readThread.setName(\u0026#34;Thread-read-\u0026#34; + i); readThread.start(); } for (int i = 0; i \u0026lt; 2; i++) { Thread writeThread = new Thread(write); writeThread.setName(\u0026#34;Thread-write-\u0026#34; + i); writeThread.start(); } } } 4—倒计数器CountDownLatch # 实现倒计数的功能\npublic CountDownLatch(int count)\ncountDown() 通知已完成一项任务 await() 要求所有的任务完成后继续执行 public class CountdownLatchExample { private static final CountDownLatch latch = new CountDownLatch(10); private static int count = 0; public static void main(String[] args) { for (int i = 0; i \u0026lt; 10; i++) { Thread thread = new Thread(() -\u0026gt; { try { // 执行任务 Thread.sleep(2000); System.out.println(++count); } catch (InterruptedException e) { e.printStackTrace(); } finally { latch.countDown(); // 任务完成，计数器减一 } }); thread.start(); } try { latch.await(); // 等待计数器达到零 System.out.println(\u0026#34;All threads have completed their tasks.\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } } } 5—循环栅栏CyclicBarrier # 与倒计数器类似，更复杂强大。这个计数器可以反复使用，凑齐一批线程后计数器归零\npublic CyclicBarrier(int parties, Runnable barrierAction) 多一个参数，指定完成一次计数后系统会执行的动作\nawait() 要求完成一次计数后继续执行\n异常除了 InterruptedException 外，还有 BrokenBarrierException 表示当前的循环栅栏已经破损，可能无法等待所有线程到齐了。\npublic class CyclicBarrierDemo { private static int count; public static void main(String[] args) { CyclicBarrier cb = new CyclicBarrier(2, ()-\u0026gt;{ System.out.println(\u0026#34;线程执行完一轮.\u0026#34;); }); for (int i = 0; i \u0026lt; 4; i++) { Thread thread = new Thread(() -\u0026gt; { try { System.out.println(++count); System.out.println(Thread.currentThread().getName() + \u0026#34;暂停到屏障之前.\u0026#34;); cb.await(); System.out.println(Thread.currentThread().getName() + \u0026#34;继续到屏障之后.\u0026#34;); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } }); thread.setName(\u0026#34;Thread-\u0026#34; + i); thread.start(); } } } 输出：\n2 4 1 3 Thread-2暂停到屏障之前. Thread-1暂停到屏障之前. Thread-0暂停到屏障之前. Thread-3暂停到屏障之前. 线程执行完一轮. 线程执行完一轮. Thread-1继续到屏障之后. Thread-3继续到屏障之后. Thread-2继续到屏障之后. Thread-0继续到屏障之后. 6—线程阻塞工具类 LockSupport # 可以在线程内任意位置让线程阻塞。使用了类似信号量的机制，为每一个线程准备一个许可，许可可用则park方法立即返回，并消费这个许可。如果许可不可用，就会阻塞。unpark方法会使一个许可变为可用。\npark() 静态方法，可以阻塞当前线程，类似 suspend() parkNanos() 、parkUtil() 限时阻塞 unpark(Thread thread) 类似 resume() public class LockSupportExample { public static void main(String[] args) { Thread thread = new Thread(() -\u0026gt; { System.out.println(\u0026#34;Thread started.\u0026#34;); LockSupport.park(); // 线程阻塞 if(Thread.interrupted()){ System.out.println(Thread.currentThread().getName() + \u0026#34; 被中断了\u0026#34;); } System.out.println(\u0026#34;Thread resumed.\u0026#34;); }); thread.start(); try { Thread.sleep(2000); // 主线程休眠2秒钟 // thread.interrupt(); LockSupport.unpark(thread); // 解除线程的阻塞状态 } catch (InterruptedException e) { e.printStackTrace(); } } } LockSupport.park() 还支持中断响应，和其他接收中断的方法不一样，不会抛出 InterruptedException 异常，仍会继续执行线程任务，可以 Thread.interrupted() 等方法中获得中断标记\nLockSupport.park(); if(Thread.interrupted()){ System.out.println(Thread.currentThread().getName() + \u0026#34; 被中断了\u0026#34;); } 7 —理解锁 AbstractQueuedSynchronizer # 重入锁和信号量都实现了一个 AbstractQueuedSynchronizer 的子类 Sync （核心代码都不在这里）。\n同步等待队列：保存等待在这个锁上的线程（由 lock() 操作引起的等待）\n条件等待队列：维护等待在条件变量上的等待线程。一个重入锁可以有多个条件等待队列，每个条件变量对象内部都维护了一个等待队列，唤醒后加入同步等待队列尾部\n队列都由Node对象组成\nclass Node: int waitStatus;\t# CANCELLED=1，线程取消等待 # SIGNAL=-1，后续节点需要被唤醒 # CONDITION=-2，线程在条件等待队列中 # PROPAGATE=-3，共享模式下，无条件传播releaseSared状态 # 0，初始状态 Node prev; Node next; Thread thread; Node nextWaiter;\t# 对于条件变量等待队列，下一个在条件变量队列中等待的节点 acquire 请求锁\npublic final void acquire(int arg){ // 尝试获取许可，arg为许可的个数。对于重入锁，每次请求为 1 个 // 若tryAcquire 失败，则先使用addWaiter将当前线程加入同步等待队列再获取锁 if(!tryAcquire(arg) \u0026amp;\u0026amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } tryAcquire 尝试获取一个许可。抽象方法，实现在子类中。\naddWriter 将当前线程加入同步等待队列\nprivate Node addWriter(Node node) { Node node = new Node(Thread.currentThread(), mode);\t// 当前线程对象 // 快速加入尾端，可能失败 NOde pred = tail; if(pred != null) { node.pred = pred; if(compareAndSetTail(pred, node)) { pred.next = node; return node; } } // 如果快速加入失败，就用enq方法将node加入队列尾端 enq(node); return node; } acquireQueued 对已经在队列中的线程请求锁\nfinal Boolean acquireQueued(final Node node, int arg) { Boolean failed = true; try { Boolean interrupted = false; for(;;) { // 只有第二个节点才能尝试，第一个节点已经请求锁在运行了 final Node p = node.predecessor(); if (p == head \u0026amp;\u0026amp; tryAcquire(arg)) { setHead(node);\t// 把自己设置到头部\tp.next = null; failed = false;\t// 标记成功 return interrupted; } // 请求失败时需不需要阻塞 \u0026amp;\u0026amp; 对前序节点是SIGNAL的需压执行park操作；对于已取消的节点，跳过；对初始节点和PROPAGATE节点，设置为SIGNAL if (shouldParkAfterFailedAcquire(p, node) \u0026amp;\u0026amp; parkAndCheckInterrupt()) interrupted = true; } } finally { if (failed)\t// 意外失败，取消这个请求 cancelAcquire(node); } } 8—流量控制、限流算法、RateLimiter # 漏桶算法：利用一个缓冲区，请求进入系统时，先保存在缓冲区，用固定的流速流出缓冲区加以处理。容积和流出速率是两个主要参数 令牌桶算法：桶存放令牌，需要先获取令牌才能对请求进行处理。没有令牌时，要么丢弃请求，要么等待令牌。每个单位时间产生一定数量的令牌放入桶中。 RateLimiter采用的令牌桶算法：\n下面的代码 2行限制只能每秒处理两个请求。 13行调用 accquire 方法控制流量\npublic class Demo { static RateLimiter limiter = RateLimiter.create(2); public static class Task implements Runnable { @Override public void run() { System.out.println(System.currentTimeMillis()); } } public static void main(String args[]) throws InterruptedException { for(int i = 0; i \u0026lt; 50; i++) { limiter.acquire(); new Thread(new Task()).start(); } } } 下面的场景是直接丢弃过载请求：\nfor(int i = 0; i \u0026lt; 50; i++) { if(!limiter.tryAcquire()){ continue; } new Thread(new Task()).start();\t// 只有一个输出，500ms内for就可以执行完，这样只能拿到一个令牌 } 9—线程池 # 9.1 Executors线程工厂 # public static ExecutorService newFixedThreadPool(int nThreads)\n返回一个固定线程数量的线程池。当有新的任务提交时，若线程池有空闲线程，则立即执行。若没有，新的任务会暂存到任务队列，待有线程空闲时处理任务队列的任务。\npublic static ExecutorService newSingleThreadPool()\n返回一个只有一个线程的线程池。多余一个新的任务提交会被暂存到任务队列\npublic static ExecutorService newCachedThreadPool()\n返回一个可根据实际情况调整线程数量的线程池。线程数量不确定，但若有线程可以复用，则优先使用可复用的线程。无线程可复用会创建新的线程处理任务。\npublic static ScheduledExecutorService newSingleThreadScheduledExecutor()\n返回一个 ScheduledExecutorService 对象，线程池大小为 1。\nScheduledExecutorService 接口拓展了 ExecutorService 在给定时间内执行某任务的功能，如固定延时后执行、周期性执行某任务\nschedule(Runnable command, long delay, TimeUnit unit) 在给定时间对任务进行一次调度 scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) 在上一个任务开始之后的period 时间调度下一个任务。任务抛异常，后续的子任务都会停止调度。执行时间\u0026gt;period时，周期变为长的那个 scheduleAtFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) 在上一个任务结束之后的delay 时间后调度下一个任务 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize)\n与上面一样，但可以指定线程数量\n线程池对象.submit(Thread thread) 提交任务给线程池\n9.2 核心线程池的内部实现—ThreadPoolExecutor # public ThreadPoolExecutor(int corePoolSize,\t// 常驻线程数量 int maximumPoolSize,\t// 最大线程数量 IO密集型：2n， CPU密集型：n+1 n是电脑核心数 long keepAliveTime,\t// 当线程数量超过常驻数量时，多余线程的存活时间 TimeUnit unit,\t// keepAliveTime 单位 BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,\t// 任务队列——被提交但是未执行的任务 ThreadFactory threadFactory,\t// 线程工厂——用于创建线程，一般使用默认的 RejectedExecutionHandle handle\t// 拒绝策略 ) 直接提交的任务队列 SynchronousQueue：是一种特殊阻塞队列，没有容量，每个插入操作都要等待一个对应的删除操作，反之亦然。提交的任务不会被真实保存，总是将任务提交给线程执行。若没有空闲线程，尝试创建新线程，若数量已达最大，执行拒绝策略。所以使用时退出要设置很大的最大线程数量 有界的任务队列 ArrayBlockingQueue：必须带一个容量参数。新任务来时，若线程数量 \u0026lt; corePoolSize 则创建新线程。否则将新任务加入等待队列；若等待队列已满，在总线程数 \u0026lt; maximumPoolSize 的前提下，创建新线程执行任务，否则执行拒绝策略。 无界的任务队列 LinkedBlockingQueue: 若线程数量 \u0026lt; corePoolSize 则创建新线程。否则将新任务加入等待队列 优先任务队列 PriorityBlockingQueue：特殊的无界任务队列，确保高优先级任务先执行 核心线程池的实现： # // 不存在线程动态变化，阻塞队列可能会迅速膨胀直到耗尽系统资源 public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()) } // 退化的固定线程池 public static ExecutorService newSingleThreadExecutor(int nThreads) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())) } // 没有任务时，线程为0，有任务时，会迫使开新线程执行任务，执行完毕后空闲线程在指定时间（60s）被回收 public static ExecutorService newCachedThreadPool(int nThreads) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()) } 线程调度逻辑： # public void execute(Runnable command) { if (command == null) throw new NullPointerException(); int c = ctl.get(); if (workCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true))\t// 线程数小于常驻线程数量，调度运行 return; c = ctl.get(); } if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) {\t// 进入等待队列 int recheck = ctl.get(); if (!isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false))\t// 进入等待队列失败（如有界队列），则将任务直接提交给线程池 reject(command);\t// 线程数达到最大，执行拒绝策略 } 拒绝策略 # 实现 RejectedExecutionHandler 接口\nAbortPolicy 直接抛出异常，阻止系统正常工作 CallerRunsPolicy 只要线程池未关闭，直接在调用者线程中运行当前被丢弃的任务。显然不会真正丢弃，但是任务提交线程的性能极有可能急剧下降 DiscardOldestPolicy 丢弃最老的一个请求（即将被执行的请求），并再次提交当前任务。 DiscardPolicy 丢弃无法处理的任务，不做任何处理 ThreadFactory # 是个接口，用于线程创建，只有一个方法 Thread newThread(Runnable r)\n可以方便地跟踪线程池，创建多少线程、自定义线程每次、组、优先级、设置为守护线程等等。\n拓展线程池 # ThreadPoolExecutor 提供了beforeExecute() 执行之前 、afterExecute() 执行完成 、 terminated() 线程池退出 三个接口对线程池进行控制。\n使用 executorService.execute(thread) 来提交任务，而不是 submit 提交任务完成后，使用 executorService.shutdown() 方法关闭线程池。他会等待所有任务完成后再关闭线程池，在其之后这个线程就不能再接受其它新的任务 估算线程池大小 # Java Concurrency in Practice 书中给出： $$ N_{cpu} = CPU的数量\\ U_{cpu} = 目标CPU使用率\\ W/C = 等待时间与计算时间的比率\\ 最优线程池大小 N_{threads} = N_{cpu} \\times U_{cup} \\times (1 + W/C) $$\n10—JDK容器源码 # ConcurrentHashMap: 高效并发的HashMap CopyOnWriteArrayList：在读多写少的场景，远远优于Vector ConcurrentLinkedQueue：高效并发的队列，使用链表实现，可以看作线程安全的LinkedList BlockingQueue：接口，JDK内部跳过链表、数组等方式实现了这个接口。适合作为数据共享的通道 ConcurrentSkipListMap：使用跳表实现的Map 10.1 HashMap 与 ConcurrentHashMap # 10.1.1 HashMap # 默认容量 16、装载因子 0.75、最大容量 $2^{30}$ 、链表树化阈值 8、反树化阈值 6、最小树化容量 64\nthreshold 保存下一次扩容的容量，modCount 表示版本，用于 fail-fast 并发修改错误\n计算槽位的过程\n计算 hash = h(key.hashcode) ^ (h \u0026gt;\u0026gt;\u0026gt; 16) ：高 16位的异或参与可以增加结果随机性 null key的hash = 0\n计算 (n(数组长度) - 1) \u0026amp; hash 作为目标槽位\n给定容量求大于它的最小二次幂\nstatic final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u0026gt;\u0026gt;\u0026gt; 1;\t// 逐渐将最高位 1 之后的未知 1 或 0 转为全 1 n |= n \u0026gt;\u0026gt;\u0026gt; 2; n |= n \u0026gt;\u0026gt;\u0026gt; 4; n |= n \u0026gt;\u0026gt;\u0026gt; 8; n |= n \u0026gt;\u0026gt;\u0026gt; 16; return (n \u0026lt; 0) ? 1 : (n \u0026gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;\t// 加一后进位到合适容量 } get 流程\ntable 非空、长度大于 0 并且对应槽位的首节点非空则继续，否则返回 null 首节点hash = 目标key的hash 且 key与首节点key是一个对象 || key != null \u0026amp;\u0026amp; key与首节点key值相等，则返回首节点 若次节点非空，且是树节点，使用 getTreeNode 方法查找。否则顺着链表查询是否有满足哈希值相等且key一致的节点返回 put 流程\ntable 空 或 table 长度为 0。使用 resize 方法初始化 table\n若读取的槽位为 null，tab[i] = newNode(hash, key, value, null)。++modCount，若 size \u0026gt; threshold，进行 resize\n否则，若为槽位首节点，则将值置为值，返回旧值\n否则，若首节点为树节点，调用 putTreeVal 返回节点 e ，将值置为值，返回旧值\n否则，为链表节点，要么找到待修改节点e，要么在链表尾部创建新节点。若链表长度 \u0026gt; 8，调用 treeifyBin 检测 table 长度是否达到最小树化容量，没达到就调用 resize，达到则进行树化。添加新节点时 ++modCount，若数组长度超过阈值，进行 resize\n10.1.2 ConcurrentHashMap # 可以使用 Collections.synchronizedMap(new HashMap()) 产生一个线程安全的HashMap，但是并发度不高 内部结构\nint sizeCtl：记录参与Map扩展的线程数量，也可以记录新的table的扩容阈值，默认n - (n\u0026raquo;\u0026gt;2) = 12\nCounterCell[] counterCells：记录元素数量。是个数组，避免多线程产生的冲突\nNode\u0026lt;K,V\u0026gt;[] table：存放Map\nNode\u0026lt;K,V\u0026gt;[] nextTable：需要扩容时，把新的元素填充到此\n与HashMap类似，数组（默认16）加链表，链表长度 \u0026gt; 8转为红黑树。\n数组阈值 \u0026gt; 0.75 进行扩容，如果老的数组元素已完成复制，会将老数组的元素使用ForwardingNode对象替代，表示当前槽位已经处理，这样多线程同时参与扩容时不会发生冲突。\nput\n如果没有初始化数组，则尝试初始化数组\n如果当前正在扩容，则参与帮助扩容\n将给定的K-V放入对应的槽位（使用synchronized锁住桶）\n统计元素总数\n触发扩容操作\n根据高位是否为 1 分为 high节点/low节点。high节点放到原位置 + n（老数组长度）的位置，low节点留在原地\n使用CAS获取槽位上的节点，为空时，会使用CAS操作填充节点\nget\n根据hash得到槽位 (n - 1) \u0026amp; hash\n若第一个元素key就是请求的key，直接返回\n否则调用find方法查找\nForwardingNode.find()\nTreeBin.find()\n链表顺序查找\n10.2 List的线程安全 # Vector\nCopyOnWriteArrayList\nCollections.synchronizedList(new LinkedList\u0026lt;String\u0026gt;())\n10.3 ConcurrentLinkedQueue队列 # 节点定义\nprivate static class Node\u0026lt;E\u0026gt; { volatile E item; volatile Node\u0026lt;E\u0026gt; next; } 对Node操作，使用了CAS操作：\n// 设置当前Node的值，当前值=期望值cmp时，会将目标值置为val boolean casItem(E cmp, E val) { return UNSAFE.compareAndSwapObject(this, itemOffest, cmp, val); } void lazySetNext(Node\u0026lt;E\u0026gt; val) { UNSAFE.putOrderedObject(this, nextOffest, val); } // 设置当前next的值，当前值=期望值cmp时，会将目标值置为val boolean casNext(Node\u0026lt;E\u0026gt; cmp, Node\u0026lt;E\u0026gt; val) { return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); } 10.4 CopyOnWriteArrayList——并发的ArrayList # 读操作不需要加锁，写操作也不会阻塞读操作。在修改时，先加锁，进行一次自我复制。写完之后用修改的副本替换原有的数据。复制操作对于原数组来说只是读取操作，不改变原数组状态，因此完全可以同其他读操作同时执行\nprivate volatile transient Object[] array\n10.5 BlockingQueue 接口 # 适合用作数据共享通道。会让服务线程在队列为空时进行等待，当新的消息进入队列后自动将线程唤醒。\nArrayBlockingQueue：\n使用 takeIndex 和 putIndex 两个变量表示队列头部和尾部位置，实现循环队列。\nfinal Object[] items;\t// 存放对象 final ReentrantLock lock; private final Condition notEmpty; private final Condition notFull; offer、put 都可以压入元素。offer元素满了就会立即返回false。put会一直等待，直到队列空闲\n队列满时，put 会让当前线程在 notFull上等待。出队时，通知等待入队的线程。\npoll、take 都可以弹出元素。poll 若队列为空则返回 null，take会等待直到队列中有可用元素\n若队列为空，take 会让当前线程在 notEmpty 上等待【notEmpty.await()】。新元素入队时，则进行一次 notEmpty 上的通知【notEmpty.signal()】\n3. 锁优化及注意事项 # 1.提高锁性能的建议 # 减少锁持有时间\n只在有必要时进行同步\n减少锁粒度——缩小锁定对象的范围，降低锁冲突的可能性\n如 ConcurrentHashMap 只锁住桶，而不是整个数组\n用读写分离锁替换独占锁——读多写少时使用读写锁提升并发能力\n锁分离\n在 LinkedBlockingQueue 中，take 和 put 方法分别实现了从队列中取得和向队列添加数据的功能。两个操作分别作用于队头和队尾\nprivate final ReentrantLock takeLock = new ReentrantLock();\tprivate final Condition notEmpty = takeLock.newCondition(); private final ReentrantLock putLock = new ReentrantLock(); private final Condition notNull = putLock.newCondition(); take:\npublic E take() throws InterruptedExecption { E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; takeLock.lockInterruptibly();\t// 不能有两个线程同时取数据 try { try { while (count.get() == 0) notEmpty.await();\t// 当前无可用数据，等待put方法通知 } catch (InterruptedException ie) { notEmpty.signal();\t// 通知其它未中断线程 throw ie; } x = extract();\t// 取得第一个数据 c = count.getAndDecrement();\t// 数量 -1，原子操作。c是 -1之前的值 if(c \u0026gt; 1) notEmpty.signal();\t// 通知其它take方法 } finally { takeLock.unlock();\t// 释放锁 } if(c == capacity) signalNotFull();\t// 通知put方法，已有空余空间 return x; } put:\npublic void put(E e) throws InterruptedExecption { if(e == null) throw new NullPointerExecption(); int c = -1; final ReentrantLock putLock = this.putLock; final AtomicInteger count = this.count; putLock.lockInterruptibly();\t// 不能有两个线程同时调用put try { try { while (count.get() == capacity) notFull.await();\t// 若队列已满，等待 } catch (InterruptedExecption ie) { notFull.signal();\t// 通知未中断线程 throw ie; } insert(e);\t// 插入数据 c = count.getAndIncrement();\t// 更新总数，c是+1之前的值 if(c + 1 \u0026lt; capacity) notFull.signal();\t// 有足够的空间，通知其它线程 } finally { putLock.unlock();\t// 释放锁 } if(c == 0) signalNotEmpty();\t// 插入成功，通知take方法取数据 } 锁粗化\n一连串的对同一个锁不断请求和释放的操作，会把所有的锁操作整合成对锁的一次请求。如循环内加锁，可放到循环外部。\n2.JVM对锁的优化 # 偏向锁 -XX:+UseBiasedLocking\n若一个线程获得锁，锁进入偏向模式。当这个线程再次请求锁时，无须再做任何同步操作。对于几乎没有锁竞争的场合有比较好的优化效果，竞争激烈的场合偏向模式失效。\n轻量级锁\n自旋锁\nJVM假设在不久的将来，线程可得到这把锁。因此让当前线程做几个空循环。经过若干空循环后，如果可以得到锁，就顺利进入临界区；否则才会真正在OS层面挂起。自旋次数默认十次 -XX:PreBlockSpin 修改\n锁消除 -XX:EliminateLocks\n去除不可能存在共享资源竞争的锁。\n如对局部变量的锁，因为局部变量在线程栈中私有，不可能被其它线程访问，就可以去除锁。\n再如多个String相加时，会转为StringBuffer对象的append操作，每个append方法都有个同步块。[JDK 1.7之前]\n逃逸分析：观察某个变量是否会逃逸出一个作用域。-XX:+DoEscapeAnalysis\n3.ThreadLocal # ThreadLocal 中填充的的是当前线程的变量，该变量对其他线程而言是封闭且隔离的，ThreadLocal 为变量在每个线程中创建了一个副本，这样每个线程都可以访问自己内部的副本变量。\n常见的场景：\n在spring事务中，保证一个线程下，一个事务的多个操作拿到的是一个Connection 在hiberate中管理session 在JDK8之前，为了解决SimpleDateFormat的线程安全问题 获取当前登录用户上下文 临时保存权限数据 使用MDC保存日志信息 public class ThreadLocalTest02 { public static void main(String[] args) { ThreadLocal\u0026lt;String\u0026gt; local = new ThreadLocal\u0026lt;\u0026gt;(); IntStream.range(0, 10).forEach(i -\u0026gt; new Thread(() -\u0026gt; { local.set(Thread.currentThread().getName() + \u0026#34;:\u0026#34; + i); System.out.println(\u0026#34;线程：\u0026#34; + Thread.currentThread().getName() + \u0026#34;,local:\u0026#34; + local.get()); }).start()); } } set 方法：\npublic void set(T value) { Thread t = Thread.currentThread();\t//首先获取当前线程对象 ThreadLocalMap map = getMap(t);\t//获取线程中变量 ThreadLocal.ThreadLocalMap if (map != null) map.set(this, value);\t//如果不为空，设置值 else //如果为空，初始化该线程对象的map变量，其中key 为当前的threadlocal 变量 createMap(t, value); } //初始化线程内部变量 threadLocals ，key 为当前 threadlocal void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } ThreadLocalMap 为 ThreadLocal 的一个静态内部类，里面定义了Entry 来保存数据。而且是继承的弱引用。在Entry内部使用ThreadLocal作为key （不是线程本身），使用我们设置的value作为value。\n对于每个线程内部有个ThreadLocal.ThreadLocalMap 变量，存取值的时候，也是从这个容器中来获取。\nget方法：\npublic T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } return setInitialValue(); } 内存泄露：\n如果 key 为 null 了，这个 entry 就可以清除了。ThreadLocal是一个弱引用，当为null时，会被当成垃圾回收。但是此时我们的ThreadLocalMap（thread 的内部属性）生命周期和Thread的一样，它不会回收，这时候就出现了一个现象。那就是ThreadLocalMap的key没了，但是value还在，这就造成了内存泄漏。\n解决办法：使用完ThreadLocal后，执行remove操作，避免出现内存溢出情况。归还线程之前记得清除ThreadLocalMap，要不然再取出该线程的时候，ThreadLocal变量还会存在。\nkey如果是强引用会导致线程不被回收，key对应ThreadLocal也不被回收，所以要改为弱引用。至于value一定是强引用，所以必须用完调用remove方法。\n4.无锁 # 使用CAS操作，CAS(V, E, N) 只有 $V = E$（期望值）时，将 $V$ 置为 $N$\n4.1 AtomicInteger、AtomicLong、AtomicBoolean、AtomicReference # 线程安全的各种操作。修改操作都是CAS实现的\n4.2 Unsafe类——Java中的指针 # 封装了一些类似指针的操作。如根据偏移量获取或设置对应位置上的数据。只能由Bootstrap类加载器加载的类使用。\n","externalUrl":null,"permalink":"/docs/java/juc/","section":"Docs","summary":"1.线程的基本操作 # Java 线程主流实现是使用内核线程 1:1 实现，每个","title":"JUC","type":"docs"},{"content":"https://juejin.cn/post/7094121178373029895\n三大核心功能：内存管理、解释执行虚拟机指令、即时编译\n一、组成 # 1.类加载器\n2.运行时数据区域\n3.执行引擎\n4.本地接口\n二、字节码文件 # 1.打开 # 推荐 jclasslib 软件\n2.组成 # 基础信息 # 魔数、字节码文件对应的Java版本号、访问标识、父类和接口\n魔数：文件的头几个字节，用于校验文件类型\n主版本号：标识大版本号，用于判断是否兼容——jdk1.2之后，jdk版本 = 大版本号 - 44\n副版本号：主版本号相同时，区分不同版本（一般不需理会）\n常量池 # 字符串常量、类或接口名、字段名\n字符串常量：避免重复内容重复定义，节省空间。\n常量池中的数据都有个编号（从 1 开始），在字段或字节码指令中可以通过编号找到对应的数据。\n字节码指令中通过编号找到对应的数据（符号引用）\n字段 # 当前类或接口声明的字段信息(字段名、描述符、访问标识)\n方法 # 方法的字节码指令（放在方法的Code属性中）\n常见指令：\niconst_0\t将0放入操作数栈\nistore_1\t将操作数栈中弹出一个放入局部变量表数组中位置1处\niload_1\t将局部变量表数组中位置1处复制到操作数栈中\niadd\t对出栈两个数求和后压栈\niinc 1 by 2\t将局部变量表数组中位置1处数据加2\nreturn 方法结束\nputstatic #2 \u0026lt;init/Demo1.value : I\u0026gt; 从操作数栈中获取值放到静态变量中\ni = i++; i = ++i; i++; 线程不安全，i += 1; 线程安全：\ni = i++ 先把 i 取出放到临时的操作数栈，再对 i 加1，最后又使用之前的操作数栈的值覆盖得到 i\niload_1 iinc 1 by 1 istore_1 i = ++i\t结果为 i+1\niinc 2 by 1 iload_2 istore_2 i++\niload_1 // 将i的值加载到栈顶 iinc 1, 1 // 对i进行自增 i += 1\nint i=0,j=0,k=0; i++;\t//iinc 1 by 1 j=j+1;\t//iload_2 //iconst_1 //iadd //istore_2 k+=1;\t//iinc 3 by 1 javap [字节码文件名称] 命令\n-v 属性 # 类的属性，如文件名、内部类的列表等\n三、类加载器 # 1.类的生命周期 # 加载\u0026ndash;连接（验证、准备、解析）\u0026ndash;初始化\u0026ndash;使用\u0026ndash;卸载\n加载 # 类加载器根据类的全限定名通过不同的渠道（本地文件、动态代理生成、网络传输）以二进制流的方式获取字节码信息。 将字节码中的信息保存到 方法区。 生成一个 InstanceKlass 对象，保存类的所有信息（包含实现特定功能如多态的信息） 在堆中生成一份类似的 java.lang.Class 对象 。用于在Java代码中获取类的信息以及存储静态字段的数据（JDK8之后） 连接 # 验证：验证内容是否满足Java虚拟机规范，一般不需要程序员参与\n1.文件格式验证\n2.元信息验证，如类必须有父类\n3.验证程序执行指令的语义\n4.符号引用验证\n版本号检测：\nreturn (major \u0026gt;= JAVA_MIN_SUPPORTED_VERSION) \u0026amp;\u0026amp;\t// 主版本号 JAVA_MIN_SUPPORTED_VERSION=45（JDK 1.0） (major \u0026lt;= max_version) \u0026amp;\u0026amp; // 支持的最高版本号 JDK8 中为52（JDK8） ((major != max_version) ||\t// 使用主版本号为最大主版本号 minor \u0026lt;= JAVA_MAX_SUPPORTED_MINOR_VERSION)\t// 副版本号 JAVA_MAX_SUPPORTED_MINOR_VERSION=0（JDK未使用） 准备：给静态变量分配内存（堆区）并赋初值\nint long short byte 0 boolean false double 0.0 char \u0026#39;\\u0000\u0026#39; 引用类型 null final 修饰的静态变量，会直接赋值\n解析：将常量池中的符号引用替换成指向内存的直接引用\ncp_info #6 转为 public class java.lang.Object @0x00000007c0000f28\n初始化 # 执行静态代码块中的代码（字节码文件中clinit部分），并为静态变量赋值\nclinit 方法中执行顺序和 Java 中的顺序一致\n会导致类的初始化：\n访问一个类的静态变量/静态方法 调用Class.forName(String className) new 一个该类的对象 执行Main方法的当前类 子类初始化会导致父类初始化，访问父类的静态变量，只初始化父类 变量是final修饰的且赋值的内容需要执行指令才能得出结果时 不会导致类初始化：\n数组的创建不会导致元素的类进行初始化 变量是final修饰的且等于常量的不会触发初始化（连接阶段） package other; public class jvm{ public static void main(String[] args) { System.out.println(\u0026#34;A\u0026#34;); new jvm(); new jvm(); } public jvm(){ // 构造函数——优先级3 System.out.println(\u0026#34;B\u0026#34;); } { // 构造代码块--会被放到构造函数中去，优先于构造函数——优先级2 System.out.println(\u0026#34;C\u0026#34;); } static{ // 静态代码块——只执行一次，优先级1 System.out.println(\u0026#34;D\u0026#34;); } } // DACBCB // 只会初始化一次--D // A -- 构造 CB 特定情况下clinit不会出现：\n无静态代码块且无静态变量赋值语句 有静态变量声明，无赋值 静态变量定义使用了final 代码块执行顺序：\n静态代码块 构造代码块 构造函数 父类先执行 2.类加载器 # 类加载器是Java虚拟机提供给应用程序去实现获取类和接口字节码数据的技术\n本地接口 JNI，允许Java调用其它语言编写的方法。在Hotpot类加载中，主要用于调用Java虚拟机中的方法，这些方法使用C++编写。\n1）类加载器的分类 # JDK8 及之前：\n使用 Java 实现\n拓展类加载器Extension：允许拓展Java中比较通用的类\n应用程序类加载器Application：加载应用使用的类\n使用 JVM 底层源码实现(C++)\n启动类加载器Bootstrap：加载Java中最核心的类\n自定义类加载器\n2）启动类加载器 # 默认加载 /jre/lib 下的类文件，如 rt.jar tools.jar 等\nClassLoader classLoader = String.class.getClassLoader(); System.out.println(classLoader);\t// null, 无法获取到启动类加载器 通过启动类加载器加载用户jar包：\n放到目录下（不推荐） 使用参数进行拓展 -Xbootclasspath/a:jar包目录/jar包名称 3）拓展类加载器和应用程序类加载器 # 都位于 sun.misc.Launcher 中，是静态内部类。继承自URLClassLoader。 拓展类加载器 # 默认加载 /jre/lib/ext 下的类文件\n​\t加载用户jar包：\n放入文件夹\n使用参数拓展 -Djava.ext.dirs=\u0026quot;jar包目录1;目录2\u0026quot; ; (windows使用 ; 来追加) :(macos/linux 使用 : 追加)\n应用程序类加载器\n加载 classpath 下的类文件\n​\t加载用户jar包：\n放入文件夹 使用参数拓展 -Djava.ext.dirs=\u0026quot;jar包目录\u0026quot; ; (windows使用 ; 来追加) :(macos/linux 使用 : 追加) 4）双亲委派机制 # 每个类加载器会先检查是否已经加载了该类，若已加载会直接返回，否则会将加载请求委派给父类加载器。最终会检查能否加载（在不在加载目录之下）。第二次加载也是向上查找自己有没有加载，没有就继续委派给父类加载。\n面试题：\nString类会被覆盖吗？ 不会，会被启动类加载器加载原来的 java.lang.String 类，之后加载到自定义的 java.lang.String 类时，会发现已经加载过了。 双亲委派机制的作用： 保证类加载的安全性 避免重复加载 使用代码主动加载一个类：\n使用Class.forName方法，使用当前类的类加载器去加载指定的类 获取到类加载器，通过类加载器的loadClass方法指定某个类加载器加载 每个类加载器中都保存了一个成员变量：父类加载器（上级而非继承）\nprivate final ClassLoader parent;\n启动类加载器是C++编写，无父类加载器 应用程序类加载器：拓展类加载器 拓展类加载器：null，但还是会将启动任务提交给启动类加载器 使用Arthas的 classloader -t 可以显示该树型结构\n+-BootstrapClassLoader +-jdk.internal.loader.ClassLoaders$PlatformClassLoader@3d16d686 +-com.taobao.arthas.agent.ArthasClassloader@9aea133 +-jdk.internal.loader.ClassLoaders$AppClassLoader@1d44bcfa 5）打破双亲委派机制 # A. 自定义类加载器 # Tomcat 使用自定义类加载器来实现应用之间类的隔离。每一个应用会有一个独立的类加载器加载对应的类。\nClassLoader 中的四个核心方法：\n// 类加载的入口，提供了双亲委派机制。内部会调用findClass public Class\u0026lt;?\u0026gt; loadClass(String name) // 由子类实现，获取二进制数据调用defineClass protected Class\u0026lt;?\u0026gt; findClass(String name) // 做类名校验(java.开头的包必须由启动类加载，否则报错)，调用JVM底层方法将字节码信息加载到虚拟机内存中 protected final Class\u0026lt;?\u0026gt; defineClass(String name, byte[] b, int off, int len) // 执行类的连接阶段 protected final void resolveClass(Class\u0026lt;?\u0026gt; c) Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (parent != null) { c = parent.loadClass(name, false); } else { c = findBootstrapClassOrNull(name); } if (c == null) { // 自己加载 } 自定义加载器的默认 parent 为启动类加载器\n不同类加载器加载的同名类不是同一个类\n一般不打破双亲委派机制：重写 findClass 方法\nB. 线程上下文类加载器 # JDBC使用了DriverManager 类由启动类加载器加载，但是他加载引入的jar包由（委派给）应用程序类加载器加载（打破了双亲委派机制）；另一种说法：JDBC通过初始化阶段触发了驱动类的加载，类的加载依然遵循双亲委派机制（因为没有重写loadClass方法）\nSPI机制：使用了线程上下文中保存的类加载器进行类的加载，这个类一般是应用程序类加载器\n在类路径下的META-INF/services 文件夹中，以接口的全限定名来命名文件名，对应的文件里面写该接口的实现。 使用 ServiceLoader 加载该类 @CallerSensitive public static \u0026lt;S\u0026gt; ServiceLoader\u0026lt;S\u0026gt; load(Class\u0026lt;S\u0026gt; service) { ClassLoader cl = Thread.currentThread().getContextClassLoader();// 也可以set为目标加载器 return new ServiceLoader\u0026lt;\u0026gt;(Reflection.getCallerClass(), service, cl); } C. Osgi框架的类加载器 # 存在同级之间的类加载器的委托加载。还实现了热部署的功能\n6）JDK9之后的类加载器 # 引入了模块的概念\n启动类加载器使用 Java 编写，位于jdk.internal.loader.ClassLoader 类中。BootStrap继承自URLClassLoader\u0026ndash;\u0026gt;BootClassLoader继承自 BuiltinClassLoader [实现从模块中找到要加载的字节码资源文件]。启动类加载器依然无法获取到，返回null 扩展类加载器被替换为平台类加载器（Platform Class Loader）。遵循模块化方式加载字节码文件，继承变为BuiltinClassLoader 。平台类加载器的存在更多是为了与老版本的设计方案兼容，自身没有特殊的逻辑 四、运行时数据区域 # 1.划分 # 线程不共享：程序计数器、Java 虚拟机栈、本地方法栈\n线程共享：方法区、堆\n2.PC # PC寄存器，每个线程会通过PC记录当前要执行的字节码指令的地址\n3.JVM栈 # 每个方法的调用使用一个栈帧保存。Java 虚拟机栈随着线程的创建而创建，在线程销毁时回收。\n栈帧组成：\n局部变量表 \u0026mdash;- 在方法执行过程中存放所有的局部变量。编译为字节码文件时就可以确定局部变量表的内容\n是一个数组，每一个位置称为槽（slot），long 和 double 占两个槽，其它类型占一个槽\n保存：实例方法的 this 对象，方法参数，方法体中声明的局部变量\n为了节省空间，局部变量表的槽可以复用，一旦某个局部变量不再生效，当前槽就可以再次被使用\n操作数栈 \u0026mdash;- 虚拟机在执行指令过程中用来存放临时数据的区域\n在编译期就可以确定操作数栈的最大深度，从而在执行时分配正确的内存大小\n帧数据 \u0026mdash;- 主要包含动态链接、方法出口、异常表的引用\n当前类的字节码指令引用了其他类的属性或者方法时，需要将符号引用（编号）转为对应的运行时常量池中的内存地址。动态链接保存了编号到内存地址的映射关系。\n方法在正确或异常结束时，当前栈帧会被弹出，同时PC应该指向上一个栈帧中的下一条指令\n异常的处理信息，包含了 try 代码块和 catch 代码块执行后跳转到的字节码指令位置\n栈溢出(StackOverflowError)：\n若不指定栈大小，JVM 将创建一个默认大小[取决于OS和体系结构]的栈\nWindows Linux BSD Solaris 基于OS默认值 x86(64位): 1MB ppc:2MB x86(64位):1MB 64位:1MB 虚拟机参数 -Xss栈大小[单位] 默认字节（必须1024倍数）、k或K(KB)、m或M(MB)、g或G(GB)\n或者 -XX:ThreadStackSize=1024 也可以设置\n$HotSpot JVM$ 要求： $180K \u0026lt; 栈内存 \u0026lt; 1024M$ 可以设置为 $256K$ 节省内存\n4.本地方法栈 # 存储 native 本地方法的栈帧。在 $HotSpot JVM$ 中，虚拟机栈和本地方法栈实现上使用了同一个栈空间。本地方法栈会在栈内存上生成一个栈帧，临时保存方法的参数，同时方便异常时把本地方法的栈信息打印出来。\n5.堆 # 创建的对象都存在堆上。堆内存一般最大。\n栈的局部变量表中，可以存放堆上对象的引用。静态变量也可以存放堆对象的引用，通过静态变量可以实现线程之间的共享。\n堆溢出(OutOfMemoryError)：默认的堆内存比较大，并不是used=max=total时才溢出（与垃圾回收有关）\n堆空间：Used \u0026ndash; 已使用的内存、Total \u0026ndash; 已分配的可用内存、Max \u0026ndash; JVM可以分配的最堆内存\nArthas：dashboard -i [刷新频率（毫秒）] 指令或者 memory 指令可以查看\n不设置任何参数，默认分配系统内存的1/4，total 默认 1/16\n设置参数：-Xmx值 -Xms值 分别设置 max 最大值（$\u0026gt; 2MB$）和初始的 total（$\u0026gt; 1MB$）。一般设置 max=total，减少了申请的开销。也不会将 total 收缩。\n获取到的值不一样是因为使用了JMX技术内存获取方式（与垃圾回收相关），计算的是可以分配对象的内存，部署整个内存\n6.方法区 # 元信息、运行时常量池、字符串常量池\nJDK7及之前：放在堆区的 永久代空间（ps_perm_gen） -XX =:MaxPermSize=值\nJDK8及之后：元空间（metaspace） ，位于OS维护的直接内存中，默认情况下只有不超过OS承受上限，可以一直分配\n-XX:MaxMetaspaceSize=值 将设置元空间最大值，推荐256M\n1）元信息 # 存储每个类的基本信息。一般称为 InstanceKlass 对象。在类的加载阶段完成。\n2）运行时常量池 # 当常量池加载到内存中之后，可以通过内存地址快速定位到常量池中的内容，这种常量池为运行时常量池。\n静态常量池 $\u0026mdash;加载、连接\u0026mdash;\u0026gt;$ 运行时常量池\nByteBuddy框架：\n引入依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12.23\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 创建ClassWriter对象\nClassWriter classWriter = new ClassWriter(0); 调用visit方法，创建字节码数据\nclassWriter.visit(Opcodes.V1_8, Opcodes.ACC_PUBLIC, name, null,\u0026#34;java/lang/Object\u0026#34;, null); byte[] bytes = classWriter.toByteArray(); 368974 Exception in thread \u0026#34;main\u0026#34; java.lang.OutOfMemoryError: Metaspace at java.base/java.lang.ClassLoader.defineClass1(Native Method) at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017) at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:878) at com.tcmatj.bytebuddytest.Demo.Demo1.main(Demo1.java:19) 3）字符串常量池 # 存储在代码中定义的常量字符串内容。\npublic class Test { public static void main(String[] args) { String s1 = new String(\u0026#34;abc\u0026#34;);\t// 编译后 \u0026#34;abc\u0026#34; 加载到静态常量池中 // 读取到内存后，自动加载到字符串常量池 // new 创建的对象会放到堆内存。所以 s1 指向堆中的 \u0026#34;abc\u0026#34; 对象 String s2 = \u0026#34;abc\u0026#34;;\t// 指向字符串常量池中的 \u0026#34;abc\u0026#34; System.out.println(s1 == s2); // false System.out.println(s1.equals(s2)); // true } } JDK7之前：存于永久代\nJDK7：堆\nJDK7之后：堆\npublic class Test1 { public static void main(String[] args) { String s1 = \u0026#34;a\u0026#34;;\t// 常量池 String s2 = \u0026#34;c\u0026#34;; String s3 = \u0026#34;ac\u0026#34;; String s4 = s1 + s2;\t// 底层实现调用了new，指向新的字符串对象 String s5 = \u0026#34;a\u0026#34; + \u0026#34;c\u0026#34;;\t// 指向常量池中 \u0026#34;ac\u0026#34; System.out.println(s4 == s3); // false System.out.println(s5 == s3); // true System.out.println(s4.equals(s3)); // true } } String.intern() 可以手动将字符串放入字符串常量池中\nJDK6及之前，会将第一次遇到的字符串复制到常量池中，返回指向常量池的地址\nJDK7之后，会把第一次遇到的字符串的引用放入字符串常量池，返回指向常量池的地址\npublic class Test2 { public static void main(String[] args) { // 不使用new String是会把这个值直接返到常量池 String s1 = new StringBuilder().append(\u0026#34;think\u0026#34;).append(\u0026#34;123\u0026#34;).toString(); String s2 = new StringBuilder().append(\u0026#34;nu\u0026#34;).append(\u0026#34;ll\u0026#34;).toString(); // \u0026#34;null\u0026#34; 会默认放入常量池 // JDK6\tJDK8\tSystem.out.println(s1.intern() == s1);\t// false\ttrue\tSystem.out.println(s2.intern() == s2);\t// false\tfalse\tchar[] f = {\u0026#39;a\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;d\u0026#39;}; String a = \u0026#34;asd\u0026#34;; // 常量池 String b = new String(\u0026#34;asd\u0026#34;); // 堆 String c = new String(f); // 堆 数组形式new String d = new String(f).intern(); // 常量池 数组形式new，使用intern方法 String b2 = new String(\u0026#34;asd\u0026#34;).intern(); // 常量池 } } 7.直接内存 # 不在 JVM 规范中，不属于 Java 运行时的内存区域。\n在 JDK 1.4 引入 NIO 机制，使用了直接内存，解决：\nJava 堆中对象回收时会影响对象的创建和使用 提升IO操作的效率。把文件读入直接内存（缓冲区）再把数据复制到 Java 堆中。现在可以直接引用直接内存，只需要保存一个对象引用，减少了复制数据的开销。 内存溢出： OutOfMemoryError: Direct buffer memory\n​\t-XX:MaxDirctMemorySize=值\n创建直接内存上的数据：ByteBuffer\nByteBuffer directBuffer = ByteBuffer.allocateDirect(size);\t// 分配内存空间 五、垃圾回收器 # 内存泄漏指的是不再使用的对象在系统中未被回收，内存泄漏的积累可能会导致内存溢出。\n垃圾回收器主要负责堆上的内存回收\n解决系统僵死的问题 性能优化 自动垃圾回收：\n降低程序员实现难度、降低对象回收bug的可能性 程序员无法控制内存回收的及时性 手动垃圾回收：\n回收及时性高，由程序员把控回收的时机 编写不当容易出现悬空指针、重复释放、内存泄漏等问题 1.方法区的回收-类 # 判断一个类可以被卸载，需要满足：\n此类的所有实例对象都已被回收，在堆中不存在该类的实例对象以及子类对象 加载该类的类加载器已被回收 该类对应的 java.lang.Class 对象没有在任何地方被引用 手动触发回收：\nSystem.gc(); //不一定会立即回收垃圾，只是发一个请求 开发环境中此场景较少。在 JSP 热部署等应用场景中，每个 jsp 文件对应唯一的类加载器，当一个 jsp 文件修改了，就直接卸载这个 jsp 类加载器。重新创建类加载器，重新加载 jsp 文件\n2.堆回收-对象 # Java 对象是否能被回收，是根据对象是否被引用来决定（不可达的对象也会被回收）\n栈帧中，局部变量表保存 new 出来对象的 引用\n1）引用计数法（Java未使用） # 维护一个引用计数器，对象被引用时 +1 ，取消引用时 -1\n实现简单 维护指针的开销 循环引用无法回收 虚拟机参数 ： -verbose:gc 查看垃圾回收\n2）可达性分析法（Java使用） # 可达性分析法将对象分为两类：垃圾回收的根对象（GC Root）和普通对象，对象与对象之间存在引用关系。\n思想：若某个对象可以由 GC Root 对象可达，则不可回收\nGC Root对象（四大类\u0026ndash;对象都在堆内）：\n线程对象 \u0026mdash;- 引用线程栈帧中的方法参数、局部变量等\n对象指向栈帧\n系统类加载器加载的 java.lang.Class 对象 \u0026mdash;- 引用类中的静态变量\n监视器对象，用来保存同步锁关键字持有的对象\nsynchronized(ClassA.class)\t// ClassA 对象不可回收\n本地方法调用时使用的全局对象\n查看：\nArthas heapdump命令将堆内存快照保存到本地磁盘 MAT 工具打开堆内存快照文件，选择GC Root功能查看所有的GC Root 3.其它引用 # 1.软引用 # 一个对象只有软引用关联时，程序内存不足就会将软引用的数据回收。常用于缓存，Java 提供了 SoftReference 类来实现软引用\n执行过程：\n使用软引用对象包装起来， new SoftReference\u0026lt;对象类型\u0026gt;(对象) 内存不足时，VM 尝试进行垃圾回收 仍不能解决问题时，回收软引用中的对象 依然内存不足，抛出 OutOfMemory 异常 当软引用对象也需要回收时，如何知道那些要被回收？\n队列机制：\n软引用创建时，通过构造器传入引用队列 new SoftReference\u0026lt;对象类型\u0026gt;(对象, 引用队列) 在软引用中包含的对象被回收时，该软引用对象会被放入引用队列 通过遍历引用队列，将软引用的强引用删除 2.弱引用 # 与软引用的区别：弱引用在包含的对象在垃圾回收时，不管内存够不够都会被直接回收。WeakReference 类，可以 get() 到包含的对象。主要用在ThreadLocal中使用\n3.虚引用 # 幽灵引用、幻影引用。不能通过虚引用获取到包含的对象。唯一的用途是当对象被垃圾回收时可以接收到对应的通知。 PhantomReference 类。直接内存中为了及时知道对象不再使用，使用了虚引用来实现\n4.终结器引用 # 对象需要被回收时，对象会被放置在 Finalizer 类中的引用队列中，并在稍后由一条 FinalizerThread 线程从队列获取对象，然后执行对象的 finalize 方法。\n4.垃圾回收算法 # 1.评价标准 # GC线程在部分阶段需要停止所有的用户线程。这个过程称之为 $Stop The World(STW)$。\n$吞吐量 = \\frac{执行用户代码时间}{执行用户代码时间 + GC时间}$，越高，垃圾回收的效率越高 最大暂停时间 $STW_{max}$ 堆使用效率\u0026mdash;复制算法会把堆内存一分为二，每次只能利用一半内存 上诉三者不可兼得：一般堆内存越大，最大暂停时间越长。减少最大暂停时间，就会降低吞吐量\n不同的垃圾回收算法适用于不同的场景\n2.回收算法 # a)标记清除算法 # $Step1$ 标记阶段：将所有存活的对象进行标记。使用可达性分析算法，从 GC Root 开始通过引用链遍历出所有存活对象\n$Step2$ 清除阶段：从内存中删除没有被标记的对象\n优点：实现简单\n缺点：1. 碎片化问题。在对象被删除后，出现很多细小的可用内存单元。\n​\t2. 分配速度慢。由于存在内存碎片，需要维护一个空闲链表，有可能每次遍历到链表末尾才能获得合适的空间\nb)复制算法 # $Step1$ 准备两块 From 空间和 To 空间，每次在对象分配阶段，只能使用其中一块空间（From空间）\n$Step2$ 在垃圾回收阶段，将 From 空间中存活对象复制到 To 空间。将两个空间的名字互换\n优点：1.吞吐量高。比标记-整理算法少了一次遍历的过程，但是需要进行对象移动\n​\t2.不会发生碎片化问题\n缺点：内存利用率低\nc)标记-整理算法 # $Step1$ 标记阶段：将所有存活的对象进行标记。使用可达性分析算法，从 GC Root 开始通过引用链遍历出所有存活对象\n$Step2$ 整理阶段：将存活对象移动到堆的一端\n优点：1.内存利用率高\n​\t2.不会发生碎片化问题\n缺点：整理阶段的效率不高\nd)分代垃圾回收算法 # 将上述GC算法组合使用。将内存划分为年轻代（对象存活时间短）[伊甸园区、幸存者区(2块)]、老年代\n-XX:+UseSerialGC\t使用分代GC\n-Xms 设置堆的最小和初始大小\u0026gt;=1MB -Xmx 设置堆的最大大小\u0026gt;=2M -Xmn 新生代的大小 -XX:SurivivorRatio 伊甸园区和幸存者区的比例，默认8:1:1 -XX:+PrintGCDetails 或 verbose:gc 打印 GC 日志 $Step1$ 创建出来的对象，先放入 Eden 伊甸园区\n$Step2$ 若 Eden 区满，触发年轻代的 GC，称为 Minor GC 或者 Young GC。Minor GC 会把需要需要 Eden 区中和 幸存者From区 需要回收的对象回收，没回收的放入幸存者To区。每次记录年龄，初始值为0，每次GC后 +1\n$Step3$ 年龄到达 15（最大的默认值） 后，对象晋升到老年代\n$Step4$ 老年代满，先尝试 Minor GC 还是不足，会触发 Full GC，会对整个堆进行垃圾回收。若无法回收掉老年代对象，对象继续放入老年代时，抛出 OutOfMemory 异常\n5.垃圾回收器 # 1.年轻代-Serial垃圾回收器 # 单线程 复制算法 单CPU下吞吐量非常出色 多CPU下吞吐量不如其它GC，堆若偏大，会让用户长时间等待 适用于客户端程序或者硬件配置有限的场景 2.老年代-SerialOld垃圾回收器 # -XX:+UseSerialGC 新时代、老年代都会使用串行回收器\n标记整理算法 单CPU下吞吐量非常出色 多CPU下吞吐量不如其它GC，堆若偏大，会让用户长时间等待 与Serial搭配使用，或者在CMS特殊情况下使用 3.年轻代-ParNew垃圾回收器 # -XX:UseParNewGC 本质上是对Serial的优化，使用多线程进行垃圾回收\n年轻代\n复制算法\n多CPU下停顿时间较短\n吞吐量和停顿时间不如G1，JDK9之后不建议使用\n适用于JDK8及之前，与CMS GC搭配使用\n4.老年代-CMS垃圾回收器 # CMS(Concurrent Mark Sweep) JDK14废弃\n-XX:UseConcMarkSweepGC 关注于系统的暂停时间，允许用户线程和GC线程在某些步骤中同时执行，减少用户线程的等待时间\n老年代\n标记清除算法\n用户体验好\n内存碎片化问题\n在Full GC时整理。使用-XX:CMSFullGCsBeforeCompaction=N 来调整N次Full GC后再整理\n退化问题\u0026ndash;退化为单线程\n若老年代内存不足，会退化为单线程回收老年代\n浮动垃圾问题\u0026ndash;回收不掉\n第四步时，用户新建的不需要的对象，不能立马回收，要等到下次清理\n适用于大型系统，用户请求数据量大、频率高的场景\n执行步骤：\n$Step1$ 初始标记，用极短时间标记处GC Root直接关联到的对象\n$Step2$ 并发标记，标记所有的对象，用户线程不需要暂停\n$Step3$ 重新标记，第二步可能会发生对象变化，存在错漏等情况\n$Step4$ 并发清理，清理死亡的对象，用户线程不需要暂停\n5.年轻代-Parallel Scavenge垃圾回收器 # JDK8 的默认回收器，多线程并行回收，自动调整堆内内存大小\n复制算法 吞吐量高，手动可控制吞吐量 会动态调整堆的参数 不能保证单次的停顿时间 适用于后台任务，不需要与用户交互，且容易产生大量对象。如大数据处理，大文件导出 6.老年代-Parallel Old垃圾回收器 # -XX:+UseParallelGC 或 -XX:+UseParallelOldGC 都会使用 PS + PO 的组合。\n允许手动设置最大暂停时间和吞吐量，官方建议不要设置堆内存的最大值，GC会根据最大暂停时间和吞吐量自动调整内存大小\n-XX:MaxGCPauseMillis=n 设置最大停顿毫秒数为n\n-XX:GCTimeRatio=n 设置吞吐量为n\n-XX:+UseAdaptiveSizePolicy 设置GC根据吞吐量和最大停顿毫秒数自动调整内存大小，默认开启\n标记整理算法 并发收集，多核下效率较高 暂停时间较长 与PS配套使用 7.G1(Garbage First)垃圾回收器 # JDK9之后的默认GC\n设计目标：\n支持巨大的堆空间回收，并有较高的吞吐量 支持多CPU并行垃圾回收 允许用户设置最大暂停时间 G1之前的GC，内存结构一般是连续的。G1的整个堆会被划分为多个大小相等的区域，称之为区Region，$Region大小 = \\frac{堆大小}{2048}$，也可以通过参数 -XX:G1HeapRegionSize=32m 指定，2的指数幂，范围1M-32M。分为Eden、Survivor、Old区，不要求是连续的。\n回收方式：\n年轻代回收（Young GC）、混合回收（Mixed GC）\n年轻代回收：\n回收Eden区和Survivor区中不用的对象。会导致STW。-XX:MaxGCPauseMillis=n 默认200，设置每次垃圾回收时的最大暂停时间毫秒数，GC会尽可能地保证暂停时间\n执行流程：\n$Step1$ 新创建的对象会放在Eden区。当G1判断年轻代不足（占总堆60%），无法分配对象时执行Young GC\n$Step2$ 标记Eden和Survivor区中的存活对象\n$Step3$ 根据配置的最大暂停时间选择某些区域将存活对象复制到一个新的Survivor区中(年龄+1)，清空这些区域\n会记录每次GC时平均耗时，作为下次GC的参考，计算最多可以回收多少个Region区域 $Step4$ Survivor区中存活对象会被搬运到另一个Survivor区\n$Step5$ 当某个存活对象的年龄达到阈值（默认15），将被放入老年代\n$Note$ 部分对象如果大小超过Region的一半，会直接放入老年代，称为Humongous区（有可能横跨多个Region）。\n$Step6$ 多次回收后，会出现很多Old老年代区，此时总堆占有率达到阈值（-XX:InitiatingHeapOccupancyPercent 默认40%）会触发混合回收MixedGC。回收所有年轻代和部分老年代以及大对象。[复制算法]\n混合回收：\n$Step1$ 初始标记，标记 GC Roots 引用的对象\n$Step2$ 并发标记，标记所有的对象，用户线程不需要暂停\n$Step3$ 最终标记，标记一些改变、漏标的对象\n$Step4$ 并发复制清理，不会产生内存碎片，用户线程不需要暂停\nG1对老年代的清理会选择存活度最低的区域来进行回收，可以保证回收效率最高。也是G1名称的由来。\nFull GC：\n如果清理过程中发现没有足够的空Region存放转移的对象，会出现Full GC。单线程执行标记-整理算法，此时会导致用户线程暂停。所有要尽量保证应该用的堆内存有一定多余的空间。\n总结：\n-XX:+UseG1GC JDK9之后默认打开 -XX:MaxGCPauseMillis=n 最大暂停毫秒数 复制算法 延迟可控 无内存碎片 并发标记的SATB算法效率高 JDK8之前不够成熟 适用于JDK8最新版本、JDK9之后建议默认使用 8.组合选择 # JDK8及之前：\nParNew + CMS（关注暂停时间）\nParallel Scavenge + Parallel Old（关注吞吐量）\nG1（JDK8之前不建议，较大堆[\u0026gt;6G]并且关注暂停时间）\nJDK9之后\nG1\n六、其他 # 1. GC调优思路 # 理解需求和问题，确定调优目标 定位具体的问题，确定是否真的有GC调优的必要 确定是 Minor GC 还是 Mixed GC 分析具体调整的参数、软硬件配置 G1： 新生代 并行复制算法，有STW 老年代 并发标记整理 -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintAdaptiveSizePolicy // 打印G1 Ergonomics相关信息 G1 调优的一个基本建议就是避免进行大量的 Humongous 对象分配，如果 Ergonomics 信息说明发生了这一点，那么就可以考虑要么增大堆的大小，要么直接将 region 大小提高\nArthas # 1.下载与启动 # curl -O https://arthas.aliyun.com/arthas-boot.jar java -jar arthas-boot.jar 选择应用 java 进程序号后回车\n2.退出 # 退出当前的连接，可以用 quit 或者 exit\n完全退出 arthas，可以执行 stop 命令\n3.命令列表 # dump -d [目标目录] [全类名] 命令：将已加载类的字节码文件保存到特定目录\njad [全类名] ：反编译已加载类的源码\n#jvm 相关 # dashboard - 当前系统的实时数据面板 getstatic - 查看类的静态属性 heapdump - dump java heap, 类似 jmap 命令的 heap dump 功能 jvm - 查看当前 JVM 的信息 logger - 查看和修改 logger mbean - 查看 Mbean 的信息 memory - 查看 JVM 的内存信息 ognl - 执行 ognl 表达式 perfcounter - 查看当前 JVM 的 Perf Counter 信息 sysenv - 查看 JVM 的环境变量 sysprop - 查看和修改 JVM 的系统属性 thread - 查看当前 JVM 的线程堆栈信息 vmoption - 查看和修改 JVM 里诊断相关的 option vmtool - 从 jvm 里查询对象，执行 forceGc #class/classloader 相关 # classloader - 查看 classloader 的继承树，urls，类加载信息，使用 classloader 去 getResource dump - dump 已加载类的 byte code 到特定目录 jad - 反编译指定已加载类的源码 mc - 内存编译器，内存编译.java文件为.class文件 redefine - 加载外部的.class文件，redefine 到 JVM 里 retransform - 加载外部的.class文件，retransform 到 JVM 里 sc - 查看 JVM 已加载的类信息 sm - 查看已加载类的方法信息 #monitor/watch/trace 相关 # 注意\n请注意，这些命令，都通过字节码增强技术来实现的，会在指定类的方法中插入一些切面来实现数据统计和观测，因此在线上、预发使用时，请尽量明确需要观测的类、方法以及条件，诊断结束要执行 stop 或将增强过的类执行 reset 命令。\nmonitor - 方法执行监控 stack - 输出当前方法被调用的调用路径 trace - 方法内部调用路径，并输出方法路径上的每个节点上耗时 tt - 方法执行数据的时空隧道，记录下指定方法每次调用的入参和返回信息，并能对这些不同的时间下调用进行观测 watch - 方法执行数据观测 #profiler/火焰图 # profiler - 使用async-profiler在新窗口打开对应用采样，生成火焰图 jfr - 动态开启关闭 JFR 记录 #鉴权 # auth - 鉴权 #options # options - 查看或设置 Arthas 全局开关 #管道 # Arthas 支持使用管道对上述命令的结果进行进一步的处理，如sm java.lang.String * | grep 'index'\ngrep - 搜索满足条件的结果 plaintext - 将命令的结果去除 ANSI 颜色 wc - 按行统计输出结果 #后台异步任务 # 当线上出现偶发的问题，比如需要 watch 某个条件，而这个条件一天可能才会出现一次时，异步后台任务就派上用场了，详情请参考这里\n使用 \u0026gt; 将结果重写向到日志文件，使用 \u0026amp; 指定命令是后台运行，session 断开不影响任务执行（生命周期默认为 1 天） jobs - 列出所有 job kill - 强制终止任务 fg - 将暂停的任务拉到前台执行 bg - 将暂停的任务放到后台执行 #基础命令 # base64 - base64 编码转换，和 linux 里的 base64 命令类似 cat - 打印文件内容，和 linux 里的 cat 命令类似 cls - 清空当前屏幕区域 echo - 打印参数，和 linux 里的 echo 命令类似 grep - 匹配查找，和 linux 里的 grep 命令类似 help - 查看命令帮助信息 history - 打印命令历史 keymap - Arthas 快捷键列表及自定义快捷键 pwd - 返回当前的工作目录，和 linux 命令类似 quit - 退出当前 Arthas 客户端，其他 Arthas 客户端不受影响 reset - 重置增强类，将被 Arthas 增强过的类全部还原，Arthas 服务端关闭时会重置所有增强过的类 session - 查看当前会话的信息 stop - 关闭 Arthas 服务端，所有 Arthas 客户端全部退出 tee - 复制标准输入到标准输出和指定的文件，和 linux 里的 tee 命令类似 version - 输出当前目标 Java 进程所加载的 Arthas 版本号 4. 热部署 # 重启之后，字节码文件会恢复，除非将class文件放入jar包中进行更新\n在问题服务器上部署一个arthas，启动\njad --source-only 类全限定名 \u0026gt; 目录/文件名.java\njad 命令反编译，可以使用vim来修改源码\nmc -c 类加载器的hashcode 目录/文件名.java -d 输出目录\nmc 命令编译修改过的代码\nretransform 类文件所在目录/文件名.class\n加载新的字节码，不能添加方法或字段，也不能更新正在执行中的方法\n","externalUrl":null,"permalink":"/docs/java/jvm/","section":"Docs","summary":"https://juejin.cn/post/7094121178373029895 三大核心功能：内存管理、解释执行虚拟机指令、即时编译 一、组","title":"JVM","type":"docs"},{"content":" 一、网络 # 1. 固定IP地址 # /etc/dhcpd.conf 文件\ninterface wlan0 static ip_address=192.168.1.110/24 # ip和子网掩码（可能被占用，就需要更改） static routers=192.168.1.1 # 网关 static domain_name_servers=192.168.1.1 # DNS地址 /etc/resolv.conf 文件 (DNS配置文件会自动加入)\n# Generated by resolvconf nameserver 10.11.248.114 nameserver 10.11.248.115 nameserver 192.168.1.1 2.开机启动 # 方法一、修改 /etc/rc.local 文件\nsudo nohup python -u /home/pi/Downloads/weather_pi/weather.py \u0026gt;\u0026gt; /home/pi/Downloads/weather_pi/result.txt 2\u0026gt;\u0026amp;1 \u0026amp; 方法二、在 /home/pi/.bashrc 文件的末尾添加启动命令文本。\necho Running at boot sudo python /home/pi/sample.py 方法三、把应用程序加入到 /etc/init.d 目录\n需要加入启动配置信息\thttps://wiki.debian.org/LSBInitScripts\n# /etc/init.d/sample.py ### BEGIN INIT INFO # Provides: sample.py # Required-Start: $remote_fs $syslog # Required-Stop: $remote_fs $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: Start daemon at boot time # Description: Enable service provided by daemon. ### END INIT INFO 确保这个脚本具有执行权限。\nsudo chmod +x sample.py sudo update-rc.d sample.py defaults 方法四、配置 systemd 文件 /lib/systemd/system/sample.service\n权限需要设置成 sudo chmod 644\n添加下面的内容：\n名为 Sample Service 的服务，我们希望在多用户环境下依然有效启动一次。 Type 设置为 idle 是为了确保其他一切都已加载之后再执行 [Unit] Description=My Sample Service After=multi-user.target [Service] Type=idle ExecStart=/usr/bin/python /home/pi/sample.py [Install] WantedBy=multi-user.target 方法五、crontab 文件\n指令\nservice cron start service cron restart service cron stop service cron status systemctl is-enabled cron.service --- 查看crond是否为开机自启动 systemctl enable cron.service --- 将服务设置为开启启动 systemctl disable cron.service --- 关闭服务开机自启动 内容\nSHELL=/bin/bash PATH=/sbin:/bin:/usr/sbin:/usr/bin MAILTO=root # 更多细节 man 4 crontabs # 计划任务定义的例子: # .---------------- 分 (0 - 59) # | .------------- 时 (0 - 23) # | | .---------- 日 (1 - 31) # | | | .------- 月 (1 - 12) # | | | | .---- 星期 (0 - 7) (星期日可为0或7) # | | | | | # * * * * * 执行的命令 * * * * * date \u0026gt;\u0026gt; /time.txt 2\u0026gt;\u0026amp;1 % stdin input 固定值 1 列表值 2,12,22 连续范围值\t1-7 步长值\t1-24/3 第一个%表示标准输入的开始,其余%表示换行符 定时语句解析工具 https://crontab.guru/\n易错例子 0 3 * * * command 每天三点执行\n0 0 1-7 * * command 每月的1-7每天零时执行\n0 0 * * 1 command 每星期的星期一零时执行\n0 0 1-7 * 1 command 每月的1-7和每星期一零时执行\n日志文件位置 /var/log/cron\n二、指令 # 常用指令 # ifconfig 显示网络设备信息\nls\n-l 显示详细列表\n-lh 人性化显示\n-a 显示所有文件\ncd\n~ home目录\n- 返回上次的路径\npwd 查看当前目录\nmkdir 创建文件夹(当前目录)\n-p 自动创建父目录\na/{c, .d} 指定目录下创建多个文件夹\ntouch 创建空文件(当前目录)\nrm 删除\nrm a -r 删除a目录\nrm * 删除当前目录下所有内容（不会删除隐藏文件）\nhistory\n命令 + --help 帮助文档\nman + 命令 部分帮助文档\ncp fileA positionB 复制文件A到路径B\n-a 复制所有内容\n-f 覆盖已存在的目标文件，不提示\n-i 覆盖前会询问\n-r 若给出的fileA为目录文件，递归复制目录下的子目录和文件\n-v 显示拷贝速度\nmv fileA fileB 移动/重命名\ncat 查看文件内容\nmore 查看文件内容，分页\n按空格 向下翻页\n按b 往回翻页\n按q 退出\ncurl https://www.example.com get请求 Web 服务器\n-A/-H 指定客户端的用户代理标头，即User-Agent\n-b \u0026lsquo;cookie参数\u0026rsquo; 发送 Cookie\n-d\t发送 POST 请求的数据体\t-d \u0026rsquo;login=em＆password=12\u0026rsquo; [-X POST] https://google.com/login\n​\t读取本地文本文件的数据，向服务器发送 -d \u0026lsquo;@data.txt\u0026rsquo; https://google.com/login\n-F\t向服务器上传二进制文件\t-F \u0026lsquo;file=@photo.png\u0026rsquo; https://google.com/profile\n-o 将服务器的回应保存成文件\t-o example.html https://www.example.com\n-O\t将服务器的回应保存成文件,文件名为url最后部分\n命令 \u0026gt; 文件 将命令结果覆盖到文件中\n命令 \u0026gt;\u0026gt; 文件 追加\ngrep 查找文件内容\n-n 显示内容的行号\n-i 不区分大小写\n-v 反向查找（查找不包含的）\nln -s fileA Link 给文件/目录创建软链接Link\n相当于快捷方式\nln fileA Link 硬链接Link(不能指向目录)\n与原文件同步更新\nfind 查找文件\nscp 用于 Linux 之间复制文件和目录\n是 secure copy 的缩写, scp 是 linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。scp 是加密的，rpc是不加密的，scp 是 rcp 的加强版\nscp localfile username@ip:folder\t# 复制文件到远程文件夹，保留名称，只需要输入密码 scp localfile username@ip:file\t# 复制文件到远程，修改名称，只需输入密码 scp localfile ip:folder\t# 复制文件到远程文件夹，保留名称，需要输入用户名和密码 scp localfile ip:file\t# 复制文件到远程，修改名称，需要输入用户名和密码 scp -r localfolder username@ip:folder\t# 复制本地文件夹到远程目录下，只需要输入密码 scp -r localfolder ip:folder\t# 复制本地文件夹到远程目录下，需要输入用户名和密码 awk 文本分析工具\nawk [参数] [处理内容] [操作对象] 查看修改代码行数：git log --author=\u0026#34;i_matijun\u0026#34; --oneline --shortstat | grep \u0026#34;files changed\u0026#34; | awk \u0026#39;BEGIN {plus=0; minus=0} {plus+=$4; minus+=$6} END {print \u0026#34;增加行数：\u0026#34; plus \u0026#34; 减少行数：\u0026#34; minus}\u0026#39; 查询每个人提交次数：git shortlog -s -n 内建变量：\nNF 当前处理行的字段个数【列数】\nNR 当前处理行的行号\n$0 表示当前处理的整行内容\n$n 表示当前处理行的第 n 个字段\nFILENAME 被处理的文件名称\nRS 行分隔符，预设值为 \\n\n参数示例：\n-F ':| ' 使用冒号和空格分隔一行 处理内容：一个或多个 `pattern {action}` 若 `pattern` 为非 0 值，则执行 `action` - `'$15 \u0026gt; 60 {print $12, $15}'` 分隔后第十五列【】大于60 时，格式化输出12列和15列的值 - `'NR==1,NR==3 {print}'` 输出 1 至 3 行内容 - `'NR==1;NR==3 {print}'` 输出 1 和 3 行内容 - `'(NR\u0026gt;=1)\u0026amp;\u0026amp;(NR\u0026lt;=3) {print}'` 输出 1 至 3 行内容 - `'BEGIN{sum=0;n=0} {n+=1;sum+=$15} END{print sum,n,sum/n}'` 求第 15 列 平均值 - `'/[^CT]all/'` 匹配正则表达式 \\[^CT]all - `'BEGIN { for (i = 1; i \u0026lt;= 5; ++i) print i }'` 使用循环 - `'+$1 { printf(\u0026quot;%10s | %4d\\n\u0026quot;, $3, $1) }'` 第一列非 0 则使用 printf 函数格式化输出。模式中的一元加号强制在数字上下文中对 $1 求值。 e 压缩、权限相关指令 # tar\ntar -czvf test.tar.gz file1 file2 \u0026hellip; 将文件打包为test.tar.gz\n-z 使用gzip压缩\n-j 使用bzip2压缩\n-v 显示指令执行过程\n-c 创建包\n-f 指定备份文件（必须在最后）\n-t 列出包的内容\ntar tf test.tar.gz -x 解压\n-C \u0026lt;目的目录\u0026gt; 切换到指定目录\ntar zxf test.tar.gz positionA/ gzip 对打包的文件压缩、解压缩\n-r 压缩.gz文件\n-d 解压\nzip -r 压缩\nunzip -d 解压\n压缩率：zip \u0026lt; gzip \u0026lt; bzip2\n通用性：zip \u0026gt; gzip \u0026gt; bzip2\nwhich 查看命令的位置\nsu 切换用户\npasswd UserName newPassword 修改密码\nwho 查看当前登录的用户\nreboot 重启（无需权限）\nshutdown (root权限)\n-r now\t重新启动\n-h now\t立即关机\n-h +10\t十分钟后关机\n-h 20:20\t定时关机\nchmod 权限设置\nrwx:读、写、可执行\n或者4210：读、写、可执行、无权限\nchmod 777 fileA\t将全部的权限赋给三组用户\nd表示是一个目录\n前三个：所有者的权限\tu\n中三个：所属组的权限\tg\n后三个：其他用户的权限 o\na 所有用户\n+ 添加权限\n- 删除权限\n= 设置权限\nchmod u+r fileA 将文件A的读权限添加给u\nsudo gpasswd -a \u0026lt;你的用户名\u0026gt; docker 将用户添加到docker组\n添加后重启会话, 这一步是必须的，否则因为 groups 命令获取到的是缓存的组信息，刚添加的组信息未能生效\n性能分析指令 # top [-] [d delay] [q] [c] [S] [s] [i] [n] [b]\n-d \u0026lt;秒数\u0026gt;：指定 top 命令的刷新时间间隔，单位为秒。 -n \u0026lt;次数\u0026gt;：指定 top 命令运行的次数后自动退出。 -p \u0026lt;进程ID\u0026gt;：仅显示指定进程ID的信息。 -u \u0026lt;用户名\u0026gt;：仅显示指定用户名的进程信息。 -H：在进程信息中显示线程详细信息。 -i：不显示闲置（idle）或无用的进程。 -b：以批处理（batch）模式运行，直接将结果输出到文件。 -c：显示完整的命令行而不截断。 -S：累计显示进程的 CPU 使用时间。 面板数据：\n%us：表示用户空间程序的cpu使用率（没有通过nice调度）\n%sy：表示系统空间的cpu使用率，主要是内核程序。\n%ni：表示用户空间且通过nice调度过的程序的cpu使用率。\n%id：空闲cpu\n%wa：cpu运行时在等待io的时间\n%hi：cpu处理硬中断的数量\n%si：cpu处理软中断的数量\n%st：被虚拟机偷走的cpu\nPR（优先级）：进程的优先级\nNI（Nice值）：进程的优先级调整值。\nVIRT（虚拟内存）：进程使用的虚拟内存大小。\nRES（常驻内存）：进程实际使用的物理内存大小。\nSHR（共享内存）：进程共享的内存大小。\nS：运行状态。\n- R 运行 - S 睡眠 - I 空闲 - D 磁盘休眠 （不可中断睡眠状态） - T 停止 - Z 僵尸 - X 死亡（几乎捕捉不到） TIME+：进程的累计 CPU 时间。\n交互：\nq 退出top界面 ?、h 进入帮助界面 m 切换内存信息显示 t 切换cpu状态显示 l 触发负载均值显示 1 显示cpu核数信息 L 定位关键字 n、# 显示最大任务数量 u、U 过滤所属用户的进程 R 根据进程号(PID)排序(顺序或逆序) H 触发线程显示 C 显示当前行、列所在总行列数的位置 up、down、left、right 上下移动行，左右移动列，通常配合C使用 home、end 跳转至第一行、最后一行 k 根据PID杀死对应的进程 Z、z 颜色设置，触发颜色 d、s 设置top刷新周期，默认3秒刷新一次 f、F 进入设置界面，可设置top显示的项目和排序规则\nnetstat\n-a 详细的网络状况\n-nu 当前户籍UDP连接状况\n-apu UDP端口号的使用情况\n-i 网卡列表\n-g 组播组的关系\n-s 网络统计信息\n-l 监听的套接口\ndu 显示目录或者文件所占空间\n-h 可读性\ndf 显示文件系统的磁盘使用情况统计(1k块的使用信息)\n-h 可读性\n\u0026ndash;total 显示总的使用情况\n-i 显示inode信息而非块使用量\nfree 显示内存状态\n-b -k -m 以B、KB、MB显示\n-h 合适的单位显示\n-s\u0026lt;间隔秒数\u0026gt; 持续观察内存使用状况\n-t 显示总内存（内存+交换）\niostat\njsp\n三、Vim # 进入输入模式 # i 插入光标前一个字符\nI 插入行首\na 插入光标后一个字符\nA 插入行末\no 向下新开一行\nO 向上新开一行\n移动光标 # h j k l 左下上右\nM 光标移动到中间行\nL 屏幕的最后一行\nn-G 移动到n行\nw 向后移动 b 向前移动\n{ } 按段上下移\nCtrl-d Ctrl-u 向上、下翻半屏\nCtrl-f Ctrl-b 向下、向上翻一屏\ngg 文件开头\tGG 文件末尾\n删除 # x 删除光标后一个字符\nX 删除前一个字符\ndd 删除当前行\tn-dd 删除n行\ndD 删除光标前的所有内容\ndw 删除光标开始的内容（包括光标）\n撤销 # u 撤销一步\nCtrl-r 反撤销\n重复命令 # .\n复制粘贴 # yy n-yy\np\n文本移动 # \u0026gt;\u0026gt; 文本行右移\n\u0026lt;\u0026lt; 文本行左移\n可视模式 # v 按字符移动，选中文本\nV 按行移动，选中文本\n替换操作 # r 替换当前字符\nR 替换当前光标之后的字符，直到按下ESC\n查找 # /\nn 下一个\nN 上一个\n","externalUrl":null,"permalink":"/docs/%E5%B7%A5%E5%85%B7/linux/","section":"Docs","summary":"一、网络 # 1. 固定IP地址 # /etc/dhcpd.conf 文件 interface wlan0 static ip_address=192.168.1.110/24 # ip和子网掩码（可","title":"Linux","type":"docs"},{"content":" 1. 基本使用 # 类型 备注 nil 空 boolean 布尔值 number 数字 string 字符串 table 表 x = 123 -- 全局变量 local y = 456 -- 本地变量 -- 方法： function hello() return \u0026#34;hello\u0026#34; end -- 迭代: for i = 1, 10 do print(i) end -- 条件 if a == 1 then print(\u0026#34;a=1\u0026#34;) else if a == 2 then print(\u0026#34;a=2\u0026#34;) else print(\u0026#34;a != 1 \u0026amp;\u0026amp; a != 2\u0026#34;) end -- 字符串拼接 print(\u0026#34;str1\u0026#34; .. \u0026#34; str2\u0026#34;) -- table——数组，索引从 1 开始 data = {1.0, 123, \u0026#34;redis\u0026#34;, true, false, hello} print(data[3]) -- 输出 redis -- table——hash map = {lua = 1990, js = 1995, python = 2000, ruby = 2005} print(\u0026#34;Python was created in \u0026#34; .. map[\u0026#34;python\u0026#34;]) -- 转x为十六进制数字，默认不传为十进制 tonumber(x, 16) -- math.floor() 四舍五入 -- math.ceil() 向上取整 -- string.len() 字符长度 2. Redis 中使用 # 2.1 eval 命令 # EVAL script numkeys key [key ...] arg [arg ...] \u0026gt; eval \u0026#34;return {KEYS[1],KEYS[2],ARGV[1],ARGV[2]}\u0026#34; 2 key1 key2 first second 1) \u0026#34;key1\u0026#34; 2) \u0026#34;key2\u0026#34; 3) \u0026#34;first\u0026#34; 4) \u0026#34;second\u0026#34; 2.2 evalsha 命令 # Redis Evalsha 命令根据给定的 sha1 校验码，执行缓存在服务器中的脚本。将脚本缓存到服务器的操作可以通过 SCRIPT LOAD 命令进行\nredis 127.0.0.1:6379\u0026gt; SCRIPT LOAD \u0026#34;return \u0026#39;hello dewu\u0026#39;\u0026#34; \u0026#34;0422b381d7dc6b66e9ddba1436cd3b392ac8d226\u0026#34; redis 127.0.0.1:6379\u0026gt; EVALSHA \u0026#34;0422b381d7dc6b66e9ddba1436cd3b392ac8d226\u0026#34; 0 \u0026#34;hello dewu\u0026#34; 2.3 script 命令 # script flush：清除所有的脚本缓存 script load：将脚本装入脚本缓存，不立即运行并返回其校验和 script exists：根据指定脚本校验和，检查脚本是否存在于缓存 script kill：杀死当前正在运行的脚本（防止脚本运行缓存，占用内存） 127.0.0.1:6379\u0026gt; script load \u0026#34;return \u0026#39;hello dewu\u0026#39;\u0026#34; \u0026#34;0422b381d7dc6b66e9ddba1436cd3b392ac8d226\u0026#34; 127.0.0.1:6379\u0026gt; script exists \u0026#34;0422b381d7dc6b66e9ddba1436cd3b392ac8d226\u0026#34; 1) (integer) 1 127.0.0.1:6379\u0026gt; evalsha \u0026#34;0422b381d7dc6b66e9ddba1436cd3b392ac8d226\u0026#34; 0 \u0026#34;hello dewu\u0026#34; 127.0.0.1:6379\u0026gt; script flush OK 127.0.0.1:6379\u0026gt; script exists \u0026#34;0422b381d7dc6b66e9ddba1436cd3b392ac8d226\u0026#34; 1) (integer) 0 3. 使用场景 # 3.1 分布式锁 # local key1 = KEYS[1] local arg1 = ARGV[1] local arg2 = ARGV[2] -- 为redis设置一个锁 local Flag = redis.call(\u0026#34;setnx\u0026#34;, key1, arg1) -- 设置失效时间 redis.call(\u0026#34;expire\u0026#34;, key1, arg2) return Flag 3.2 秒杀 # -- 接收参数 local user_id = KEYS[1] local goods_id = KEYS[2] -- 拼接字符串 local stock_key = \u0026#34;dewu:\u0026#34; .. goods_id .. \u0026#34;:stock\u0026#34; -- 秒杀商品库存key local users_key = \u0026#34;dewu:\u0026#34; .. goods_id .. \u0026#34;:users\u0026#34; -- 成功秒杀商品的用户集合key -- 判断用户是否已经成功秒杀过该商品，如果已经存在在集合中，说明已经成功秒杀该商品，直接返回标志2，防止重复抢购 local user_exists = redis.call(\u0026#39;sismember\u0026#39;, users_key, user_id) if tonumber(user_exists, 10) == 1 then return 2 end -- 获取当前商品库存，如果库存小于等于0，表名商品已经被抢购完了，否则库存-1，并将抢购成功的用户放入集合中 local left_goods_count = redis.call(\u0026#39;get\u0026#39;, stock_key) if tonumber(left_goods_count, 10) \u0026lt;= 0 then return 0 else redis.call(\u0026#39;decr\u0026#39;, stock_key) redis.call(\u0026#39;sadd\u0026#39;, users_key, user_id) end return 1 3.3 分布式限流 # 3.3.1 计数器 # 计数器限流的核心是 INCRBY 和 EXPIRE 指令，通常计数器算法容易出现不平滑的情况，瞬间的 qps 有可能超过系统的承载\n-- 获取调用脚本时传入的第一个 key 值（用作限流的 key） local key = KEYS[1] -- 获取调用脚本时传入的第一个参数值（限流大小） local limit = tonumber(ARGV[1]) -- 获取计数器的限速区间 TTL local ttl = tonumber(ARGV[2]) -- 获取当前流量大小 local curentLimit = tonumber(redis.call(\u0026#39;get\u0026#39;, key) or \u0026#34;0\u0026#34;) -- 是否超出限流 if curentLimit + 1 \u0026gt; limit then -- 返回 (拒绝) return false else -- 没有超出 value + 1 redis.call(\u0026#39;INCRBY\u0026#39;, key, 1) -- 如果 key 中保存的并发计数为 0，说明当前是一个新的时间窗口，它的过期时间设置为窗口的过期时间 if (curentLimit == 0) then redis.call(\u0026#39;EXPIRE\u0026#39;, key, ttl) end -- 返回 (放行) return true end 问题：若key已存在、未设置TTL且已超出限流，则一直都会返回拒绝\n3.3.2 固定窗口 # 3.3.3 滑动窗口 # ","externalUrl":null,"permalink":"/docs/redis/lua/","section":"Docs","summary":"1. 基本使用 # 类型 备注 nil 空 boolean 布尔值 number 数字 string 字符串 table 表 x = 123 -- 全","title":"Lua脚本","type":"docs"},{"content":" 1.作用 # 依赖管理 统一项目结构 项目构建 2.安装 # 下载 zip 文件 配置本地仓库 修改 conf/settings.xml 中的 \u0026lt;localRepository\u0026gt; 为一个指定目录 配置阿里云私服 修改 conf/settings.xml 中的 \u0026lt;mirros\u0026gt; 标签为： \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;/mirror\u0026gt; 配置环境变量:MAVEN_HOME 为 maven 的解压目录，并将 bin 目录加入 PATH 环境变量 3.Idea配置 # 1.配置当前工程 # 文件-\u0026gt;设置-\u0026gt;构建、执行、部署-\u0026gt;构建工具-\u0026gt;Mave 设置maven主路径、用户设置路径和仓库路径 文件-\u0026gt;设置-\u0026gt;构建、执行、部署-\u0026gt;构建工具-\u0026gt;Maven-\u0026gt;运行程序 将JRE设置为11版本 文件-\u0026gt;设置-\u0026gt;构建、执行、部署-\u0026gt;编译器-\u0026gt;Java编译器 将项目字节码版本设置为11版本\n2.全局配置 # 文件-\u0026gt;关闭文件 自定义 选择 所有设置，重复上述步骤\n4.坐标 # \u0026lt;!--mybatis起步依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis.spring.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 5.导入Maven项目 # 方式一、IDEA选择右侧Maven面板，点击 + ，选择对应项目的 pom.xml 文件 方式二、IDEA 文件-\u0026gt;项目结构-\u0026gt;模块-\u0026gt; + -\u0026gt;导入模块 ，选择对应项目的 pom.xml 文件 6.依赖 # 查找依赖的网站\n6.1 依赖传递 # 在pom文件右键选择图标（Diagrams）可以查看依赖传递 排除依赖，主动断开依赖的资源，无需指定版本，举个例子： \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 6.2 依赖范围 # 依赖的jar包，默认情况下，可以在任何地方使用。可以通过\u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt;设置作用范围 作用范围有：主程序范围（main）、测试程序范围（test）、是否参与打包（package指令范围内） scope值 主程序 测试程序 打包（运行） 范例 compile(默认) Y Y Y log4j test - Y - junit(单元测试) provided Y Y - servlet-api runtime - Y Y jdbc驱动 6.3 生命周期 # 有三套相互独立的生命周期：\nclean 清理工作 default 核心工作，如：编译、测试、打包、安装、部署等 site 生成报告、发布站点等 同一套生命周期中，运行后面的阶段时前面的阶段都会运行\n主要关注五个阶段：\nclean 中的 clean default 中的 compile、test、package、install clean：移除上次构建生成的文件 compile：编译项目源代码 test：使用合适的单元测试框架进行测试 package：将编译后的文件打包 install：安装项目到本地仓库 执行方式：\nidea 中点击 命令行 mvn clean ","externalUrl":null,"permalink":"/docs/%E5%B7%A5%E5%85%B7/maven/","section":"Docs","summary":"1.作用 # 依赖管理 统一项目结构 项目构建 2.安装 # 下载 zip 文件 配","title":"Maven","type":"docs"},{"content":"传统JDBC流程：\n1、使用JDBC编程需要链接数据库，注册驱动和数据库信息。 2、操作Connection，打开Statement对象。 3、通过Statement执行SQL语句，返回结果放到ResultSet对象。 4、使用ResultSet读取数据。 5、关闭数据库相关的资源。 import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; public class JdbcDemo{ public static void main(String[] args) throws Exception { Class.forName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); String username = \u0026#34;root\u0026#34;; String password = \u0026#34;root\u0026#34;; String url = \u0026#34;jdbc:mysql://localhost:3306/test?useUnicode=true\u0026amp;characterEncoding=UTF-8\u0026amp;useSSL=false\u0026#34;; Connection conn = DriverManager.getConnection(url,username,password); String sql = \u0026#34;insert into t_person values(\u0026#39;Tom\u0026#39;, 25, \u0026#39;138xxxxxxxx\u0026#39;, \u0026#39;北京\u0026#39;);\u0026#34;; PreparedStatement pstm = conn.prepareStatement(sql); int update = pstm.executeUpdate(); System.out.println(update); pstm.close(); conn.close(); } } 持久层框架，用于简化 JDBC 的开发 使用步骤：\n创建springboot工程，勾选MySQL驱动和MyBatis框架，数据库，实体类\n引入依赖，配置数据库连接信息\n\u0026lt;!-- Mybatis依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.5\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Mysql连接依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.21\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; #驱动类名称 spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver #数据库连接的url url: jdbc:mysql://localhost:3306/tlias #连接数据库的用户名 username: tc2023 #连接数据库的密码 password: 1234 编写SQL语句（注解/XML） IDEA Alt + Enter 选择注入语言为 MySQL\nxml写法：\n配置yaml文件 mybatis: #mapper配置文件 mapper-locations: classpath:mapper/*.xml type-aliases-package: com.sky.entity configuration: #开启驼峰命名 map-underscore-to-camel-case: true 编写xml文件 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34; \u0026gt; \u0026lt;mapper namespace=\u0026#34;com.sky.mapper.EmployeeMapper\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;pageQuery\u0026#34; resultType=\u0026#34;com.sky.entity.Employee\u0026#34;\u0026gt; select * from employee \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;name != null and name != \u0026#39;\u0026#39;\u0026#34;\u0026gt; and name like concat(\u0026#39;%\u0026#39;, #{name}, \u0026#39;%\u0026#39;) \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; order by create_time desc \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; 1.数据库连接池 # 容器，负责分配、管理数据库连接。允许应用程序重复使用现有的数据库连接，而不是再建立一个。释放空闲空闲时间超过最大空闲时间的连接，来避免因为没有释放连接而引起的数据库连接泄露。\n标准接口：DataSource 官方提供的数据库连接池接口，由第三方组织实现此接口 功能：获取连接 常见产品：C3P0 DBCP Druid Hikari qiehuan1阿里云数据库\n#驱动类名称 spring: datasource: driver-class-name: com.mysql.cj.jdbc.Driver #数据库连接的url url: jdbc:mysql://localhost:3306/tlias #连接数据库的用户名 username: tc2023 #连接数据库的密码 password: 1234 2. 原理 # 1、SQLSessionFactoryBuilder(构造器):它会根据配置信息或者代码生成SqlSessionFactory。 2、SqlSessionFactory(工厂接口)：依靠工厂生成SqlSession。 3、SqlSession(会话)：是一个既可以发送SQL去执行并且返回结果，也可以获取Mapper接口。 4、SQL Mapper:是由一个JAVA接口和XML文件(或注解)构成，需要给出对应的SQL和映射规则。SQL是由Mapper发送出去，并且返回结果。 #{}和${}的区别是什么？ # （1）mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值。\n（2）mybatis在处理${}时，就是把${}替换成变量的值。\n（3）使用#{}可以有效的防止SQL注入，提高系统安全性。原因在于：预编译机制。预编译完成之后，SQL的结构已经固定，即便用户输入非法参数，也不会对SQL的结构产生影响，从而避免了潜在的安全风险。\n（4）预编译是提前对SQL语句进行预编译，而其后注入的参数将不会再进行SQL编译。我们知道，SQL注入是发生在编译的过程中，因为恶意注入了某些特殊字符，最后被编译成了恶意的执行操作。而预编译机制则可以很好的防止SQL注入。\nlombok # IDEA自带这个插件\n能通过注解自动生成构造器、getter/setter、equals、hashcode、toString等方法，并可以自动化生成日志变量，简化java开发、提高效率\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 注解 作用 @Getter/@Setter 为所有的属性提供get/set方法 @ToString 自动生成toString方法 @EqualsAndHashCode 根据类所拥有的非静态字段重写equals方法和hashCode方法 @Data 等于上述所有注解加起来 @NoArgsConstructor 为实体类生成无参构造 @AllArgsConstructor 为实体类生成除static修饰的字段之外带有各参数的构造 ","externalUrl":null,"permalink":"/docs/mysql/mybatis/","section":"Docs","summary":"传统JDBC流程： 1、使用JDBC编程需要链接数据库，注册驱","title":"Mybatis","type":"docs"},{"content":" 1. 连接数据库 # 创建用户 create user 'tc2023' identified by '1234';\n授权所有权限： GRANT ALL PRIVILEGES ON tlias.* TO 'tc2023'@'%'; 刷新系统权限： flush privileges;\n登录\nmysql -utc2023 -p1234 mysql -utc2023 -p 后输入密码 退出： exit\n连接远程MySQL数据库：mysql -h192.168.150.101 -P3306(端口号) -utc2023 -p1234\n创建数据库： create database tlias;\n2. SQL # 结构化查询语言，语法：\nSQL可以单行或多行书写，以分号结尾 忽略空格，不区分大小写 注释：单行： \u0026ndash; 注释内容或 # 注释内容（MySQL） 多行：/* 注释内容 */\n分类：\nDDL 数据定义语言 DML 数据操作语言 DQL 数据查询语言 DCL 数据控制语言 创建数据库用户、控制数据库的访问权限 2.1 DDL # show databases; 查询所有数据库 select databases(); 查询当前数据库 use 数据库名称; create database [if not exists] 数据库名称; []不需要输入，里面的内容可选 drop database [if exists] 数据库名称; 类型 描述 char 定长字符串 varchar 变长字符串 tinyblob 不超过255个字符的二进制数据 0-255 tinytext 短文本字符串 blob 二进制形式的长文本数据 0-65535 text 长文本数据 mediumblob 二进制形式的中等文本数据 0-1677215 mediumtext 中等文本数据 longblob 二进制形式的极大文本数据 longtext 极大文本数据 日期类型 格式 date YYYY-MM-DD time HH:MM:SS year YYYY datetime YYYY-MM-DD HH:MM:SS使用now()可以生成现在的时间，固定的8字节，从1000-01-01 00:00:00到9999-12-31 23:59:59 timestamp YYYY-MM-DD HH:MM:SS 从 1970-01-01 00:00:01 到 2038-01-19 03:14:07 之间，使用32位整数存储 创建表 create table 表名( 字段1 字段类型 [约束] [comment 字段1注释], .... 字段n 字段类型 [约束] [comment 字段n注释] )[comment 表注释]; 约束：作用于表中字段上的规则 非空约束 not null 唯一约束 unique 主键约束 primary key 默认约束 default 如果未指定字段值，则采用默认值 外键约束 foreign key\ncreate table tb_user{ id int primary key comment \u0026#39;ID,唯一标识\u0026#39;, username varchar(20) not null unique comment \u0026#39;用户名\u0026#39;, name varchar(10) not null comment \u0026#39;姓名\u0026#39;, age int comment \u0026#39;年龄\u0026#39;, gender char(1) default \u0026#39;男\u0026#39; comment \u0026#39;性别\u0026#39; }comment \u0026#39;用户表\u0026#39;; show tables; 查询当前数据库所有表 desc 表名; 查询表结构 show create table 表名; 查询建表语句 修改表格： 10. alter table 表名 add 字段名 类型(长度) [comment 注释] [约束]; 添加字段 11. alter table 表名 modify 字段名 新数据类型(长度); 修改字段类型 12. alter table 表名 change 旧字段名 新字段名 类型(长度) [comment 注释] [约束]; 修改字段名和字段类型 13. alter table 表名 drop column 字段名; 删除字段 14. rename table 表名 to 新表名; 修改表名 15. drop table [if exists] 表名 删除表\n2.2 DML # 2.2.1 增 # insert into 表名(字段名1, 字段名2) values(值1, 值2); 指定字段添加数据 insert into 表名 values(值1, 值2,...); 全部字段添加数据 insert into 表名(字段名1, 字段名2) values(值1, 值2),(值1, 值2); 批量添加数据（指定字段） insert into 表名 values(值1, 值2, ...),(值1, 值2, ...); 批量添加数据（全部字段） Note # 插入数据时，顺序应一一对应 字符串和日期数据应该包含在引号中 插入数据大小应该在字段规定的范围内 2.2.2 删 # delete from 表名 [where 条件];\n如果没有条件，会修改整张表的数据 不能删除某一个字段的值，可以使用 update，将该字段的值置为 NULL 2.2.3 改 # update 表名 set 字段名1=值1, 字段名2=值2, ...[where 条件];\n如果没有条件，会修改整张表的数据 2.3 DQL # 关键词：SELECT/select 语法： 8. select 字段列表 from 表名列表 where 条件列表 group by 分组字段列表 having 分组后条件列表 order by 排序字段列表 limit 分页参数\n查询多个字段：select 字段1, 字段2, 字段3 from 表名; 查询所有字段：select * from 表名; 设置别名：select 字段1 [as 别名1], 字段2 [as 别名2] from 表名; 查询结果显示为别名 去除重复记录：select distinct 字段列表 from 表名; 条件查询： # 不等于 \u0026lt;\u0026gt; / != 且 and / \u0026amp;\u0026amp; 或 or / || 非 not / ! between '2000-01-01' and '2010-01-01' 介于两者日期之间 in (2, 3, 4) 多选一 like 模糊匹配（_ 匹配单个字符，% 匹配任意个字符） is null ifnull(referee_id, 0) 遇到null设置为0 length() 返回字符串 str 的字节数 char_length() 返回字符串 str 的长度 分组查询 # select 字段列表 from 表名列表 [where 条件列表] group by 分组字段列表 [having 分组后条件列表]\n字段列表只能是分组字段和聚合函数\n聚合函数 # select 聚合函数(字段列表) from 表名;\ncount 统计数量 统计可以使用count(*) count(字段) count(常量), 推荐使用count(*)\ncount(*)\t会统计值为 NULL 的行，而 count(列名)不会统计此列为 NULL 值的行\ncount(distinct col) 计算该列除 NULL 之外的不重复行数\nmax 最大值\nmin 最小值\navg 平均值\nsum 求和\nROUND(字段, 精度)\nwhere 与 having 的区别\n执行时机不同：where 是分组之前进行 过滤，不满足 where 条件，不参与分组； having 是分组之后对结果进行过滤 判断条件不同：where 不能对聚合函数进行判断 排序查询 # select 字段列表 from 表名列表 [where 条件列表] group by 分组字段列表 [having 分组后条件列表] order by 字段1 排序方式1, 字段2 排序方式2 ...\n排序方式 ASC 升序（默认） DESC 降序\n分页查询 # select 字段列表 from 表名 limit 起始索引, 查询记录数;\n起始索引由 0 开始，起始索引 = （查询页码 - 1） * 每页显示记录数 是方言，MySQL 中是 limit 查询的是第一页的数据，起始索引可以省略 函数 # if(表达式, tvalue, fvalue); 当表达式为 true 时，取值 tvalue case expr when value1 then result1 [when value2 then value2 ...][else result] end DATE_FORMAT(date, format) ：用于以不同的格式显示日期/时间数据。date 参数是合法的日期，format 规定日期/时间的输出格式，比如将 2019-01-02 转换成 2019-01 DATE_FORMAT(trans_date, '%Y-%m') lower() 转为小写、upper() 转为大写 concat(str1, str2) 首尾相连 group_concat(列名1 order by 列名2 separator 分隔符【如','】) 将多行的值拼接为一个字符串 substr(str, pos[,len]) 指定位置开始取字符串，len 可选【pos 从 1 开始】 length(str) 存储长度 char_length(str) 字符个数 reverse(str) 正则表达式 REGEXP # 打斜线需要两个，转义\nSELECT patient_id, patient_name, conditions FROM Patients WHERE conditions REGEXP \u0026#39;\\\\bDIAB1.*\u0026#39;; -- 匹配邮箱地址 leetcode 1517 select * from users where mail regexp \u0026#34;^[a-zA-Z][a-zA-Z_0-9./-]*\\\\@leetcode\\\\.com$\u0026#34; ^：匹配字符串的开始位置。例如，^hello会匹配以\u0026quot;hello\u0026quot;开头的字符串。\n$：匹配字符串的结束位置。例如，world$会匹配以\u0026quot;world\u0026quot;结尾的字符串\n.：匹配除换行符以外的任意字符。例如，a.b会匹配\u0026quot;a+b\u0026quot;、\u0026ldquo;a@b\u0026quot;等\n*：匹配前面的模式零次或多次。例如，a*b会匹配\u0026quot;b\u0026rdquo;、\u0026ldquo;ab\u0026rdquo;、\u0026ldquo;aab\u0026quot;等\n+：匹配前面的模式一次或多次。例如，a+b会匹配\u0026quot;ab\u0026rdquo;、\u0026ldquo;aab\u0026rdquo;、\u0026ldquo;aaab\u0026quot;等\n?：匹配前面的模式零次或一次。例如，a?b会匹配\u0026quot;b\u0026rdquo;、\u0026ldquo;ab\u0026rdquo;\n[]：定义字符集合。例如，[abc]会匹配\u0026quot;a\u0026quot;、\u0026ldquo;b\u0026rdquo;、\u0026ldquo;c\u0026quot;中的任意一个字符\n[^]：否定字符集合。例如，[^abc]会匹配除了\u0026quot;a\u0026rdquo;、\u0026ldquo;b\u0026rdquo;、\u0026ldquo;c\u0026quot;之外的任意字符\n\\d：匹配数字。等价于[0-9]\n\\w：匹配字母、数字或下划线。等价于[A-Za-z0-9_]\n\\s：匹配空白字符，包括空格、制表符、换行符等\n\\b：匹配单词边界。例如，\\btest\\b会匹配单独的单词\u0026quot;test\u0026rdquo;\n2.4 多表查询 # 2.4.0 集合操作 # 交 INTERSECT 并 UNION 差 EXCEPT CROSS JOIN 交叉连接 2.4.1 一对多 # 在数据库表中多的一方，添加字段，来关联一的一方的主键\n外键约束 # 物理外键\n创建表时指定 create table 表名( 字段名 数据类型, ... [constraint][外键名称] foreign key(外键字段名) references 主表(字段名) ); 建表后添加 alter table 表名 add constraint 外键名称 foreign key(外键字段名) references 主表(字段名); 缺点：\n影响增删改的效率（需要检查外键关系） 仅用于单节点数据库，不适用于分布式、集群场景 容易引发数据库死锁问题，消耗性能 逻辑外键 在业务逻辑中，解决外键关联\n2.4.2 一对一 # 多用于单表拆分，将一张表的基础字段放在一张表中，其他字段放在另一张表中，提升操作效率\n实现：在任意一方加入外键，关联另一方的主键，设置外键为唯一\n2.4.3 多对多 # 如一个学生可以选修多门课程，一门课程也可以供多个学生选择\n2.4.4 多表查询 # 连接查询\n内连接 相当于查询A、B交集部分数据 外连接 左外连接：查询左表所有数据 右外连接：查询右表所有数据 子查询 2.4.4.1 内连接 # 隐式内连接 select 字段列表 from 表1, 表2 where 条件; select tb_emp.name,tb_dept.name from tb_emp, tb_dept where tb_emp.dept_id = tb_dept.id; 显式内连接 select 字段列表 from 表1 [inner] join 表2 on 连接条件 ...; select tb_emp.name,tb_dept.name from tb_emp inner join tb_dept on tb_emp.dept_id = tb_dept.id; 给表起别名后只能使用别名： select e.name,f.name from tb_emp e inner join tb_dept f on e.dept_id = f.id; 2.4.4.2 外连接 # 左外连接 select 字段列表 from 表1 left [outer] join 表2 on 连接条件...; 右外连接 select 字段列表 from 表1 right [outer] join 表2 on 连接条件...; 2.4.4.3 子查询 # 嵌套查询\nselect * from t1 where column1 = (select column1 from t2 ...); 子查询的外部语句可以是 insert / update / select 分类：\n标量子查询：子查询返回值为单个值 列子查询：返回值为一列 行子查询：返回值为一行 表子查询：返回值为一个表 标量子查询\nselect * from tb_emp where dept_id = (select id from tb_dept where name = \u0026#39;教研部\u0026#39;); 列子查询\nselect * from tb_emp where dept_id in (select id from tb_dept where name = \u0026#39;教研部\u0026#39; or name = \u0026#39;咨询部\u0026#39;); 行子查询\nselect * from tb_emp where (entrydate,job) = (select entrydate,job from tb_emp where name = \u0026#39;韦一笑\u0026#39;); 表查询\nselect * from (select * from tb_emp where entrydate \u0026gt; \u0026#39;2006-01-04\u0026#39;) e ,tb_dept d where e.dept_id = d.id; 3. 事务 # 一组操作的集合，它是一个不可分割的工作单位。要么同时成功，要么同时失败\n开始事务 start transaction; sql语句序列... 提交事务 commit; 回滚事务 rollback; 四大特性:\n原子性 要么同时成功，要么同时失败 一致性 数据保持一致状态 隔离性 隔离机制，保证事务在不受外部并发操作影响的独立环境下运行 持久性 事务一旦提交或回滚，对数据库中的数据的改变就是永久的 4. 索引 # 帮助数据库高效获取数据的数据结构\n主键、唯一约束都会自动创建索引\ncreate [unique] index 索引名 on 表名(字段名, ...); 展示索引信息： show index from 表名; 删除索引: drop index 索引名 on 表名; 无索引：全表扫描 有索引：B + 树（默认）、哈希索引\n缺点：\n占用存储空间 降低增删改的效率 ","externalUrl":null,"permalink":"/docs/mysql/mysql%E8%AF%AD%E6%B3%95/","section":"Docs","summary":"1. 连接数据库 # 创建用户 create user 'tc2023' identified by '1234'; 授权所有权限： GRANT ALL PRIVILEGES ON tlias.* TO","title":"MySQL语法","type":"docs"},{"content":" 1. 索引 # 1.1 索引选择 # 1.1.1 哈希 # 使用哈希算法实现的索引虽然可以做到快速检索数据，但是没办法做数据高效范围查找，因此哈希索引是不适合作为 Mysql 的底层索引的数据结构。\n1.1.2 二叉查找树 # 普通的二叉查找树可以提供范围查找，但有个致命缺点：极端情况下会退化为线性链表，二分查找也会退化为遍历查找，时间复杂退化为 O（N），检索性能急剧下降。在数据库中，数据的自增是一个很常见的形式，如果采取二叉树这种数据结构作为索引，那上面介绍到的不平衡状态导致的线性查找的问题必然出现。\n1.1.3 红黑树 # 红黑树拥有不错的平均查找效率，也不存在极端的 O(n)情况，那红黑树作为 Mysql 底层索引实现是否可以呢？当数据是顺序插入时，树的形态一直处于“右倾”的趋势。从根本上上看，红黑树并没有完全解决二叉查找树虽然这个“右倾”趋势远没有二叉查找树退化为线性链表那么夸张\n1.1.4 AVL树 # 优点：\n不错的查找性能（O（logn）），不存在极端的低效查找的情况。 可以实现范围查找、数据排序 数据库查询数据的瓶颈在于磁盘 IO，如果使用的是 AVL 树，每一个树节点只存储了一个数据，一次磁盘 IO 只能取出来一个节点上的数据加载到内存里。所以设计数据库索引时需要首先考虑怎么尽可能减少磁盘 IO 的次数。\n1.1.5 B树 # 优点：\n优秀检索速度，时间复杂度：B 树的查找性能等于 O（h*logn），其中 h 为树高，n 为每个节点关键词的个数； 尽可能少的磁盘 IO，加快了检索速度； 可以支持范围查找。 1.1.6 B+树 # 不同：\n第一，B 树一个节点里存的是数据，而 B+树存储的是索引（地址），所以 B 树里一个节点存不了很多个数据，但是 B+树一个节点能存很多索引，B+树叶子节点存所有的数据。\n第二，B+树的叶子节点使用一个链表串联起来，便于范围查找。\n在 MySQL 中 B+ 树的一个节点大小为“1页”，也就是16k。之所以设置为一页，是因为对于大部分业务，一页就足够了： 首先InnoDB的B+树中，非叶子节点存的是key + 指针；叶子节点存的是数据行。 对于叶子节点，如果一行数据大小为1k，那么一页就能存16条数据；对于非叶子节点，如果key使用的是bigint，则为8字节，指针在mysql中为6字节，一共是14字节，则16k能存放 16 * 1024 / 14 = 1170 个索引指针。于是可以算出，对于一颗高度为2的B+树，根节点存储索引指针节点，那么它有1170个叶子节点存储数据，每个叶子节点可以存储16条数据，一共 1170 x 16 = 18720 条数据。而对于高度为3的B+树，就可以存放 1170 x 1170 x 16 = 21902400 条数据（两千多万条数据），也就是对于两千多万条的数据，我们只需要高度为3的B+树就可以完成，通过主键查询只需要3次IO操作就能查到对应数据。所以在 InnoDB 中B+树高度一般为3层时，就能满足千万级的数据存储，所以一个节点为1页，也就是16k是比较合理的。\n1.2 InnoDB与Myisam实现 # Innodb 创建表后生成的文件有：\nfrm:创建表的语句 idb:表里面的数据+索引文件 Myisam 创建表后生成的文件有\nfrm:创建表的语句 MYD:表里面的数据文件（myisam data） MYI:表里面的索引文件（myisam index） MyISAM 引擎把数据和索引分开了，一人一个文件，这叫做非聚集索引方式；\nMyISAM 在建表时以主键作为 KEY 来建立主索引 B+树，树的叶子节点存的是对应数据的物理地址:\nInnodb 引擎把数据和索引放在同一个文件里了，这叫做聚集索引方式。\n首先 InnoDB 会根据主键 ID 作为 KEY 建立索引 B+树，如左下图所示，而 B+树的叶子节点存储的是主键 ID 对应的数据，比如在执行 select * from user_info where id=15 这个语句时，InnoDB 就会查询这颗主键 ID 索引 B+树，找到对应的 user_name='Bob'。\n为表里某个字段加索引时 InnoDB 会怎么建立索引树？比如要给 user_name 这个字段加索引，那么 InnoDB 就会建立 user_name 索引 B+树，节点里存的是 user_name 这个 KEY，叶子节点存储的数据的是主键 KEY。注意，叶子存储的是**主键 KEY！**拿到主键 KEY 后，InnoDB 才会去主键索引树里根据刚在 user_name 索引树找到的主键 KEY 查找到对应的数据。\n1.3 索引失效 # 1.3.1 隐式类型转换造成索引失效 # 当操作符左右两边的数据类型不一致时，会发生隐式转换。\n当 where 查询操作符左边为数值类型时发生了隐式转换，那么对效率影响不大，但还是不推荐这么做。\n当 where 查询操作符左边为字符类型时发生了隐式转换，那么会导致索引失效，造成全表扫描效率极低。\n字符串转换为数值类型时，非数字开头的字符串会转化为0，以数字开头的字符串会截取从第一个字符到第一个非数字内容为止的值为转化结果。\n1.3.2 模糊查询只有左前缀使用索引 # 1.3.3 反向条件不走索引 != 、 \u0026lt;\u0026gt; 、 NOT IN、IS NOT NULL # 1.3.4 对条件计算(使用函数或者算数表达式)不走索引 # 使用函数计算不走索引，无论是对字段使用了函数还是值使用了函数都不走索引，解决办法通过应用程序计算好，将计算的结果传递给sql，而不是让数据库去计算\n1.3.5 or 只有两边都有索引才走索引 # 1.3.6 若不是按最左列开始查找，无法使用索引 # 1.3.7 跳过索引的某列，则只能使用之前的索引列 # 1.3.8 某列有范围查找，则之后的列无法使用索引查找 # 1.4 索引适用 # 1.4.1 全键值、键值范围、键前缀查找 # 1.4.2 匹配最左前缀（只使用索引的第一列） # 1.4.3 匹配列前缀（匹配某列的开头部分，也只使用索引的第一列） # 1.5 高性能索引 # 1.5.1 前缀索引、索引选择选 # 前缀索引只使用字段的前一部分字符进行索引——以求节省空间，但会降低选择选 $$ 索引选择选 = \\frac{不重复的索引值}{记录总数} $$ 选择性高的索引可以过滤掉更多的行，唯一索引的选择性 = 1性能最好\n计算合适的前缀数目：\n计算完整列的选择性\tSELECT COUNT(DISTINCT city)/COUNT(*) FROM table\n计算不同前缀长度的选择性\nSELECT COUNT(DISTINCT LEFT(city, 3))/COUNT(*) AS sel3, COUNT(DISTINCT LEFT(city, 4))/COUNT(*) AS sel4, COUNT(DISTINCT LEFT(city, 5))/COUNT(*) AS sel5, COUNT(DISTINCT LEFT(city, 6))/COUNT(*) AS sel6, COUNT(DISTINCT LEFT(city, 7))/COUNT(*) AS sel7 FROM table; 找出选择性提升不大的前缀长度作为前缀索引长度\n添加前缀索引 ALTER TABLE table ADD KEY(city(7))\n1.5.2 多列索引 # 为每列创建单独的索引：OR、AND的查询会使用 索引合并 策略【EXPLAIN 后在 Extra 可以看到 Using Union】。\n有时效果不错，但更多时候效果糟糕 通常需要在算法的缓存、排序和合并操作耗费大量CPU和内存资源。且不会计算到 查询成本cost 中，使得查询成本被低估，导致执行计划可能还不如直接全表扫描 为多列创建索引：\n将选择性高的放到索引最前列 1.5.3 聚簇索引 # 相互关联的数据保存在一起\n数据访问较之非聚簇索引快，因为索引和数据保存在一个B+树中\n使用主键查询可以做到覆盖查询\n插入速度严重依赖于插入顺序，若不是顺序插入会导致插入时间长、占空间大（页分裂和碎片导致）\n尽可能按主键顺序插入数据，尽可能按单调递增的聚簇键顺序插入\n更新代价高\n插入数据时可能导致页分裂\n二级索引访问需要两次索引查找，一次查找主键，第二次使用主键查找数据\n1.5.4 覆盖索引 # 一个索引包含所有需要查询的字段的值【EXPLAIN 后在 Extra 可以看到 Using index】\n索引条目远小于数据行大小，极大减少数据访问量 对IO密集的范围查找快 若有索引 A, B 主键 ID 则需要查询 A, B, ID 时就可以完成索引覆盖\n1.5.5 索引扫描排序 # MySQL两种生成有序的结果：排序操作、按索引扫描【EXPLAIN 后在 type 可以看到 index】\n当索引顺序和 ORDER BY 子句顺序一致，且列的排序方向一致（都倒序或正序）时，使用索引对结果排序\n联接多表查询，只有 ORDER BY 子句引用的字段全部在第一张表中，才能使用索引排序。\nORDER BY 也需要满足索引的最左前缀要求。特殊情况：\n前导列是常量（不能是范围查询） 1.5.6 冗余索引 # 一个索引是另一个索引的前缀索引。如对于索引(A, B)，索引(A) 就是冗余的。索引(A, ID) 也是冗余的，主键已经包含在二级索引中\n大多数情况下应尽量扩展已有的索引而不是新建索引。优化最简单的方法就是让索引能覆盖查询。\n1.5.7 重复索引、未使用的索引——建议删除 # 重复定义的索引，如给主键加上唯一约束、普通索引等，不会报错，但么必要\n未使用的索引——如只用于限制不重名的唯一索引\n2. 日志 # 2.1 redo log # InnoDB存储引擎独有的，它让MySQL拥有了崩溃恢复能力。比如 MySQL 实例挂了或宕机了，重启时，InnoDB存储引擎会使用redo log恢复数据，保证数据的持久性与完整性。\n硬盘上存储的 redo log 日志文件不只一个，而是以一个日志文件组的形式出现的，每个的redo日志文件大小都是一样的。\nInnoDB 将 redo log 刷到磁盘上有几种情况：\n事务提交：当事务提交时，log buffer 里的 redo log 会被刷新到磁盘（可以通过innodb_flush_log_at_trx_commit参数控制） log buffer 空间不足时：log buffer 中缓存的 redo log 已经占满了 log buffer 总容量的大约一半左右，就把这些日志刷新到磁盘上 事务日志缓冲区满：InnoDB 使用一个事务日志缓冲区（transaction log buffer）来暂时存储事务的重做日志条目。当缓冲区满时，会触发日志的刷新，将日志写入磁盘 Checkpoint（检查点）：InnoDB 定期会执行检查点操作，将内存中的脏数据（已修改但尚未写入磁盘的数据）刷新到磁盘，并且会将相应的重做日志一同刷新，以确保数据的一致性 后台刷新线程：InnoDB 启动了一个后台线程，负责周期性（每隔 1 秒）地将脏页刷新到磁盘，并将相关的重做日志一同刷新 正常关闭服务器：MySQL 关闭的时候，redo log 都会刷入到磁盘里去 innodb_flush_log_at_trx_commit 的值有 3 种，也就是共有 3 种刷盘策略：\n0：设置为 0 的时候，表示每次事务提交时不进行刷盘操作。这种方式性能最高，但是也最不安全，因为如果 MySQL 挂了或宕机了，可能会丢失最近 1 秒内的事务 1：默认值，表示每次事务提交时都将进行刷盘操作。这种方式性能最低，但是也最安全，因为只要事务提交成功，redo log 记录就一定在磁盘里，不会有任何数据丢失 2：设置为 2 的时候，表示每次事务提交时都只把 log buffer 里的 redo log 内容写入 page cache（文件系统缓存）。page cache 是专门用来缓存文件的，这里被缓存的文件就是 redo log 文件。这种方式的性能和安全性都介于前两者中间 2.2 binlog # binlog 是逻辑日志，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于MySQL Server 层。不管用什么存储引擎，只要发生了表数据更新，都会产生 binlog 日志\n数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性\nbinlog 日志有三种格式，可以通过binlog_format参数指定。\nstatement\n记录的内容是SQL语句原文。同步数据时，会执行记录的SQL语句，但是有个问题，update_time=now()这里会获取当前系统时间，直接执行会导致与原库的数据不一致\nrow\n不再是简单的SQL语句了，还包含操作的具体数据。通常情况下都是指定为row，这样可以为数据库的恢复与同步带来更好的可靠性\nmixed\n内容是前两者的混合。MySQL会判断这条SQL语句是否可能引起数据不一致，如果是，就用row格式，否则就用statement格式\n2.3 两阶段提交 # 为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用两阶段提交方案。\n原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交。\n写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。\nredo log设置commit阶段发生异常，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。\n2.4 undo log # 要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。\n另外，MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改\n3. 多版本并发控制（MVCC） # MVCC 是一种并发控制机制，用于在多个并发事务同时读写数据库时保持数据的一致性和隔离性。它是通过在每个数据行上维护多个版本的数据来实现的。当一个事务要对数据库中的数据进行修改时，MVCC 会为该事务创建一个数据快照，而不是直接修改实际的数据行。\n1、读操作（SELECT）：\n当一个事务执行读操作时，它会使用快照读取。快照读取是基于事务开始时数据库中的状态创建的，因此事务不会读取其他事务尚未提交的修改。具体工作情况如下：\n对于读取操作，事务会查找符合条件的数据行，并选择符合其事务开始时间的数据版本进行读取。 如果某个数据行有多个版本，事务会选择不晚于其开始时间的最新版本，确保事务只读取在它开始之前已经存在的数据。 事务读取的是快照数据，因此其他并发事务对数据行的修改不会影响当前事务的读取操作。 2、写操作（INSERT、UPDATE、DELETE）：\n当一个事务执行写操作时，它会生成一个新的数据版本，并将修改后的数据写入数据库。具体工作情况如下：\n对于写操作，事务会为要修改的数据行创建一个新的版本，并将修改后的数据写入新版本。 新版本的数据会带有当前事务的版本号，以便其他事务能够正确读取相应版本的数据。 原始版本的数据仍然存在，供其他事务使用快照读取，这保证了其他事务不受当前事务的写操作影响。 3、事务提交和回滚：\n当一个事务提交时，它所做的修改将成为数据库的最新版本，并且对其他事务可见。 当一个事务回滚时，它所做的修改将被撤销，对其他事务不可见。 4、版本的回收：\n为了防止数据库中的版本无限增长，MVCC 会定期进行版本的回收。回收机制会删除已经不再需要的旧版本数据，从而释放空间。\nMVCC 通过创建数据的多个版本和使用快照读取来实现并发控制。读操作使用旧版本数据的快照，写操作创建新版本，并确保原始版本仍然可用。这样，不同的事务可以在一定程度上并发执行，而不会相互干扰，从而提高了数据库的并发性能和数据一致性。\n3.1 隐藏字段、undo log # InnoDB 存储引擎为每行数据添加了三个隐藏字段:\nDB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务 id。此外，delete 操作在内部被视为更新，只不过会在记录头 Record header 中的 deleted_flag 字段将其标记为已删除 DB_ROLL_PTR（7字节） 回滚指针，指向该行的 undo log 。如果该行未被更新，则为空 DB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该 id 来生成聚簇索引 在 InnoDB 存储引擎中 undo log 分为两种：insert undo log 和 update undo log：\ninsert undo log：指在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见，对其他事务不可见，故该 undo log 可以在事务提交后直接删除。\nupdate undo log：update 或 delete 操作中产生的 undo log。该 undo log可能需要提供 MVCC 机制，因此不能在事务提交时就进行删除。提交时放入 undo log 链表，等待 purge线程 进行最后的删除。不同事务或者相同事务的对同一记录行的修改，会使该记录行的 undo log 成为一条链表，链首就是最新的记录，链尾就是最早的旧记录。\n3.2 ReadView # class ReadView { /* ... */ private: trx_id_t m_low_limit_id; /* 大于等于这个 ID 的事务均不可见 */ trx_id_t m_up_limit_id; /* 小于这个 ID 的事务均可见 */ trx_id_t m_creator_trx_id; /* 创建该 Read View 的事务ID */ trx_id_t m_low_limit_no; /* 事务 Number, 小于该 Number 的 Undo Logs 均可以被 Purge */ ids_t m_ids; /* 创建 Read View 时的活跃事务列表 */ m_closed; /* 标记 Read View 是否 close */ } 3.3 数据可见性 # 在 InnoDB 存储引擎中，创建一个新事务后，执行每个 select 语句前，都会创建一个快照（Read View），快照中保存了当前数据库系统中正处于活跃（没有 commit）的事务的 ID 号。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务 ID 列表（即 m_ids）。当用户在这个事务中要读取某个记录行的时候，InnoDB 会将该记录行的 DB_TRX_ID（最后一次更新的事务id） 与 Read View 中的一些变量及当前事务 ID 进行比较，判断是否满足可见性条件：\n如果记录 DB_TRX_ID \u0026lt; m_up_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之前就提交了，可见 如果 DB_TRX_ID \u0026gt;= m_low_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之后才修改该行，所以该记录行的值对当前事务不可见。跳到步骤 6 若 DB_TRX_ID = m_creator_trx_id，表示是本事务修改，可见 m_ids 为空，则表明在当前事务创建快照之前，修改该行的事务就已经提交了，可见 如果 m_up_limit_id \u0026lt;= DB_TRX_ID \u0026lt; m_low_limit_id 表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照的时候可能处于**“活动状态”或者“已提交状态”**；所以就要对活跃事务列表 m_ids 进行查找（源码中是用的二分查找，因为是有序的） 如果在活跃事务列表 m_ids 中能找到 DB_TRX_ID，表明：① 在当前事务创建快照前，该记录行的值被“DB_TRX_ID事务”修改了，但没有提交；或者 ② 在当前事务创建快照后，该记录行的值被“DB_TRX_ID事务”修改了。这些情况下，这个记录行的值对当前事务都是不可见的。跳到步骤 6 在活跃事务列表中找不到，则表明“DB_TRX_ID事务”在修改“该记录行的值”后，在“当前事务”创建快照前就已经提交了，可见 在该记录行的 DB_ROLL_PTR 指针所指向的 undo log 取出快照记录，用快照记录的 DB_TRX_ID 跳到步骤 1 重新开始判断，直到找到满足的快照版本或返回空 3.4 MVCC只能在读取已提交和可重复读两个级别使用 # 在事务隔离级别 读取已提交/不可重复读 RC 和 可重复读 RR （InnoDB 存储引擎的默认事务隔离级别）下，InnoDB 存储引擎使用 MVCC（非锁定一致性读），但它们生成 Read View 的时机却不同\n在 RC 隔离级别下的 每次select 查询前都生成一个Read View (m_ids 列表)——因此造成不可重复读 在 RR 隔离级别下只在事务开始后 第一次select 数据前生成一个Read View（m_ids 列表） 3.5 MVCC➕Next-key-lock 防止幻读 # InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock （间隙锁+记录锁）来解决幻读问题：\n1、执行普通 select，此时会以 MVCC 快照读的方式读取数据\n在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”\n2、执行 select\u0026hellip;for update/lock in share mode、insert、update、delete 等当前读\n在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！InnoDB 使用 Next-key Lock 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读\n4. 事务隔离级别 # READ-UNCOMMITTED(读取未提交) ：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交) ：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读) ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化) ：最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）\nInnoDB 实现的 REPEATABLE-READ 隔离级别其实是可以解决幻读问题发生的，主要有下面两种情况：\n快照读：由 MVCC 机制来保证不出现幻读。 当前读：使用 Next-Key Lock 进行加锁来保证不出现幻读，Next-Key Lock 是行锁（Record Lock）和间隙锁（Gap Lock）的结合，行锁只能锁住已经存在的行，为了避免插入新行，需要依赖间隙锁。 ","externalUrl":null,"permalink":"/docs/mysql/mysql%E5%8E%9F%E7%90%86/","section":"Docs","summary":"1. 索引 # 1.1 索引选择 # 1.1.1 哈希 # 使用哈希算法实现的索引虽然可以做","title":"MySQL原理","type":"docs"},{"content":" 1、方法与函数的区别 # 方法作用于对象，函数大多需要传入参数\n方法：对象. 方法名()\n函数：函数名(参数1,\u0026hellip;)\n2、字符串： # rstrip()/lstrip()/strip() 方法：剔除两端的空白\n拼接变量时使用 str() 函数将变量转换为字符串\ntitle() 方法将首字母变为大写\ninput()函数输入的均为字符串，要获取数值输入，使用类型转换，() 内可以添加输入提示\nreplace('A','B') 方法替换全部符合的字符，A 都被替换为 B\n3、运算 # ** 乘方\n// 整除\nand or not 布尔表达式\nint() 函数，转换类型为 int\nenumerate(list) 函数，获取每个元素的索引及其值\n4、列表 # 访问：\n后第 1 个元素：listname[-1]\n添加：\n末尾添加元素：listname.append(\u0026quot;hei\u0026quot;) 中间插入元素：listname.insert(0,\u0026quot;hello\u0026quot;) 在 0 位置插入元素 删除：\ndel listname[0] 删除 0 位置的元素 deleted = listname.pop() 删除末尾元素，返回被删除的元素 deleted = listname.pop(0) 删除 0 位置的元素，返回被删除的元素 listname.remove('hello') 从列表删除一个 hello 排序：\nlistname.sort() 永久排序，按字母顺序，可以添加参数 reverse=True 按字典反序排列 listname.sorted() 临时排序 listname.reverse() 永久地反转列表 len(listname) 计算长度 range() 函数\nrange(1,5) 输出1~4的列表 range(1,10，2) 输出1~8的列表，步幅为2 列表解析：listname = [x**2 + 1 for x in range(1, 10, 2)] 切片\nlistname[0:3] 得到 0~2 位置的子列表\nlistname[2:] 得到 2~end 位置的子列表\nlistname[:] 得到列表的副本，复制列表时使用，若使用 listname1 = listname2 则代表两个列表都指向一个列表\n5、元组 # 不能修改的列表，可以重新赋值 删除元组 del tuplename tuplename.count(50) 50在元组中出现的次数 tuplename.index(44) 44在元组中的索引值 tuplename.index(44,4) 44在元组中的索引值，从4开始查找 6、字典 # 存放键值对\n创建空字典 dict = {}\n添加键值对 dict[key] = value\n删除键值对 del dict[key]\n遍历字典 for key,value in dict.items() items() 方法返回键值对列表、乱序\n遍历键 for key in dict.keys() 或 for key in dict 、乱序\n按顺序遍历键 for key in sorted(dict.keys())\n按顺序遍历值 for value in sorted(dict.values())\n7、循环 # for\nfor item in items: for i in range(8): while\nwhile count \u0026lt;= 9: continue break\n8、函数 # 实参、形参\n函数中使用的是传入的形参，是实参的副本，不会改变实参的内容\n传递列表时，传递的是列表本身，如不想改变列表值，需要传入副本，使用切片 [:]\n默认值\n返回值\n任意数量的实参\ndef function(*args):# * 创建一个空元组 ... 如果有其他实参，则放到之前\n任意数量的关键字实参\ndef function(arg1, arg2, **args):\t# ** 创建一个空字典 ... 9、导入 # import pygame from module_name import function_0, function_1 import matplotlib.pyplot as plt\t# 导入pyplot模块后取别名 from module_name import * # 导入所有函数 from module_name import Class_name # 导入类 10、类 # 例子：\nclass Animal(): \u0026#34;\u0026#34;\u0026#34;文档字符串，对类的功能进行描述\u0026#34;\u0026#34;\u0026#34; def __init__(self, name, age):\t# 初始化，实例化时会调用 self.name = name self.age = age def fly(self): print(self.name + \u0026#34; is flying\u0026#34;) class Dog(Animal):\t# 继承动物类 \u0026#34;\u0026#34;\u0026#34;文档字符串，对类的功能进行描述\u0026#34;\u0026#34;\u0026#34; def __init__(self, name, age): \u0026#34;\u0026#34;\u0026#34;初始化属性name和age\u0026#34;\u0026#34;\u0026#34; super().__init__(name, age) self.sick_num = 0\t# 有默认值的属性，可以不加到参数列表 self.clothes = Clothes(\u0026#39;T-shirt\u0026#39;)\t# 将实例用作属性 def sit(self):\t# 类中的函数--方法 \u0026#34;\u0026#34;\u0026#34;模拟小狗被命令时蹲下\u0026#34;\u0026#34;\u0026#34; print(self.name.title() + \u0026#34; is now sitting.\u0026#34;) def fly(self):\t# 重写父类方法 print(\u0026#34;Dog doesn\u0026#39;t fly!\u0026#34;) 创建实例\\调用方法 my_dog = Dog(\u0026#39;Tuantuan\u0026#39;, 3) my_dog.sit() my_dog.clothes.show_all_clothes() 规范\n类名应采用驼峰命名 每个类都应该在类定义后包含一个文档字符串 使用一个空行分隔方法，两个空行分隔类 导入完标准库后使用一个空行分隔后再导入自己编写的模块 静态方法与类方法：\n类方法用于定义类的公共方法，不需要传入 self，不需要实例化类就可以调用\n静态方法不传入 类 和 self，不需要实例化类就可以调用\nclass MyClass: @classmethod def class_method(cls): print(\u0026#34;This is a class method.\u0026#34;) @staticmethod def static_method(): print(\u0026#34;This is a static method.\u0026#34;) def instance_method(self): print(\u0026#34;This is an instance method.\u0026#34;) # 调用类方法 MyClass.class_method() # 调用静态方法 MyClass.static_method() # 创建类的实例并调用实例方法 obj = MyClass() obj.instance_method() 11、文件 # 读取文件\nopen() 函数\nopen(file, mode, encoding='utf-8')\n路径，在 Linux 和 OS X 中使用 / ，在 Windows 中使用 \\\nwith 关键字在不需要访问文件后将其关闭 # read() 方法读取文件的全部内容，作为一个长字符串，文件末尾会多出一个空行，可以使用 rstrip() 方法删除空行\nwith open(\u0026#39;file_name.txt\u0026#39;) as file_object: contents = file_object.read() print(contents.rstrip()) 逐行读取\n每一行都有一个换行符，会使得多打印出一个换行符，可以使用 rstrip() 方法删除空行\nfilename = \u0026#39;file_name.txt\u0026#39; with open(filename) as f: for line in f: print(line.rstrip()) 一次读取所有行，生成一个列表存储\nwith open(filename) as f: lines = f.readlines() 写入文件\n如果写入的文件不存在，open 函数会创建它\n若文件已存在，则会清空文件\nwith open(filename, \u0026#39;w\u0026#39;) as f: f.write(\u0026#34;I Love You!\u0026#34;) 'w' 写入模式\n'r' 只读模式，默认的模式\n'a' 附加模式\n'r+' 读取写入模式\nwrite 函数不会在末尾增加换行符\n12、异常 # 避免用户看到Traceback；可以继续分析其他内容；\n使用pass占位\ntry: print(x/y) with open(filename, \u0026#39;w\u0026#39;) as f: f.write(\u0026#34;I Love You!\u0026#34;) except ZeroDivisionError: print(\u0026#34;You can\u0026#39;t divide by zero!\u0026#34;) except FileNotFoundError: pass else: print(answer) 13.1、UnitTest # TestCase # 为函数编写测试用例步骤：\nimport unittest 以及需要测试的函数\n创建一个继承 unittest.TestCase 的类\n编写一系列方法对函数行为的不同方面进行测试\nimport unittest from name_function import get_formatted_name class NamesTestCase(unittest.TestCase): \u0026#34;\u0026#34;\u0026#34;测试name_function.py\u0026#34;\u0026#34;\u0026#34; def test_first_last_name(self):\t# test_开头的方法都会自动运行 formatted_name = get_formatted_name(\u0026#39;janis\u0026#39;, \u0026#39;joplin\u0026#39;) self.assertEqual(formatted_name, \u0026#39;Janis Joplin\u0026#39;)\t# 断言方法 unittest.main() 断言方法：\n方法 用途 assertEqual(a,b) 核实a==b assertNotEqual(a,b) 核实a!=b assertTrue(x) 核实x为True assertFalse(x) 核实x为False assertIn(item, list) 核实item在list中 assertNotIn(item, list) 核实item不在list中 setUp() 方法会在运行 test_ 开头的方法之前运行。可以在其中创建一系列实例并设置它们的属性，再在测试方法中直接使用这些实例。\nimport unittest from survey import AnonymousSurvey class TestAnonymousSurvey(unittest.TestCase): def setUp(self): question = \u0026#34;....\u0026#34; self.my_survey = AnonymousSurvey(question) self.responses = [\u0026#39;English\u0026#39;, \u0026#39;Spanish\u0026#39;, \u0026#39;Mandarin\u0026#39;] def test_store_single_response(self): self.my_survey.store_respone(self.responses[0]) self.assertIn(self.responses[0], self.my_survey.responses) def test_store_three_response(self): for response in self.responses: self.my_survey.store_respone(responses) for response in self.responses: self.assertIn(responses, self.my_survey.responses) TestSuite\u0026amp;TestRunner # 管理、打包、组装测试用例文件 \u0026amp; 执行TestSuite\n实例化套件对象 ​\tsuite = unittest.TestSuite()\n添加用例方法 ​\tsuite.addTest(unittest.makeSuite(TestMyClass)\n实例化运行对象 ​\trunner = unittest.TextTestRunner()\n运行 ​\trunner.run(suite)\nimport unittest def add(a, b): return a + b class TestAddition(unittest.TestCase): def test_add_two_positive_numbers(self): result = add(3, 4) self.assertEqual(result, 7) def test_add_two_negative_numbers(self): result = add(-3, -4) self.assertEqual(result, -7) def test_add_positive_and_negative_numbers(self): result = add(3, -4) self.assertEqual(result, -1) class TestAddition1(unittest.TestCase): def test_add_two_positive_numbers(self): result = add(3, 4) self.assertEqual(result, 7) def test_add_two_negative_numbers(self): result = add(-3, -4) self.assertEqual(result, -7) def test_add_positive_and_negative_numbers(self): result = add(3, -4) self.assertEqual(result, -1) if __name__ == \u0026#39;__main__\u0026#39;: suite = unittest.TestSuite() suite.addTest(unittest.makeSuite(TestAddition)) # 或者 suite.addTest(TestAddition（\u0026#39;test_add_two_positive_numbers\u0026#39;) 这样一个个方法去添加 suite.addTest(unittest.makeSuite(TestAddition1)) runner = unittest.TextTestRunner() runner.run(suite) TestLoader # 测试加载，作用与TestSuite作用类似\n加载用例\nsuite = unittest.TestLoader().discover(start_dir, pattern='test*.py', top_level_dir=None)\nstart_dir 要测试的模块名或测试用例目录 pattern=\u0026lsquo;test*.py\u0026rsquo; 表示用例文件名的匹配原则 top_level_dir=None 测试模块的顶层目录。None \u0026lt;=\u0026gt; 测试用例不是放在多级目录中 import unittest if __name__ == \u0026#39;__main__\u0026#39;: suite = unittest.defaultTestLoader.discover(\u0026#39;other/\u0026#39;, pattern=\u0026#39;test_a.py\u0026#39;) unittest.TextTestRunner().run(suite) Fixture # 测试夹具，特定情况下会自动执行\n**方法级别：**在每个测试方法执行前后都会自动调用的结构\ndef setUp(self): pass def tearDown(self): pass 类级别： 在整个类执行前后会调用一次\n@classmethod def setUpClass(cls): pass @classmethod def tearDownClass(cls): pass 模块级别： 在每个代码文件执行前后执行一次\ndef setUpModule(): pass def tearDownModule(): pass import unittest def add(a, b): return a + b class TestAddition(unittest.TestCase): def setUp(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个方法之前执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;setUp\u0026#39;) def tearDown(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个方法之后执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;tearDown\u0026#39;) @classmethod def setUpClass(cls) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个类之前执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;setUpClass\u0026#39;) @classmethod def tearDownClass(cls) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个类之后执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;tearDownClass\u0026#39;) def test_add_two_positive_numbers(self): result = add(3, 4) self.assertEqual(result, 7) def test_add_two_negative_numbers(self): result = add(-3, -4) self.assertEqual(result, -7) def test_add_positive_and_negative_numbers(self): result = add(3, -4) self.assertEqual(result, -1) class TestAddition1(unittest.TestCase): def test_add_two_positive_numbers(self): result = add(3, 4) self.assertEqual(result, 7) def test_add_two_negative_numbers(self): result = add(-3, -4) self.assertEqual(result, -7) def test_add_positive_and_negative_numbers(self): result = add(3, -4) self.assertEqual(result, -1) if __name__ == \u0026#39;__main__\u0026#39;: suite = unittest.TestSuite() suite.addTest(unittest.makeSuite(TestAddition)) suite.addTest(unittest.makeSuite(TestAddition1)) runner = unittest.TextTestRunner() runner.run(suite) ################## setUpClass setUp tearDown .setUp tearDown .setUp tearDown .tearDownClass ... ---------------------------------------------------------------------- Ran 6 tests in 0.003s OK 测试报告 # import BeautifulReport result = BeautifulReport(suite) result.report(filename=\u0026#39;other/testdemoreport.html\u0026#39;,description=\u0026#39;测试报告\u0026#39;,log_path=\u0026#39;.\u0026#39;,report_dir=\u0026#39;.\u0026#39;) 参数化 # from parameterized import parameterized data = [ (\u0026#39;pn\u0026#39;,[1,-2],-1),# 名称，输入，输出 (\u0026#39;pp\u0026#39;,[2,3],5), (\u0026#39;nn\u0026#39;,[-3,-4],-7) ] @parameterized.expand(data) def test_add(self,a,b,c): self.assertEqual(fun(b[0],b[1]),c) 跳过测试方法 # @unittest.skip(\u0026#39;跳过原因\u0026#39;) @unittest.skipIf(条件判断, \u0026#39;跳过原因\u0026#39;) 13.2 Pytest # 文件名都需要满足 test_*.py 格式或 *_test.py 格式\n与上面类似，在命令行执行\npytest [目标目录] [命令行参数] [-html=报告名称.html --self-contained-html]\n-s 显示测试代码中print的内容\n-v 更详细的执行信息\n-k 匹配函数名称包含的\n-m 根据标签执行\npytest [*.py][目录1][目录2][*.py::Test_Class1][*.py::Test_Class1::test_method1] 指定模块名称/目录/类\npytest -k \u0026quot;[匹配规则1 and 规则2 ]\u0026quot; -s 规则可使用 not and or\n标签 # pytest cases -m webtest -s 根据标签执行\n@pytest.mark.[标签名称]\n可以给类、方法、模块加上标签\nimport pytest pytestmark = pytest.mark.网页测试\t# 模块标签 pytestmark = [pytest.mark.网页测试, pytest.mark.登录测试]\t# 多个标签 @pytest.mark.webtest\t# 类标签 class Test_错误密码2: @pytest.mark.webtest\t# 方法标签 def test_C001021(self): print(\u0026#39;\\n用例C001021\u0026#39;) assert 1 == 1 方法级别 # def setup_method(self): print(\u0026#39;\\n --- 初始化-方法 ---\u0026#39;) def teardown_method(self): print(\u0026#39;\\n --- 清除 -方法 ---\u0026#39;) 类级别 # @classmethod def setup_class(cls): print(\u0026#39;\\n === 初始化-类 ===\u0026#39;) @classmethod def teardown_class(cls): print(\u0026#39;\\n === 清除 - 类 ===\u0026#39;) 模块级别 # def setup_module(): print(\u0026#39;\\n *** 初始化-模块 ***\u0026#39;) def teardown_module(): print(\u0026#39;\\n *** 清除-模块 ***\u0026#39;) 目录级别 # 在需要初始化的目录下面创建 一个名为 conftest.py 的文件\nimport pytest @pytest.fixture(scope=\u0026#39;package\u0026#39;,autouse=True) def st_emptyEnv(): print(f\u0026#39;\\n#### 初始化-目录甲\u0026#39;) yield print(f\u0026#39;\\n#### 清除-目录甲\u0026#39;) ## 与unittest不同：不需要导入pytest包，只能使用assert这种原生的断言 def add(a, b): return a + b class TestAddition(): def setup_method(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个方法之前执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;setUp\u0026#39;) def teardown_method(self) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个方法之后执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;tearDown\u0026#39;) @classmethod def setup_class(cls) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个类之前执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;11111111111111111setUpClass\u0026#39;) @classmethod def teardown_class(cls) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;每个类之后执行\u0026#34;\u0026#34;\u0026#34; print(\u0026#39;111111111111111tearDownClass\u0026#39;) def test_add_two_positive_numbers(self): result = add(3, 4) assert result==7 def test_add_two_negative_numbers(self): result = add(-3, -4) assert result==-7 def test_add_positive_and_negative_numbers(self, a, b): return add(a, b) 数据驱动 # class Test_错误登录: @pytest.mark.parametrize(\u0026#39;username, password, expectedalert\u0026#39;, [ (None, \u0026#39;88888888\u0026#39;, \u0026#39;请输入用户名\u0026#39;), (\u0026#39;byhy\u0026#39;, None, \u0026#39;请输入密码\u0026#39;), (\u0026#39;byh\u0026#39;, \u0026#39;88888888\u0026#39;, \u0026#39;登录失败 : 用户名或者密码错误\u0026#39;), (\u0026#39;byhy\u0026#39;, \u0026#39;8888888\u0026#39;, \u0026#39;登录失败 : 用户名或者密码错误\u0026#39;), (\u0026#39;byhy\u0026#39;, \u0026#39;888888888\u0026#39;, \u0026#39;登录失败 : 用户名或者密码错误\u0026#39;), ]) def test_UI_0001_0005(self, username, password, expectedalert): alertText = loginAndCheck(username, password) assert alertText == expectedalert # lib\\webui.py from selenium import webdriver import time def loginAndCheck(username,password): driver = webdriver.Chrome() driver.implicitly_wait(10) driver.get(\u0026#39;http://127.0.0.1/mgr/sign.html\u0026#39;) if username is not None: driver.find_element_by_id(\u0026#39;username\u0026#39;).send_keys(username) if password is not None: driver.find_element_by_id(\u0026#39;password\u0026#39;).send_keys(password) driver.find_element_by_css_selector(\u0026#34;button[type=\u0026#39;submit\u0026#39;]\u0026#34;).click() time.sleep(2) alertText = driver.switch_to.alert.text print(alertText) driver.quit() return alertText 14、JSON # import json\njson.dump() 接收两个实参：要存储的数据 + 文件对象\njson.load() 接收一个实参：文件对象\nimport json numbers = [x**2 for x in range(1, 10, 2)] filename = \u0026#39;numbers.json\u0026#39; with open(filename, \u0026#39;w\u0026#39;) as f: json.dump(numbers, f) with open(filename) as f: nums = json.load(f) 15、CSV # import csv\nwith open(filename) as f: reader = csv.reader(f)\t# 创建一个阅读器对象 header_row = next(reader)\t# 获取文件的下一行 print(header_row)\tfor row in reader: ... 16、Django # 创建一个项目\ndjango-admin startproject django_web . 创建数据库\npython manage.py migrate 运行项目\npython manage.py runserver 建立创建应用程序所需的基础设施——会新建一个文件夹learning_logs\npython manage.py startapp learning_logs 在 model.py 中定义模型\nfrom django.db import models # Create your models here. class Topic(models.Model): \u0026#34;\u0026#34;\u0026#34;用户学习的主题\u0026#34;\u0026#34;\u0026#34; text = models.CharField(max_length=200) date_added = models.DateTimeField(auto_now_add=True) # 自动设置成当前日期与时间 def __str__(self): \u0026#34;\u0026#34;\u0026#34;返回模型的字符串表示\u0026#34;\u0026#34;\u0026#34; return self.text 激活模型 setting.py\n... INSTALLED_APPS = [ \u0026#39;django.contrib.admin\u0026#39;, \u0026#39;django.contrib.auth\u0026#39;, \u0026#39;django.contrib.contenttypes\u0026#39;, \u0026#39;django.contrib.sessions\u0026#39;, \u0026#39;django.contrib.messages\u0026#39;, \u0026#39;django.contrib.staticfiles\u0026#39;, # My Apps \u0026#39;learning_logs\u0026#39;, ] ... 修改数据库\npython manage.py makemigrations learning_logs 创建了一个名为 0001_initial.py 的迁移文件，会在数据库中为 Topic 创建一个表\n应用迁移：\npython manage.py migrate 创建超级用户\npython manage.py createsuperuser 用户名 admin、密码 123\nfrom django.contrib import admin # from learning_logs.models import Topic from learning_logs.models import Topic admin.site.register(Topic) # http://127.0.0.1:8000/admin/ 登录后看到如下页面\n在 Topics 右边点击 Add 后添加一个主题\n添加模型\nmodel.py 中：\nclass Entry(models.Model): \u0026#34;\u0026#34;\u0026#34;学到的有关某个主题的具体知识\u0026#34;\u0026#34;\u0026#34; topic = models.ForeignKey(Topic, on_delete=models.CASCADE) # 定义外键 text = models.TextField() # 没有限定长度的字段 date_added = models.DateTimeField(auto_now_add=True) class Meta: # 嵌套的类，用于存储用于管理模型的额外信息 verbose_name_plural = \u0026#39;entries\u0026#39; def __str__(self): \u0026#34;\u0026#34;\u0026#34;返回模型的字符串表示\u0026#34;\u0026#34;\u0026#34; return self.text[:50] + \u0026#39;...\u0026#39; 再次执行迁移数据库的指令\npython manage.py makemigrations learning_logs python manage.py migrate 再次向管理网站注册 Entry\nadmin.py 中：\nfrom learning_logs.models import Entry admin.site.register(Entry) Django Shell\n使用上面的代码获取 Topic 的所有实例，返回一个查询集（列表）\n获取到 id 后的用法：\n上面使用了外键关系获取数据，使用相关模型的小写、下划线、set 组合，如上图的 entry_set\nA 有外键 B，A.B_set.all() 获取与 A 关联的所有 B\n映射URL\n在 django_web/urls.py 中：\nfrom django.contrib import admin from django.urls import path,include urlpatterns = [ path(\u0026#39;admin/\u0026#39;, admin.site.urls), path(r\u0026#39;\u0026#39;, include(\u0026#39;learning_logs.urls\u0026#39;, namespace=\u0026#39;learning_logs\u0026#39;)), ] django.urls.include 函数将 learning_logs 应用的路由表接入进来，并且 include 函数的参数是路由模块路径的字符串 learning_logs.urls\n每一个 Django 路由表模块（urls.py）中都约定必须包含一个 urlpatterns 列表用来存放路由映射表。列表中每个元素是一个用 django.urls.path 函数封装好的路由映射，通常接收以下三个参数：\nroute：必须，即实际的访问路由，空字符串等于 /，即空路由 view：必须，该路由将要访问的视图 name：可选，该路由的名称，方便后续在模板中使用 为应用创建路由模块\n创建文件 learning_logs/urls.py\n\u0026#34;\u0026#34;\u0026#34;定义 learning_logs 的 URL 模式\u0026#34;\u0026#34;\u0026#34; from django.urls import path from . import views # 从当前模块所在的文件夹中导入视图 urlpatterns = [ # 主页 path(r\u0026#39;^$\u0026#39;, views.index, name=\u0026#39;index\u0026#39;), ] 其中正则表达式 ^ 表示开头 $ 表示结尾。\n匹配后，调用 view.index 函数\nname 为模式指定名称，方便在其它地方引用，每当需要提供这个主页连接时，都可以使用这个名称，而不编写 URL\n编写视图 view\n执行python managge.py sstartapp 时生成的 view.py 如下\nfrom django.shortcuts import render # Create your views here. render() 函数根据视图提供是数据渲染响应。\ndef index(request): \u0026#34;\u0026#34;\u0026#34;学习笔记的主页\u0026#34;\u0026#34;\u0026#34; context = { \u0026#39;news_list\u0026#39;: [ { \u0026#34;title\u0026#34;: \u0026#34;学习笔记\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;学习笔记帮助你持续学习\u0026#34;, } ] } return render(request, \u0026#39;learning_logs/index.html\u0026#39;, context) django.shortcuts.render 函数用于渲染模板，这个函数通常接受三个参数（有其他参数，但是这里我们不关心）：\nrequest：请求对象，直接把视图的参数 request 传进来就可以 template_name：模板名称，这里就是我们刚刚创建的 news/index.html context：传入模板的上下文对象，必须是一个字典，字典中的每个键对应模板中的变量。这里我们弄了些假数据，假装是从数据库里面取来的。 编写模板\n在 learning_logs 文件夹中创建文件 templates.learning_logs.index.html由于 Django 的模板查找机制会将所有应用里面的模板全部收集到一起，如果两个模板的名字冲突，就会导致其中一个模板不能被正确访问。如果放在 learning_logs 子文件夹里面，就能够通过 learning_logs/index.html 访问，通过命名空间的机制避免了冲突。\n{% if news_list %} \u0026lt;ul\u0026gt; {% for elem in news_list %} \u0026lt;li\u0026gt; \u0026lt;h3\u0026gt;{{ elem.title }}\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;{{ elem.content }}\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; {% endfor %} \u0026lt;/ul\u0026gt; {% else %} \u0026lt;p\u0026gt;暂无内容\u0026lt;/p\u0026gt; {% endif %} 运行报错django.core.exceptions.ImproperlyConfigured: Specifying a namespace in include() without providing an app_name is not supported. Set the app_name attribute in the included module, or pass a 2-tuple containing the list of patterns and app_name instead.\n解决办法：https://blog.csdn.net/weixin_40841752/article/details/79335345\n","externalUrl":null,"permalink":"/docs/python/python/","section":"Docs","summary":"1、方法与函数的区别 # 方法作用于对象，函数大多需要传入参数 方","title":"Python入门","type":"docs"},{"content":" Redis入门 # SQL与NoSQL区别 SQL NoSQL 数据结构 结构化 非结构化 数据关联 关联的 无关联的 查询方式 SQL查询 非SQL 事务特性 ACID BASE 存储方式 磁盘 内存 扩展性 垂直 水平 使用场景 结构固定、业务对数据安全性一致性要求较高 数据结构不固定、对一致性安全性要求不高、对性能要求 一、认识Redis # 远程词典服务器，基于内存的键值型 NoSQL 数据库\n特征：\n键值型，支持多种不同数据结构 单线程，每个命令具备原子性 低延迟，速度快（基于内存、IO多路复用、良好的编码[C语言/开源]） 支持数据持久化 支持主从集群、分片集群 支持多语言客户端 0.docker 部署 # docker run -p 6379:6379 --name redis -v D:/redis/redis.conf:/etc/redis/redis.conf -v D:/redis/data:/data -d redis redis-server /etc/redis/redis.conf --appendonly yes docker start 582e147\t运行容器 docker exec -it redis /bin/bash\t进入运行的容器 redis-cli 进入Redis控制台 docker exec -it redis redis-cli\t上面的结合体 exit\t退出容器/Redis控制台 redis-cli [-hpa] [commond]\n-h 指定要连接的redis节点的IP地址，默认127.0.0.1 -p 指定节点端口，默认6379 -a 指定访问密码 commond 不指定commond时会进入交互控制台 1.数据结构介绍 # key一般是String类型，value类型：\n基本类型 String、Hash、List、Set、SortedSet 特殊类型 GEO、BitMap、HyperLog 1.1 String # key是一个简单动态字符串（SDS），value也是一个SDS\nstruct sds{ int len;\t// 记录已使用的字节数（不含末尾\u0026#39;\\0\u0026#39;，可以保存空格\u0026#39;\\0\u0026#39;——二进制安全） int free;\t// 记录未使用的字节数 char buf[];\t// 字节数组，存储字符串 }; SDS扩容机制（空间预分配）：\n目标容量 \u0026lt;1MB\t多扩容一倍：len = 目标容量 free = 目标容量\n​\t\u0026gt;1MB\t多扩容 1MB：len = 目标容量 free = 1MB\n惰性空间释放：只改变free 和 len，不实际释放空间\n1.2 链表 # 列表键、发布与订阅、慢查询、监视器都使用到了链表\ntypedef struct listNode{ struct listNode *prev; struct listNode *next; void *value; }listNode; typedef struct list{ listNode *head;\t// 头节点 listNode *tail;\t// 尾节点 unsigned long len;\t// 节点数量 void *(*dup)(void *ptr); // 节点值复制函数 void *(*free)(void *ptr);\t// 节点值释放函数 void *(*match)(void *ptr, void *key)\t// 节点值对比函数 }list; 特点：\n双端、无环、带长度计数器、多态\n1.3 字典 # Redis数据库的原理，也是Hash键的底层实现\ntypedef struct dictht{ dictEntry **table;\t// 指向哈希表数组 unsigned long size;\t// 哈希表大小 unsigned long sizemark;\t// 哈希表大小掩码，用于计算索引值 = size - 1 unsigned long used;\t// 哈希表已有节点数量 }dictht;\t// 哈希表 typedef struct dictEntry{ void *key; union{ void *val; uint64_t u64; int64_t s64; } v;\t// 值可以是一个指针、无符号64位整数或是64位整数 struct dictEntry *next;\t// 指向下一个哈希表节点，形成链表 }dictEntry; // 哈希表节点 typedef struct dict{ dictType *type;\t// 类型特定函数 void *privdata;\t// 私有数据 dictht ht[2];\t// 哈希表 一般只使用一个哈希表，只有在rehash时才使用h[1] int rehashidx;\t// rehash的进度，当rehash不在进行时 = -1 }dict;\t// 字典 type指向dictType结构的指针，每个dictType结构保存了一簇用于操作特定类型键值对的函数\nprivdata保存需要传给哪些类型特定函数的可选参数\n哈希算法\n使用字典设置的哈希函数，计算key的哈希值：hash = dict-\u0026gt;type-\u0026gt;hashFunction(key)\n计算索引值：index = hash \u0026amp; dict -\u0026gt;ht[x].sizemask [hash 和哈希表大小掩码 的 与，根据情况 x 可以是 0 或 1]\n用作数据库底层实现、哈希键底层实现时，使用MurmurHash算法\n解决冲突：链地址法。把新加入的节点放链表头\nrehash\n哈希表扩展：\n当负载因子 \u0026gt; 1 且不在执行BGSAVE和BGREWRITEAOF时\n当负载因子 \u0026gt; 5 且在执行BGSAVE和BGREWRITEAOF时\n哈希表收缩： 负载因子 \u0026lt; 0.1\n为ht[1]哈希表分配空间（扩展：第一个$\\geq$ ht[0].used * 2 的 $2^n$ 收缩：第一个$\\geq$ ht[0].used 的 $2^n$）\n将h[0]的所有键值对rehash到ht[1]——渐进式\n为避免rehash数量过多对性能的影响\n维护一个索引计数器rehashidx，设置为0，表示rehash正式开始 在rehash期间，每次对字典进行增删改查时，会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，完成后将rehashidx加 1 当ht[0] 都rehash结束后，将rehashidx设置回 -1，表示rehash结束 rehash 期间的查询在两张表上进行，先查ht[0]。插入则一律插入ht[1]\n释放ht[0]，将ht[1]设置为ht[0]，在ht[0]创建空白哈希表\n1.4 跳跃表 # 有序集合键的底层实现、集群节点的内部数据结构\ntypedef struct zskiplistNode{ struct zskiplistLevel{ struct zskiplistNode *forword;\t// 前进指针 unsigned int span;\t// 跨度\t记录两个节点之间的距离 } level[];\t// 层 struct zskiplistNode *backward;\t// 后退指针 double score;\t// 分值\t·可以相同 robj *obj;\t// 成员对象 指向一个字符串对象（保存着一个SDS） ·必须唯一 } zskiplistNode;\t// 跳跃表节点 typedef struct zskiplist { struct zskiplistNode *header, *tail; unsigned long length;\t// 表中节点数量 int level;\t// 最大层数 }zskiplist;\t// 跳跃表 跨度用于计算元素的排位次序，将沿途的跨度求和就是节点在跳跃表中的排位\n插入：随机分配一个层数（1-32）。\n查找 ：跳表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。首先在最高级索引上查找最后一个小于当前查找元素的位置，然后再跳到次高级索引继续查找，直到跳到最底层为止，这时候以及十分接近要查找的元素的位置了(如果查找元素存在的话)。由于根据索引可以一次跳过多个元素，所以跳查找的查找速度也就变快了\n1.5 整数集合 # 集合键的底层实现之一，集合只包含整数、元素不多时使用。\ntypedef struct intset{ uint32_t encoding;\t// 编码方式 uint32_t length; int8_t contents[]\t// 保存元素的数组 }intset; contents 中不包含重复项，且按值的大小从小到大有序地排列。数组的类型真正取决于encoding的值\n升级：添加新元素，且新元素类型比数组的都要长时\n根据新元素类型，扩展数组的空间大小 将所有的元素转为新元素相同的类型 将新元素添加到数组中（要么比现有都大，放到最后；要么比现有都小，放到开头） 升级的好处：提升灵活性、节约内存\n降级: 不支持\n1.6 压缩列表 # 列表键、哈希键的底层实现之一。数量较少，且每项都是小整数值或者较短的字符串时使用。\n压缩列表：\nzlbytes\tuint32_t\t占用字节数\nzltail\tuint32_t\t记录表尾节点（最后一个entry）的偏移量\nzllen\tuint16_t\t记录列表包含节点数\nentryX\t压缩列表的各个节点\nzlend\tuint8_t\t=0xFF 标记压缩列表末尾\n压缩列表节点：\nprevious_entry_length\t记录前一个节点长度，用于计算前一个节点的地址，长度为1（长度在254之下）或5字节\nencoding\t1、2或5字节长 最高位为00、01、10的为字节数组 11开头的为整数\ncontent\n连锁更新：插入一个节点为$\\geq254$长度时，有可能后一个节点无法记录此长度，从而导致后续节点需要连锁更新为5字节长的previous_entry_length、有可能发生连锁的更新。\t删除也可能引发连锁更新。但造成的性能问题的可能很低（长的连锁更新概率小）\n1.7 对象系统 # 每次创建一个键值对，至少会创建两个对象：一个键对象（总是字符串对象）一个值对象（可以是字符串、列表、哈希、集合或有序集合对象）\ntypedef struct redisObject{ unsigned type:4;\t// 类型 unsigned encoding:4;// 编码，记录对象使用了什么数据结构 void *ptr;\t// 指向底层数据结构的指针 int refcount;\t// 引用计数 unsigned lru:22;\t// 空转时长 // ... }robj; 类型 编码 对象 场景 string REDIS_ENCODING_INT 整数值实现的字符串对象 保存整数且可以用long表示 REDIS_ENCODING_EMBSTR 使用embstr编码的SDS实现的字符串对象、只读 保存字符串、字符串长度$\\leq32$字节 REDIS_ENCODING_RAW 使用SDS实现的字符串对象 保存字符串、字符串长度$\u0026gt;32$字节 list REDIS_ENCODING_ZIPLIST 压缩列表实现的列表对象 REDIS_ENCODING_LINKEDLIST 双端列表实现的列表对象 hash REDIS_ENCODING_ZIPLIST 压缩列表实现的哈希对象 REDIS_ENCODING_HT 字典实现的哈希对象 set REDIS_ENCODING_INTSET 整数集合实现的集合对象 REDIS_ENCODING_HT 字典实现的集合对象 zset REDIS_ENCODING_ZIPLIST 压缩列表实现的有序集合对象 REDIS_ENCODING_SKIPLIST 跳跃表和字典实现的有序集合对象 字符串对象 # embstr编码的SDS将内存分配次数降低为1次，释放内存也是1次。内存连续\n列表对象 # 保存的字符串长度都小于64字节 \u0026amp;\u0026amp; 元素数量小于512 时使用ziplist编码（压缩列表）。\n否则使用双端链表（linkedlist编码）。其底层为若干字符串对象连接而成\n哈希对象 # 保存的字符串长度都小于64字节 \u0026amp;\u0026amp; 元素数量小于512 时使用ziplist编码（压缩列表）。\n​\t此时键值对按顺序排放：每次在表尾插入一个键对象、一个值对象\n其他情况：hashtable编码\n​\t字典的每个键和值都是字符串对象\n集合对象 # 元素都是整数 \u0026amp;\u0026amp; 数量小于512 使用intset编码（整数集合）\nhashtable编码：键都是字符串对象（包含一个集合元素），值都为NULL\n有序集合对象 # 保存的成员长度都小于64字节 \u0026amp;\u0026amp; 元素数量小于128 时使用ziplist编码（压缩列表）。\n​\t每个集合元素使用两个压缩列表节点：一个保存成员、一个保存分值\nskipllist编码：使用zset结构作为底层实现\ntypedef struct zset { zskiplist *zsl; dict *dict; } zset; zsl跳跃表按分值保存了所有元素\ndict字典为集合创建一个从成员到分值的映射\n这两个结构通过指针共享相同元素的成员和分值，同时使用不会浪费额外的内存\n1.8 类型检查与命令多态 # 类型检查 对需执行的命令，先检查键的值对象类型（对象结构的type属性）是否为目标命令的对象\n命令多态 对象的底层结构不唯一，因此命令是多态的。在检查完类型后，会检查值对象的编码是哪一种编码，再调用具体的函数\n1.9 内存回收、对象共享、对象的空转时长 # 引用计数：每个对象的引用计数信息在redisObject的refcount属性中保存\n对象共享：将键的值指针指向一个现有的对象，将被共享的值对象引用计数增1。\n​\t在初始化Redis服务器时，会创建一万个字符串对象（0-9999的整数值）备用\n空转时长：redisObject的最后一个属性 lru，记录最后一次被命令程序访问的时间。\n​\t若服务器开启了maxmemory选项，且回收内存的算法为volatile-lru或allkeys-lru，当内存占用超限时，空转时间高的键优先被服务器释放\n2.指令 # 官方文档 、help、大小写不敏感\n1）通用命令 # KEYS pattern 查看符合模板的所有key，不建议在生产环境上使用[O(N)，太慢了]\n？\t模糊匹配单个字符\n*\t匹配多个字符\n[ae] 其中之一\n[^a] 排除a\n[a-b] 其中之一\nDEL key [key ...] 删除指定的key O(1)\nEXISTS key [key ...] 指定key存在的数目 O(1)\nEXPIRE key seconds [NX | XX | GT | LT] O(1)\nNX 无到期时间才设置\nXX 有到期时间才设置\nGT 新时间大于旧时间才设置\nLT 旧时间大于新时间才设置\nTTL key 返回有效期（秒）O(1)\n-2 键不存在\n-1 永久\n2）String命令 # 字符串格式分三类，最大空间不能超过512M：\nstring 普通字符串 int 整数类型，可以自增自减 float 浮点类型，可以自增自减 本质都是字节数组形式存储，只是编码形式不同\nSET key value [NX | XX] [GET] [EX seconds | PX milliseconds | EXAT unix-time-seconds | PXAT unix-time-milliseconds | KEEPTTL]添加或修改 O(1)\nEX 秒——设置指定的过期时间，以秒为单位\nPX 毫秒 \u0026ndash; 设置指定的过期时间，以毫秒为单位\nEXAT timestamp-seconds \u0026ndash; 设置密钥过期的指定 Unix 时间，以秒为单位\nPXAT timestamp-milliseconds \u0026ndash; 设置密钥过期的指定 Unix 时间，以毫秒为单位\nNX \u0026ndash; 仅当key尚不存在时才设置该key\nXX \u0026ndash; 仅当key已存在时才设置该key\nKEEPTTL——保留与key相关的生存时间\nGET \u0026ndash; 返回存储在 key 处的旧字符串，如果 key 不存在则返回 nil\n​ 如果 key 存储的值不是字符串，则会返回错误并中止 SET\nGET key 获取键对应的值，不存在则返回nil O(1)\nMSET key value [key value ...] 批量添加键值对 O(1)\nMGET key [key ...] 批量获取键对应的值，不存在用nil填充 O(1)\nINCR key 将键中存储的数字加一，此操作仅限于 64 位有符号整数。返回增加后的值 O(1)\n如果key不存在，则在执行操作之前将其设置为0\n如果键包含错误类型的值或包含无法表示为整数的字符串，则会返回错误\nINCRBY key increment 按增量增加存储在 key 中的数字 O(1)\nINCRBYFLOAT key increment 将存储在 key 处的浮点数的字符串增加指定的增量 O(1)\n如果该键不存在，则在执行操作之前将其设置为 0\n键包含错误类型的值（不是字符串）或当前键内容或指定增量不可解析为双精度浮点数时报错\n无论计算的实际内部精度如何，输出的精度固定为小数点后 17 位\n可以使用 2.0e2 这类表示\nSETNX key value 若 key 不存在，则将键设置为字符串值。在这种情况下，它等效于SET O(1)\n​ 当 key 已有时，不执行任何操作。 SETNX 是“SET if Not eXists”的缩写\nSETEX key seconds value 将 key 设置为 value，并将 key 设置为在给定秒数后超时 O(1)\n等效于 SET key value EX seconds\n3）Key的层级格式 # 允许使用冒号隔开形成层级结构（并非固定）\n127.0.0.1:6379\u0026gt; set heima:user:1 \u0026#39;{\u0026#34;id\u0026#34;:1, \u0026#34;name\u0026#34;:\u0026#34;TC\u0026#34;, \u0026#34;age\u0026#34;:21}\u0026#39; 127.0.0.1:6379\u0026gt; set heima:user:2 \u0026#39;{\u0026#34;id\u0026#34;:2, \u0026#34;name\u0026#34;:\u0026#34;TCmatj\u0026#34;, \u0026#34;age\u0026#34;:23}\u0026#39; 127.0.0.1:6379\u0026gt; set heima:product:1 \u0026#39;{\u0026#34;id\u0026#34;:1, \u0026#34;name\u0026#34;:\u0026#34;ipad\u0026#34;, \u0026#34;price\u0026#34;:6150}\u0026#39; 127.0.0.1:6379\u0026gt; set heima:product:2 \u0026#39;{\u0026#34;id\u0026#34;:2, \u0026#34;name\u0026#34;:\u0026#34;pc\u0026#34;, \u0026#34;price\u0026#34;:9600}\u0026#39; 4）Hash类型 # 也叫散列，value是一个无序字典，类似于Java中HashMap结构\nHash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD\nHSET key field value [field value ...]\nHGET key field\nHMSET key field value [field value ...]\nHMGET key field [field ...]\nHGETALL key 字段和值列表\nHKEYS key 字段列表\nHVALS 值列表\nHINCRBY key field increment key不存在会新建，字段不存在会设为0\nHSETNX key field value 仅当字段不存在时，才将存储在键处的哈希中的字段设置为值。如果字段已经存在，则此操作无效。\n​ 如果key不存在，则会创建一个包含哈希值的key。\n5）List 类型 # 可以看做双向链表，既可以正向检索也支持反向检索\nLPUSH key element [element ...] 将所有值插入存储在 key 的列表的头部\tO(1)\n​\t如果 key 不存在，创建为空列表。\n​\t当 key 保存的值不是列表时，会返回错误，否则返回列表长度\nLPOP key [count] 删除并返回存储在 key 处的列表的前count个元素 O(1)\nRPUSH key element [element ...] 将所有值插入存储在 key 的列表的尾部\tO(1)\nRPOP key [count] 删除并返回存储在 key 处的列表的后count个元素 O(1)\nLRANGE key start stop 返回存储在 key 处的列表的指定元素。偏移量 start 和 stop 是从零开始的索引 O(S + N)\nBLPOP key [key ...] timeout BLPOP 是阻止列表弹出原语。它是 LPOP 的阻塞版本，因为当没有可以从任何给定列表中弹出的元素时，它会阻塞连接。从第一个非空列表的头部弹出一个元素，并按照给出的顺序检查给定的键。阻塞到 timeout 时才放行，0表示无限期阻塞\nBRPOP key [key ...] timeout RPOP 是阻止列表弹出原语。它是RPOP 的阻塞版本，因为当没有可以从任何给定列表中弹出的元素时，它会阻塞连接。从第一个非空列表的尾部弹出一个元素，并按照给出的顺序检查给定的键\n6）Set 类型 # 与HashSet类似\n无序、不重复、查找快、支持交并补\nSADD key member [member ...] 添加，已存在的会被忽略 O(1) SREM key member [member ...] 删除，不存在的会被忽略 O(1) SCARD key 返回存储在 key 处的集合的集合基数（元素数量） O(1) SISMEMBER key member 如果 member 是存储在 key 处的集合的成员，则返回1，否则返回0 O(1) SMEMBERS key 返回存储在 key 处的设置值的所有成员 O(1) SINTER key [key ...] 返回由所有给定集合的交集产生的集合的成员 O(N*M) 最坏情况，N 是最小集合的基数，M 是集合的数量 SDIFF key [key ...] 返回由第一个集合和所有连续集合之间的差异产生的集合的成员 O(N) SUNION key [key ...] 返回由所有给定集合并集产生的集合的成员 O(N) 7）SortedSet类型 # 可排序的Set集合，与Java的TreeSet类似，底层差异很大。SortedSet中的每个元素都有一个score属性，可以基于它进行排序，底层实现是一个跳表+hash表。Java TreeSet：红黑树\n可排序、不重复、查询速度快\nZADD key [NX | XX] [GT | LT] [CH] [INCR] score member [score member ...] O(log(N))\n将具有指定分数的所有指定成员添加到存储在 key 处的排序集中。可以指定多个分数/成员对\n如果指定的成员已经是排序集的成员，则更新分数并将元素重新插入到正确的位置以确保正确的排序\n分数值应该是双精度浮点数的字符串表示形式。 +inf 和 -inf 值也是有效值\nXX：只更新已经存在的元素。不要添加新元素\nNX：仅添加新元素。不要更新已经存在的元素\nLT：如果新分数小于当前分数，则仅更新现有元素。该标志不会阻止添加新元素\nGT：仅当新分数大于当前分数时才更新现有元素。该标志不会阻止添加新元素\nCH：将返回值从新增元素的数量修改为已更改元素的总数，通常ZADD的返回值只统计添加的新元素的数量\nINCR：指定此选项时，ZADD 的作用类似于 ZINCRBY。在此模式下只能指定一对分数元素\nGT、LT 和 NX 选项是互斥的\nZREM key member [member ...] 从存储在 key 处的排序集中删除指定的成员。不存在的成员将被忽略 O(M*log(N))\nZSCORE key member 返回 key 处排序集中成员的分数 O(1)\nZRANK key member [WITHSCORE] 返回存储在 key 处的排序集中成员的排名，分数从低到高排序。排名（或索引）从 0 开始\nZREVRANK key member [WITHSCORE]\t分数高到低排序 WITHSCORE ：携带分数\tO(log(N))\nZCARD key 返回存储在 key 处的排序集的排序集基数（元素数量）O(1)\nZCOUNT key min max 返回排序集中 key 上得分介于 min 和 max 之间的元素数量 O(log(N)) N 是排序集中的元素数量\nZINCRBY key increment member 按增量增加存储在 key 处的排序集中成员的分数 O(log(N))，其中 N 是排序集中的元素数量\n​\t返回成员的新分数\nZRANGE key start stop [BYSCORE | BYLEX] [REV] [LIMIT offset count] [WITHSCORES]\nO(log(N)+M) N 是排序集中的元素数量，M 是返回的元素数量\n返回存储在 的排序集中指定范围的元素。\n可以执行不同类型的范围查询：按索引（排名）、按分数或按字典顺序\nZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] O(log(N)+M)\n返回排序集中 key 上得分在 min 和 max 之间的所有元素（包括得分等于 min 或 max 的元素）。这些元素被认为是从低分到高分排序的，具有相同分数的元素按字典顺序返回\n可以使用括号代表开区间 ( 、)\nZUNION numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE \u0026lt;SUM | MIN | MAX\u0026gt;] [WITHSCORES]\nO(N)+O(M*log(M))，其中 N 是输入排序集大小的总和，M 是结果排序集中的元素数量\n求并集，与 ZUNIONSTORE 类似，但不是存储结果排序集，而是将其返回给客户端\nZDIFF numkeys key [key ...] [WITHSCORES]\nO(L + (N-K)log(N)) 最坏情况，其中 L 是所有集合中的元素总数，N 是第一个集合的大小，K 是结果集的大小\n求交集，与 ZDIFFSTORE 类似，但不是存储结果排序集，而是将其返回给客户端\nZINTER numkeys key [key ...] [WEIGHTS weight [weight ...]] [AGGREGATE \u0026lt;SUM | MIN | MAX\u0026gt;] [WITHSCORES]\nO(N*K)+O(M*log(M)) 最坏情况，其中 N 是最小输入排序集，K 是输入排序集的数量，M 是结果排序集中的元素数量\n求并集，与 ZINTERSTORE 类似，但不是存储结果排序集，而是将其返回给客户端\n3. 数据库 # 3.1 数据结构 # struct redisServer { // ... redisDb *db;\t// 保存所有数据库 int dbnum;\t// 服务器的数据库数量（默认16） struct saveparam *saveparams;\t// 保存条件的数组（bgsave） long long dirty;// 修改计数器，上次save/bgsave后对数据库的修改次数 time_t lastsave;// 上次执行保存的时间 list *clients;\t// 保存所有客户端状态 redisClient *lua_client;\t// lua脚本创建的客户端会被关联到这里，lua客户端只有服务器关闭才会被关闭 // AOF创建的伪客户端在载入完成后关闭 char *masterhost;\t// 主服务器地址 int masterport;\t// 主服务器端口 dict *pubsub_channels;\t// 所有频道的订阅关系 list *pubsub_patterns;\t// 所有模式的订阅关系 //... };\t// 服务器状态 typedef struct redisClient { // ... redisDb *db;\t// 记录客户端正使用的数据库 int fd;\t// 记录正在使用的套接字描述符 robj\t*name;\t// 客户端名称，默认为NULL int\tflags;\t// 标记客户端的角色和目前的状态（多个二进制或） sds querybuf;// 输入缓冲区，最大不超过1GB，否则关闭客户端 robj\t**argv;\t// 数组，每项都是字符串对象，是对querybuf的解析 // argv[0] 存储指令，之后的为参数 int\targc;\t// 记录argv数组的长度 struct redisCommand *cmd;\t// 指向argv[0]所对应的redisCommand结构 // 固定缓冲区 保存长度较小的回复 char buf[REDIS_REPLY_CHUNK_BYTES];\t// 固定缓冲区REDIS_REPLY_CHUNK_BYTES默认16*1024（16KB） int bufpos;\t// 记录buf目前已使用的字节数 // 可变大小缓冲区 list\t*reply;\t// 使用链表连接多个字符串对象 int authenticated;\t// 记录客户端是否通过身份验证 =0时，除了AUTH命令都会被拒绝 time_t ctime;\t// 创建时间 time_t lastinteraction;\t// 与服务器最后一次互动时间 time_t obuf_soft_limit_reached_time;\t// 缓冲区第一次到达软性限制的时间 // 输出缓冲区超过软性限制过久会被关闭客户端，超过硬性限制会被立即关闭 // ... } redisClient;\t// 客户端状态 typedef struct redisDb { // ... dict *dict;\t// 数据库键空间，保存数据库中所有的键值对 dict *expires\t// 过期字典，保存键的过期时间 // ... } redisDb; 读写键空间需要维护：\n更新服务器的键空间命中次数和miss次数 更新键的LRU时间 若已过期，需要删除过期键 被监视的键，需要标记为脏 每次修改键后，需对脏键计数器 +1 若开启了数据库通知功能，在对键修改后，按配置发送相应的数据库通知 过期字典的键为指针，指向某个键对象。值为long long 类型的过期时间（毫秒精度的UNIX时间戳）\n3.2 过期键的删除 # 采用惰性删除和定期删除的策略\n每次读写都需要对输入键进行过期检查，若已经过期，则将其删除 在规定时间内，分多次遍历服务器中的各个数据库，每次都取出一定数量的随机键进行过期键删除。current_db记录检查到数据库的进度。每个数据库最多检查过期键数量为20，若达到时间上限就跳过剩下的键，下一次检查下一个数据库。 3.3 持久化和复制功能对过期键的处理 # RDB：生成时不保存已经过期的键\n​\t载入时，主服务器不载入过期键，从服务器都载入。主服务器通过数据同步也会将从服务器的过期键删除\nAOF：写入时，若键已过期，但未被删除，不会有任何影响。被删除后，会向AOF文件追加一条删除命令\nGET 已过期的 A 键时，服务器执行： 1) 删除 A 键 2) 追加一条删除命令到AOF文件 3) 向执行GET命令的客户端返回空回复 ​\t重写AOF时，已过期的键不会保存到重写后的AOF文件中（忽略掉这个键）\n复制：\n主服务器删除过期键后，向所有从服务器发送一条删除命令 从服务器在执行客户端命令时，即使碰到过期键也不会删除它，像未过期一样处理 只有接收到主服务器的删除命令后才会进行删除 3.4 数据库通知的实现 # 需要发送通知的命令会调用notifyKeyspaceEvent函数发送通知\n若给定的通知类型不是服务器允许的类型，不做任何动作 检测是否允许发送键空间通知（某个键执行了什么命令） 检测是否允许发送键事件通知（某个命令被什么键执行了） 4. RDB持久化 # save 由Redis主进程执行RDB，会阻塞所有命令\nbgsave 子进程执行RDB，避免主进程收到影响，默认为当前目录\nsave、bgsave会被拒绝，防止竞争执行rdbSave函数 bgrewriteaof（也是交给子进程）会被拒绝，出于性能考虑 4.1Redis内部的RDB机制（redis.conf）： # # 900秒内，如果至少有 1 个key被修改，则执行bgsave，如果是save \u0026#34;\u0026#34; 表示禁用RDB save 900 1 save 300 10 save 60 10000 上面三个语句会生成三个saveparam： struct saveparam { time_t seconds;\t// 秒数 int changes;\t// 修改数 };\t# 是否压缩，建议不开启（会消耗cpu） rdbcompression yes # RDB文件名称 dbfilename dump.rdb # 文件保存路径目录 dir ./ 载入是服务器启动时自动执行的，若开启了AOF持久化，则会优先使用AOF文件还原数据库\nserverCron函数默认100毫秒执行一次，其中一项工作就是检查save选项设置的条件是否已经满足[遍历所有条件]，满足就执行bgsave命令\n4.2 RDB文件结构 # REDIS、db_sersion、databases、EOF、check_sum\nREDIS 5字节 保存着REDIS五个字符，检查文件是否是RDB文件 db_version 4字节 字符串表示的整数，0006 表示第六版 databases 0或任意个数据库 EOF 1字节 标志RDB文件内容结束 check_sum 8字节 无符号整数，校验和，检查文件是否出错或损坏 databases部分 # 每个非空数据库可以表示为：SELECTDB、db_number、key_value_pairs\nSELECTDB 1字节 标记下面的是数据库号码 db_number 1\\2\\5字节 数据库号码 key_value_pairs 所有键值对 key_value_pairs部分 # 不带过期时间：TYPE、key、value\n带过期时间：EXPIRETIME_MS、ms、TYPE、key、value\nEXPIRETIME_MS 1 字节 告知接下来为毫秒为单位的过期时间 ms 8 字节 UNIX时间戳，过期的时间 5. AOF持久化 # 保存服务器所执行的写命令形成AOF文件\n5.1 实现 # 命令追加、文件写入、文件同步\nAOF被打开时，服务器执行一个写命令之后，会将其协议格式的命令追加到 redisServer 的 aof_buf 缓冲区末尾\n每个服务器事件循环结束之前，会调用flushAppendOnlyFile文件，考虑是否需要将缓冲区的内容写入和保存到AOF文件中\ndef eventLoop(): # Redis 服务器进程 while True: processFileEvents()\t# 处理文件事件，接收命令请求、发送命令回复 processTimeEvents()\t# 处理事件事件，如定时函数 flushAppendOnlyFile()\t# 考虑是否需要将缓冲区的内容写入和保存到AOF文件中 根据appendfsync配置不同：\nalways 将缓冲区所有内容写入并同步到AOF文件 everysec 默认。将缓冲区所有内容写入AOF文件，若上次同步时间超过一秒，则再次进行AOF同步（由一个线程专门负责） no 将缓冲区所有内容写入AOF文件，但不同步。何时同步由操作系统决定 5.2 载入与还原 # 创建一个不带网络连接的伪客户端 从AOF文件中分析并读取出一条写命令 使用伪客户端执行这条命令 循环上述2-3，直到处理完毕 5.3 AOF重写 # 创建一个新的AOF文件替代现有的AOF文件，新的文件不会包含任何浪费空间的冗余命令\ndef AOF重写： 创建新AOF文件 for 遍历数据库： 忽略空数据库 写入SELECT命令，指定数据库号码 for 遍历所有数据库中的键： 忽略已过期键 按键的类型重写 若带有过期时间，重写过期时间 写入完毕，关闭文件 AOF后台重写：\n使用子进程重写。但有可能打造成数据不一致问题。\nRedis服务器设置了一个AOF重写缓冲区，创建子进程后，当执行了一条写命令后，会同时给AOF缓冲区和AOF重写缓冲区发送.\n子进程完成AOF重写工作之后，向父进程发送一个信号，父进程调用信号处理函数（会阻塞服务器）：\n将AOF重写缓冲区的所有内容写入新AOF文件 对新AOF文件改名，原子地覆盖现有AOF文件 6. 事件 # 6.1 文件事件处理器 # 通过队列传送套接字、依次处理事件\n套接字可读（客户端对套接字执行write/close操作）或有新的可应答套接字出现时，套接字产生AE_READABLE事件\n套接字可写（客户端对套接字执行read操作），套接字产生AE_WRITEABLE事件\n可读又可写时，服务器会先读套接字，再写套接字\n6.2 时间事件 # 定时事件和周期性事件，一个时间事件由id、when、timeProc组成\n服务器将所有时间事件放在一个无序链表中，每当时间事件执行器运行时，遍历整个链表，查找所有已到达的时间事件，并调用相关的事件处理器。正常模式下，只有serverCron一个时间事件，在benchmark模式下只有两个时间事件。\nserverCron事件 默认100ms[可以修改hz选项调整每秒执行次数]：\n更新各类统计信息，如时间，内存占用、数据库占用 清理过期键值对 关闭和清理连接失效的客户端 尝试进行持久化操作 若为主服务器，对从服务器进行定期同步 若处于集群模式，对集群进行定期同步和连接测试 6.3 服务器事件调度 # 时间时间会比设定的到达时间稍晚一些\n7. 复制模式 # SLAVEOF \u0026lt;master_ip\u0026gt; \u0026lt;master_port\u0026gt; 从服务器向主服务器发送PSYNC指令进行初次同步（完整重同步），主服务器创建RDB文件发给从服务器，载入后主服务器发送缓冲区的写命令给从服务器\n部分重同步：断线后重连（再次发送 PSYNC ）后，主服务器将断线期间的写命令发给从服务器\n7.1 复制偏移量、服务器运行ID # 主从服务器都维持一个复制偏移量\n主服务器每次向从服务器传播N字节数据时，自增N 从服务器接收N字节时，自增N 主服务器启动时生成一个40个十六进制字符组成的ID，用于重连时判断是否是原先的主服务器。若不是原来的主服务器则执行完整重同步\n7.2 复制积压缓冲区 # 队列、默认大小1MB、记录了最近传播的写命令，每个字节都记录了复制偏移量\n断线后，从服务器发送 PSYNC（携带复制偏移量）。主服务器根据这个偏移量是否还在复制积压缓冲区中决定执行部分重同步或完整重同步\n按需调整大小：为安全设置为 2 * 平均断线重连时间 * 平均每秒参数的写命令数据量\n7.3 复制的步骤 # 从服务器 设置主服务器的地址和端口\n从服务器 创建连向主服务器的套接字\n主服务器 为该套接字创建相应客户端\n从服务器 向主服务器发送 PING\n从服务器 向主服务器验证身份 AUTH xxxx\n从服务器 发送端口信息\n主服务器 将端口信息记录到套接字对应客户端的 slave_listening_port 属性\n从服务器 发送 PSYNC 命令，执行同步操作，执行之后主服务器也成为从服务器的客户端\n主从服务器进入命令传播阶段\n默认向主服务器每秒发送一次心跳检测：REPLCONF ACK \u0026lt;从服务器复制偏移量\u0026gt;\n检测网络连接状态\n辅助实现 min-slaves 选项\n配置min-salves-to-write 3\n​\tmin-salves-max-lag 10\n在从服务器数量少于3 或从服务器延迟都大于10秒时，主服务器拒绝执行写命令\n检测命令丢失\n复制偏移量不一致，会补发缺失数据\n8. Sentinel # Sentinel（哨兵），一种特殊状态下的Redis服务器。是Redis的高可用性解决方案。可以监视任意多个主服务器及其之下的从服务器。主服务器下线时，自动将下线主服务器的某个从服务器升级为主服务器\n8.1 启动Sentinel # redis-sentinel /path/to/your/sentinel.conf 或 redis-server /path/to/your/sentinel.conf --sentinel 初始化服务器\n不会载入RDB、AOF文件。使用端口号配置名称不同、命令列表不同\nstruct sentinelState{ uint64_t current_epoch;\t// 当前纪元，用于实现故障转移 dict *masters;\t// 保存所有被监视的主服务器，key 名称 value 指向实例的指针 int tilt;\t// 是否进入了TILT模式 int running_scripts;\t// 目前在运行的脚本数 mstime_t tilt_start_time;\t// 进入TILT模式的时间 mstime_t previous_time;\t// 最后一次执行时间处理器的时间 list *scripts_queue;\t// 队列，包含所有需要执行的用户脚本 } sentinel; 将普通Redis服务器使用的代码替换为Sentinel专用代码\n初始化Sentinel状态\n根据给定配置文件，初始化Sentinel的监视主服务器列表\n创建连向主服务器的网络连接\n命令连接——专门用于向主服务器发送命令，接收命令回复 订阅连接——订阅主服务器的 _sentinel_:hello 频道 8.2 获取主服务器信息 # 十秒发送一次INFO命令。获取信息后对实例结构更新\n主服务器本身信息：服务器运行run_id（每次启动不同）、角色 role 主服务器的所有从服务器信息 8.3 获取从服务器信息 # 发现主服务器有新的从服务器后，除了为新从服务器创建结构外，还会创建连接到从服务器的命令连接、订阅连接\n也是十秒发送一次INFO命令\n8.4 向主服务器和从服务器发送信息 # 两秒一次，向hello频道发送信息\n8.5 接收服务器的频道信息 # 所有的Sentienl都可以接收信息，接收后检查是否是Sentienl自己发送的，若是，则丢弃\n若不是，根据数据对相应主服务器的实例结构更新。发现有新的Sentinel时，创建连向其他Sentinel的命令连接。\nSentienl在连接服务器时需要订阅连接发现其他的Sentinel，而Sentinel之间只需要命令连接通信就行了\n8.6 主观下线 # 一秒一次 Sentinel向所有实例（服务器和Sentinel）发送PING。\n有效回复：+PONG、-LOADING、-MASTERDOWN 无效回复：其他回复或超时（Sentinel配置文件的 down-after-milliseconds，每个Sentinel可以不同） 若实例在超时时间内，连续向Sentinel返回无效回复，那么Sentinel会修改这个实例的结构，flags属性打开SRI_S_DOWN标识，标记为主观下线\n8.7 客观下线 # 判定一个主服务器为主观下线后，会向其他Sentinel进行询问，当有足够数量的主观下线判断后，会执行故障转移操作。\n配置 sentinel monitor master 127.0.0.1 6379 2 中的 2 表示主观下线超过2则为客观下线（每个Sentinel可以不同）\n8.8 选举领头Sentinel # 客观下线后，监视这个主服务器的Sentinel会选举一个领头Sentinel进行故障转移操作。\n主服务器下线时长超过设定的下线时长上限后，对主服务器执行故障转移操作：\n挑选一个从服务器升级 向所有原从服务器发送复制命令，让他们成为新主服务器的从服务器。所有从服务器都开始复制新主服务器时，故障转移操作执行完毕 已下线的主服务器重新上线时，设置为从服务器 期间向从服务器发送INFO的频率改为每秒一次\n9. 集群 # 节点通过握手来将其他节点添加到自己所处的集群中\n集群的16384个槽可以分配给集群的各个节点，每个节点都记录哪些槽指派给自己，哪些被指派给其他节点\n节点接收到请求时，先检查要处理的键是否由自己负责\n不是：返回MOVED错误，指引客户端转向正在负责相关槽的节点 是：自己处理 重新分片：将某个槽的所有键值对由一个节点转移到另一个节点\n若节点A正在迁移槽i到节点B，当A没在自己的数据库找到命令指定键时，返回ASK错误，指引客户端到节点B继续查找\n主从复制类似上面的非集群的主从复制\n集群通过发送/接收消息进行通信（MEET、PING、PONG、PUBLISH、FALL）\n10. 发布与订阅 # 10.1 订阅 # SUBSCRIBE 命令可以让客户端订阅一个或多个频道：每当其他客户端向频道发送消息，频道的所有订阅者都会收到\nPSUBSCRIBE 命令可以订阅一个或多个模式：模式会匹配频道（类似正则表达式）订阅了模式的客户端会收到匹配的频道信息\nstruct redisServer { // ... dict *pubsub_channels;\t// 所有频道的订阅关系，key为频道名称，value指向所有客户端的链表 list *pubsub_patterns;\t// 所有模式的订阅关系，指向pubsubPattern链表 //... };\t// 服务器状态 订阅频道\n若频道已存在，添加客户端到链表末尾 若不存在，为频道创建一个键，再将客户端添加到链表 UNSUBSCRIBE 退订频道\n查找频道对应的客户端链表，删除退订客户端信息、 若删除后成为空链表，则删除频道对应的键 订阅模式——新建 pubsubPattern 结构，添加到链表尾\ntypedef struct pubsubPattern { redisClient *client;\t// 指向客户端 robj *pattern;\t// 被订阅的模式 } PUNSUBSCRIBE 退订模式\n查链表，删除目标客户端订阅的目标模式\n10.2 发送消息 # 客户端执行：PUBLISH \u0026lt;channel\u0026gt; \u0026lt;message\u0026gt;，服务器执行：\n将message消息发送给channel频道的所有订阅者（遍历频道键的链表） 若有一个或多个pattern模式与channel频道匹配，将消息发送给pattern模式的订阅者 10.3 查看订阅消息 # PUBSUB CHANNELS [pattern] 返回服务器当前被订阅的频道，pattern可选\nPUBSUB NUMSUB [channel-1 channel-2 ...] 返回频道的订阅者数量(链表长度)\nPUBSUB NUMPAT 返回服务器当前被订阅的模式数量\n11. 慢查询日志、监视器 # 11.1 慢查询日志 # 用于记录执行时间超过给定时长【slowlog-log-slower-than选项，指定执行时间的最长时间(单位微秒)】的命令请求\n【slowlog-max-len选项，指定服务器最多保存多少条慢查询日志】\nSLOWLOG GET 查看慢查询日志\nstruct redisServer { // ... long long slowlog_entry_id;\t// 下一条慢查询日志的id，初始为0 list *slowlog;\t// 保存所有的慢查询日志的链表，保存slowlogEntry结构 // 打印和删除就是通过这个链表实现 long long slowlog_log_slower_than; unsigned long slowlog_max_len; redisClient *monitors;\t// 指向所有监视器\t//... };\t// 服务器状态 typedef struct slowlogEntry { long long id;\ttime_t time;\t// 命令执行时间 long long duration;\t// 消耗时间 robj **argv;\t// 命令与命令参数 int argc;\t// 命令与命令参数的数量 } slowlogEntry; 每次执行命令的前后，程序都会记录当前时间的UNIX时间戳，执行后，会检查是否需要创建新的慢查询日志\n11.2 监视器 # 客户端发送 MONITOR 命令，可以使之成为一个监视器（添加到monitors链表尾），实时接收并打印出服务器当前处理的命令请求相关信息\n每当客户端向服务器发送一条命令请求时，服务器都会向所有监视器发送这条命令的请求信息\n12. 事务 # MULTI、EXEC、WATCH 在执行MULTI之后提交一系列命令，再使用EXEC命令执行事务\n12.1 事务的实现 # 开始：MULTI命令将执行该命令的客户端从非事务状态切换为事务状态，通过flags属性中打开 REDIS_MULTI 标识完成\n命令入队：若处于客户端事务状态，其发送的EXEC、DISCARD、WATCH、MULTI命令会立即执行。不是这些命令，则将命令放入事务队列。\ntypedef struct redisClient { // ... multiState mstate;\t// 事务状态 // ... } redisClient; typedef struct multiState { multiCmd *commands;\t// 事务队列 int count;\t// 已入队命令计数 } multiState; typedef struct multiCmd { robj **argv;\t// 参数 int argc;\t// 参数数量 struct redisCommand *cmd;\t// 命令指针 } multiCmd; 执行事务：EXEC命令，先判断客户端的事务安全性，若安全，服务器会遍历这个客户端的事务队列，执行队列中保存的所有命令，最后将执行命令所得的所有结果全部返回。否则拒绝执行客户端提交的事务\nWATCH：乐观锁，可以再EXEC之前，监视任意数量的数据库键，在EXEC执行时，检查被监视的键是否至少有一个已经被修改过，若是，则服务器拒绝执行事务，向客户端返回代表事务执行失败的空回复。\ntypedef struct redisDb { // ... dict *watched_keys;\t// 正在被WARTCH命令监视的键，值是一个链表（记录所有监视相应数据的客户端） // ... } redisDb; 监视机制的触发：所有对数据库进行修改的命令，在执行后都需要对上述字典进行检查【是否有客户端正在监视的键被修改，若有，会将监视该键的客户端的REDIS_DIRTY_CAS标识打开，表示该客户端事务安全性已被破坏】\n12.2 事务的ACID性质 # 原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）\n原子性：事务要么整体完成，要么一个都不执行。Redis不支持事务回滚，作者认为执行的错误都是编程错误产生，很少在生产环境中。\n一致性：事务在执行前，数据库是一致的，那么在事务执行之后，无论是否执行成功，数据库也应该是一致的。一致是指数据符合数据库本身的定义和要求，没有包含非法或无效的错误数据。\nRedis保证数据一致性：\n入队错误\n一个事务在命令入队时，出现命令不存在或命令格式不对等情况时，Redis将拒绝执行这个事务\n执行错误\n事务在执行过程中的错误（最有可能的是对键的类型不符合指令形式的错误，如事务中包含：）\nSET msg \u0026#34;hello\u0026#34; RPUSH msg \u0026#34;good bye\u0026#34; \u0026#34;bye bye\u0026#34; 出错的命令不会对数据库进行修改，也不会影响一致性\n服务器停机\n无持久化的内存模式：重启后服务器为空，数据是一致的 RDB模式：服务器会根据RDB文件恢复数据，还原到一致的状态。若没有可用的RDB文件，重启后数据库是空白的。 AOF模式：与RDB一致 隔离性：数据库中多个事务并发地执行，各个事务之间也不会互相影响，且在并发状态下执行的事务和串行状态下执行的事务产生的结果完全相同。Redis是单线程执行事务，且服务器保证在执行事务期间不会对事务进行中断。因此Redis事务总是串行的。\n持久性：当一个事务执行完毕时，执行这个事务所得的结果已经保存到永久性存储介质里了，即使服务器在事务执行后停机，事务所得的结果也不会丢失。\n​\t由Redis所使用的持久化模式决定：\n无持久化的内存模式：事务不具有持久性 RDB模式：不具备持久性 AOF模式，且appendsync的选项值为always【执行命令后同步】：具备持久性 AOF模式，且appendsync的选项值为everysec【每秒同步一次】：不具备持久性 AOF模式，且appendsync的选项值为no【交由操作系统决定何时将命令数据同步】：不具备持久性 不论Redis在什么模式，在事务的最后加上SAVE命令可以保证事务的持久性 13. 常见问题 # 13.1 大Key # String 类型，\u0026gt;10kb\nhash、list、set、zset 元素数量超过 5k/1w/.\n检测：\nredis-cli \u0026ndash;bigkeys 该命令是redis自带，但是只能找出五种数据类型里最大的key。建议在从节点使用，一般不使用 https://help.aliyun.com/zh/redis/user-guide/identify-and-handle-large-keys-and-hotkeys\npython删除\npip install redis 把所有key拿到：\nimport redis pool=redis.ConnectionPool( host=\u0026#39;redis_hostname\u0026#39;, port=6379, max_connections=100) r = redis.StrictRedis(connection_pool=pool) cursor_number, keys = r.execute_command(\u0026#39;scan\u0026#39;, 0, \u0026#34;count\u0026#34;, 200000) while True: if cursor_number == 0: # 结束一次完整的遍历 break cursor_number, keys = r.execute_command(\u0026#39;scan\u0026#39;, cursor_number, \u0026#34;count\u0026#34;, 200000) # do something with keys 假设key保存到了文件中：\nimport redis import time pool=redis.ConnectionPool(host=\u0026#39;redis_hostname\u0026#39;, port=6379, max_connections=100) r = redis.StrictRedis(connection_pool=pool) start_time = time.time() SUCCESS_DELETED = 0 with open(\u0026#34;/data/rediskeys\u0026#34;) as kf: while True: # 最多读取 1 MB 内容 lines = kf.readlines(1024*1024) if not lines: break else: # 获取 lines 中以 \u0026#34;UGC:TASKKEY\u0026#34; 开头的行，并去除每行的首尾空格和换行符 taskkey_list = [i.strip() for i in lines if i.startswith(\u0026#34;UGC:TASKKEY\u0026#34;)] SUCCESS_DELETED += r.delete(*taskkey_list) print SUCCESS_DELETED end_time = time.time() print end_time - start_time, SUCCESS_DELETED 指令删除\nredis-cli --scan --pattern \u0026#34;ops-coffee-*\u0026#34; | xargs -L 2000 redis-cli del 扫描匹配以 \u0026ldquo;ops-coffee-\u0026rdquo; 开头的键，并将它们作为参数传递给 redis-cli del 命令来删除这些键\n每次删除 2000 个键，直到所有匹配的键都被删除\n13.2 缓存三兄弟 # 缓存穿透：\n不断查询数据库中都没有的数据，导致每次请求都会到数据库，压垮数据库\n解决：布隆过滤器\n当一个元素被加入集合时，通过k个散列函数将元素映射为一个位数组中的k个点，置为1\n检索时，若这些点有任何一个是0，则被检元素一定不存在。否则很可能存在\n缓存击穿：\n一个热点key在失效的同时，大量的请求过来，从而会全部到达数据库，压垮数据库\n设置热点数据永不过期 定时更新热点key过期时间 互斥锁、分布式锁：在请求MySQL数据库时上互斥锁，将获取的数据缓存后释放锁 缓存雪崩：\n缓存的数据大面积同时失效，或者Redis宕机，从而会导致大量请求直接到数据库，压垮数据库\n设置有效期时增加随机值 数据预热：对于即将来临的大量请求，将数据提前缓存在Redis中，并设置不同的过期时间 ","externalUrl":null,"permalink":"/docs/redis/redis/","section":"Docs","summary":"Redis入门 # SQL与NoSQL区别 SQL NoSQL 数据结构 结构化 非结","title":"Redis入门与原理","type":"docs"},{"content":" 1.快速入门 # 引入依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 建立连接\nprivate Jedis jedis; @BeforeEach void setUp(){ // 建立连接 jedis = new Jedis(\u0026#34;127.0.0.1\u0026#34;, 6379); // 配置密码 --- 需要密码才能使用 jedis.auth(\u0026#34;123456\u0026#34;); // 选择库 jedis.select(0); } 测试string\n@Test void testString(){ // 插入数据，方法名就是redis命令名称 String result = jedis.set(\u0026#34;name\u0026#34;, \u0026#34;TCmatj\u0026#34;); System.out.println(\u0026#34;result = \u0026#34; + result); // 获取数据 String name = jedis.get(\u0026#34;name\u0026#34;); System.out.println(\u0026#34;name = \u0026#34; + name); } 释放资源\n@AfterEach void tearDown(){ // 释放资源 if(jedis != null){ jedis.close(); } } 2.Jedis连接池 # Jedis 本身是线程不安全的，且频繁创建和销毁连接会有性能损耗，因此推荐使用 Jedis 连接池代替 Jedis 的直连方式\npublic class JedisConnectionFactory { private static final JedisPool jedisPool; static { JedisPoolConfig jedisPoolConfig = new JedisPoolConfig(); // 最大连接 jedisPoolConfig.setMaxTotal(8); // 最大空闲连接 jedisPoolConfig.setMaxIdle(8); // 最小空闲连接 jedisPoolConfig.setMinIdle(0); // 最长等待时间 jedisPoolConfig.setMaxWait(Duration.ofMillis(200)); jedisPool = new JedisPool(jedisPoolConfig, \u0026#34;127.0.0.1\u0026#34;, 6379, 1000, \u0026#34;123456\u0026#34;); } // 获取Jedis对象 public static Jedis getJedis(){ return jedisPool.getResource(); } } // 测试代码修改 jedis = JedisConnectionFactory.getJedis(); 3.Spring Data Redis # https://spring.io/projects/spring-data-redis\n提供对不同Redis客户端的整合（Lettuce 和 Jedis） 提供RedisTemplate统一API操作Redis 支持Redis发布订阅模型 支持哨兵和集群 支持基于Lettuce的响应式编程 支持基于JDK、JSON、字符串、Spring对象的数据序列化和反序列化 支持基于Redis的JDKCollection实现 1）快速启动 # 1.引入依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--连接池--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-pool2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.配置文件 配置Redis数据源\napplication.yaml\nspring: redis: host: 127.0.0.1 port: 6379 password: 123456 lettuce: pool: max-active: 8 max-idle: 8 min-idle: 0 max-wait: 100ms 3.注入RedisTemplate 编写配置类，创建RedisTemplate对象\n@Configuration @Slf4j public class RedisConfiguration { @Bean public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory){ log.info(\u0026#34;开始创建redis模板对象...\u0026#34;); RedisTemplate redisTemplate = new RedisTemplate(); // 设置redis的连接工厂对象 redisTemplate.setConnectionFactory(redisConnectionFactory); // 设置redis key的序列化器 redisTemplate.setKeySerializer(new StringRedisSerializer()); return redisTemplate; } } @Autowired private RedisTemplate redisTemplate; 4.编写测试\n@Test void testString() { Object name = redisTemplate.opsForValue().get(\u0026#34;name\u0026#34;); System.out.println(name); redisTemplate.opsForValue().set(\u0026#34;name1\u0026#34;, new int[]{1,2,3}); System.out.println(redisTemplate.opsForValue().get(\u0026#34;name1\u0026#34;)); } 2）序列化与反序列化 # @Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory connectionFactory){ // 创建对象 RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; } } @Test void testString() { Object name = redisTemplate.opsForValue().get(\u0026#34;name\u0026#34;); System.out.println(name); redisTemplate.opsForValue().set(\u0026#34;name\u0026#34;, \u0026#34;TCDDDDDDD\u0026#34;); System.out.println(redisTemplate.opsForValue().get(\u0026#34;name\u0026#34;)); } @Test void testSaveUser(){ redisTemplate.opsForValue().set(\u0026#34;user:100\u0026#34;, new User(\u0026#34;TC\u0026#34;, 32)); User o = (User) redisTemplate.opsForValue().get(\u0026#34;user:100\u0026#34;); System.out.println(o); } 此时就可以正确存入与取出 name 的键值对了，也对对象支持：会把类的字节码也进行存储\n问题：过多占用空间，所有不会使用JSON序列化器来处理value，统一使用String序列化器，要求只能存储String类型的键值对。当需要存储对象时，手动完成序列化与反序列化。\nSpringRedisTemplate\nSpring默认提供此类，序列化方式默认都是String方式，省去自定义RedisTemplate的过程\n@Autowired private StringRedisTemplate springRedisTemplate; private static final ObjectMapper mapper = new ObjectMapper(); @Test void testSaveUser() throws JsonProcessingException { // 创建对象 User user = new User(\u0026#34;TC\u0026#34;, 25); // 手动序列化 String json = mapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(\u0026#34;user:200\u0026#34;, json); // 获取数据 String jsonUser = stringRedisTemplate.opsForValue().get(\u0026#34;user:200\u0026#34;); // 手动反序列化 User user1 = mapper.readValue(jsonUser, User.class); System.out.println(user1); } ","externalUrl":null,"permalink":"/docs/redis/redis%E4%BD%BF%E7%94%A8/","section":"Docs","summary":"1.快速入门 # 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 建立连接 private Jedis jedis; @BeforeEach void setUp(){ // 建立连","title":"Redis使用","type":"docs"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":" Spring Boot # 简化 Spring 框架开发\n[toc]\n1. 创建项目 # 勾选 Web --\u0026gt; Spring Web\n2. 注解 # @SpringBootApplication 标注一个 Spring Boot 应用\n@SpringBootConfiguration\t将该类声明为配置类\n@EnableAutoConfiguration 启⽤Spring Boot的⾃动配置\n@ComponentScan\t启⽤组件扫描，参见1\n@RestController 标注一个请求处理类 = @Controller + @ResponseBody\n@RequestMapping(\u0026quot;/depts\u0026quot;) 标注接口请求路径，可以只在类级别上使用此注解，以便于指定基本路径\n@GetMapping\\@PostMapping\\@PutMapping\\@DeleteMapping\\@PatchMapping 在每个处理方法上使用更具体的注解\n@RequestParam 方法形参名称与请求参数名称不匹配时映射请求参数 参见1 参见2\n@DateTimeFormat 日期参数格式转换 参见1\n@RequestBody 将JSON格式的请求数据封装到一个实体对象 参见1\n@PathVariable 获取路径参数，请求路径也需要用大括号括起参数 参见1\n@ResponseBody 作用在Controller方法/类上，将方法返回值直接响应，若返回值是实体对象/集合，会转为JSON格式响应\n@Component 将类交给 IOC 容器管理参见1 @Controller、@Service、@Repository 参见\n@Autowired 注入运行时依赖的对象参见1\n@ComponentScan 组件扫描注解参见1\n@Mapper 在运行时，自动生成该接口的实现类对象（代理对象），并且将该对象交给 IOC 容器管理\n@Congiguration 告知Spring这是⼀个配置类，会为Spring应⽤上下⽂提供bean\n@Bean 表明方法所返回的对象会以bean的形式添加到Spring的应⽤上下⽂中。Spring Boot会自动装配，不需要显示声明配置类和bean\n@ConfigurationProperties 和 @Value 用于获取配置文件中的属性定义并绑定到Java Bean或属性中参见1\n@EnableScheduling 开启任务调度功能，与 @Scheduled 搭配使用参见1\n实体、属性注解\n@Data lombok注解，⽣成所有缺失的⽅法，同时还会⽣成所有以final属性作为参数的构造器 @Slf4j lombok注解，在运⾏时，它会在这个类中⾃动⽣成 ⼀个SLF4J（Simple Logging Facade for Java）Logger @NoArgConstructor lombok注解，无参构造 @AllArgConstructor lombok注解，全参构造 @RequiredArgConstructor lombok注解，对需要的变量（@NonNull）的构造 @JsonProperty jackson包注解,用于实体类的属性上, 功能是把属性名称转换为另一个名称(即,两个名称都指向同一个变量值) @Id JPA/Redis 或其它数据库的注解，作用是将实体类中的某个字段标识为主键字段、 @GeneratedValue JPA注解，用于自动生成主键值，可以指定策略 @Entity JPA注解，定义实体，将映射到指定的数据库表参见1 @RedisHash Redis 注解，定义实体参见1 @Table(name = \u0026quot;tbl_user\u0026quot;) JPA注解，指定和哪个数据表对应;如果省略默认表名就是类名小写 @Column 改变class中字段名与db中表的字段名的映射规则 3. 起步依赖 # web 起步依赖和 test 起步依赖的版本由父工程决定\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.10\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 4. 请求响应 # 前端发起的请求经过 DispatcherServlet（核心/前端控制器） 再转给各个 Controller 程序，处理后再进行返回给 DispatcherServlet ，DispatcherServlet 再响应浏览器\n4.1 请求： # Tomcat服务器对请求数据解析，将解析后的数据封装到 HttpServletRequest 对象（请求对象）中，应用程序可以从该对象中获取数据。\n4.1.1 简单参数 # 使用 GET 方法在 url 携带参数 http://localhost:8080/test?name=Tom\u0026amp;age=10 使用 POST 方法在 Body 中的 x-www-formurlencoded 中携带参数 http://localhost:8080/test2 x-www-formurlencoded: name TC2023 age 24\n@RestController public class RequestController { @RequestMapping(\u0026#34;/test\u0026#34;) public String test(String name, Integer age){ System.out.println(name + \u0026#34;: \u0026#34; + age ); return \u0026#34;OK\u0026#34;; } } 请求参数不一致的情况，使用@RequestParam(name = \u0026quot;name\u0026quot;)注解将 \u0026quot;name\u0026quot; 对应上，使用注解后，该请求参数必须传递。如果该参数是可选的，要将 required 属性设置为 false : @RequestParam(name = \u0026quot;name\u0026quot;, required = false) 回 @RequestMapping(\u0026#34;/test2\u0026#34;) public String test2(@RequestParam(name = \u0026#34;name\u0026#34;) String username, Integer age){ System.out.println(username + \u0026#34;: \u0026#34; + age ); return \u0026#34;OK\u0026#34;; } 4.1.2 实体参数 # 请求参数名与形参对象属性名相同，按照对象层次结构关系可以嵌套POJO属性参数 GET:http://localhost:8080/simplePojo?name=Tom\u0026amp;age=10 POST:http://localhost:8080/simplePojo x-www-formurlencoded:\nname TC2023 age 24 address.province 浙江 address.city 杭州\n@RequestMapping(\u0026#34;/simplePojo\u0026#34;) public String simplePojo(User user){ System.out.println(user.getName() + \u0026#34;: \u0026#34; + user.getAge()); return \u0026#34;OK\u0026#34;; } 4.1.3 数组集合参数 # 数组参数：请求参数名与形参数组名称一致，且请求参数为多个，定义数组类型的形参即可 GET:http://localhost:8000/arrayParam?hobby=game\u0026amp;hobby=java POST:http://localhost:8000/arrayParam x-www-formurlencoded: hobby game hobby java\n@RequestMapping(\u0026#34;/arrayParam\u0026#34;) public String arrayParam(String[] hobby){ System.out.println(Arrays.toString(hobby)); return \u0026#34;OK\u0026#34;; } 集合参数：请求参数名与形参数组名称一致，且请求参数为多个，@RequestParam 绑定关系 回 GET:http://localhost:8000/listParam?hobby=game\u0026amp;hobby=java POST:http://localhost:8000/listParam x-www-formurlencoded:\nhobby game hobby java\n@RequestMapping(\u0026#34;/listParam\u0026#34;) public String listParam(@RequestParam List\u0026lt;String\u0026gt; hobby){ System.out.println(hobby); return \u0026#34;OK\u0026#34;; } 4.1.4 日期参数 # 使用 @DateTimeFormat 注解完成日期参数格式转换 回 GET:http://localhost:8000/dateParam?updateTime=2023-05-19 20 :19:50，这里 05 和 5 都可以运行，结果不一样\n@RequestMapping(\u0026#34;/dateParam\u0026#34;) public String dateParam(@DateTimeFormat(pattern = \u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;)LocalDateTime updateTime){ System.out.println(updateTime); return \u0026#34;OK\u0026#34;; } 4.1.5 Json参数 # @RequestBody 注解，将JSON格式的请求数据封装到一个实体对象 回 POST:http://localhost:8000/JsonParam Body:\n{ \u0026#34;name\u0026#34;:\u0026#34;TC2023\u0026#34;, \u0026#34;age\u0026#34;:10, \u0026#34;address\u0026#34;:{ \u0026#34;province\u0026#34;:\u0026#34;浙江\u0026#34;, \u0026#34;city\u0026#34;:\u0026#34;杭州\u0026#34; } } @RequestMapping(\u0026#34;/JsonParam\u0026#34;) public String JsonParam(@RequestBody User user){ System.out.println(user); return \u0026#34;OK\u0026#34;; } 4.1.6 路径参数 # @PathVariable 获取路径参数，请求路径也需要用大括号括起参数，可以获取多个路径参数，用 / 分隔 回\nGET:http://localhost:8000/path/1\n@RequestMapping(\u0026#34;/path/{id}\u0026#34;) public String Path(@PathVariable Integer id{ System.out.println(id); return \u0026#34;OK\u0026#34;; } 4.2 响应： # 通过 HttpServletResponse 对象（响应对象）设置要响应的数据。\n@ResponseBody 作用在Controller方法/类上，将方法返回值直接响应，若返回值是实体对象/集合，会转为JSON格式响应\n统一响应结果：\npackage com.matj.springbootweb.pojo; public class Result { private Integer code; //1 成功，0 失败 private String msg; //提示信息 private Object data; //数据 public Result() { } public Result(Integer code, String msg, Object data) { this.code = code; this.msg = msg; this.data = data; } public Integer getCode() { return code; } public void setCode(Integer code) { this.code = code; } public String getMsg() { return msg; } public void setMsg(String msg) { this.msg = msg; } public Object getData() { return data; } public void setData(Object data) { this.data = data; } public String toString() { return \u0026#34;Result{code = \u0026#34; + code + \u0026#34;, msg = \u0026#34; + msg + \u0026#34;, data = \u0026#34; + data + \u0026#34;}\u0026#34;; } //静态方法，便于使用 public static Result success(Object data){ return new Result(1,\u0026#34;success\u0026#34;, data); } public static Result success(){ return new Result(1,\u0026#34;success\u0026#34;, null); } public static Result success(String msg){ return new Result(1,msg, null); } } 解析xml文件依赖：dom4j\n4.3 分层解耦 # 4.3.1 三层架构 # 增强拓展性和可维护性\nController 控制层，接收请求、响应数据 Service 业务逻辑层，逻辑处理 Dao 数据访问层（Data Access Object）（持久层），增删改查 4.3.2 分层解耦（IOC-DI引入） # 控制反转 IOC(Inversion Of Control) ：对象的创建控制权由程序自身转移到外部（容器）的思想 依赖注入 DI(Dependency Injection) ：容器为应用程序提供运行时所依赖的资源 Bean 对象：IOC 容器中创建、管理的对象\n步骤： # Service 层及 Dao 层的实现类，交给 IOC 容器管理 类之前加上 @Component 回\n为 Controller 及 Service 注入运行时依赖的对象 把 new Service 和 new Mapper 改为声明，放在 Controller 和 Service 类的开始，并加上注解 @Autowired 回\n@Autowired private EmpService empService;//EmpService是一个接口类型，不用与我们定义的类（实现了EmpService接口）一致 运行测试\nBean对象的声明 # 注解 说明 位置 @Component 声明Bean的基础注解 不属于以下三类时，使用此注解 @Controller 衍生注解 标注在控制器类上 @Service 衍生注解 标注在业务类上 @Repository 衍生注解 标注在数据访问类上（由于与mybatis整合，用的少） 默认的 Bean 名为首字母小写的类名，可以在注解后添加 (value = \u0026quot;Bean名\u0026quot;) 来修改名称，value= 可以省略，IDEA 在运行时可以点击 Endpoints-\u0026gt;Beans-\u0026gt;application 查看所有的 Bean 对象\nBean 组件扫描 要想bean声明生效，还需要被组件扫描注解 @ComponentScan 扫描，已经包含在启动类注解中（如下），默认扫描范围是启动类所在包及其子包 回\n@ComponentScan( excludeFilters = {@Filter( type = FilterType.CUSTOM, classes = {TypeExcludeFilter.class} ), @Filter( type = FilterType.CUSTOM, classes = {AutoConfigurationExcludeFilter.class} )} ) 显式声明扫描的包：（不推荐使用）\n在启动类注解上面加上注解 @ComponentScan({\u0026quot;dao\u0026quot;,\u0026quot;com.tc2023\u0026quot;}) 加上注解就需要把现在的包包括进去\nBean注入 @Autowired 注解，默认是按照类型进行，如果存在多个相同的 Bean，就会报错。解决：\n@Primary 标注生效的 Bean @Qualifier 在 @Autowired 上面加上，指定属性为需要的 Bean @Resource 将 @Autowired 替换，添加属性(name=\u0026quot;名称\u0026quot;) 默认按照名称注入，由JDK提供 5.注解详情 # 1、 @ConfigurationProperties和**@Value**注解用于获取配置文件中的属性定义并绑定到Java Bean或属性中 # 回\n添加依赖：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-configuration-processor\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; 简单使用：\n@Data @Configuration @ConfigurationProperties(prefix = \u0026#34;mail\u0026#34;) public class ConfigProperties { private String hostName; private int port; } 可以在配置文件配置：\nmail: hostname: host@mail.com port: 9000 如果没有加@Configuration注解，需要在主类中添加@EnableConfigurationProperties注解来绑定属性到POJO中，如下： @SpringBootApplication @EnableConfigurationProperties(ConfigProperties.class) public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 与@ConfigurationPropertiesScan注解一起使用回 从Spring Boot 2.2 开始，Spring通过类路径扫描查找并注册@ConfigurationProperties类。因此，不需要使用@Component（以及其他元注释，如@Configuration） 来注释此类类，甚至不需要使用@EnableConfigurationProperties： @Data @ConfigurationProperties(prefix = \u0026#34;mail\u0026#34;) public class ConfigProperties { private String hostName; private int port; private String from; } // @SpringBootApplication启用的类路径扫描器找到了ConfigProperties类，即使我们没有用@Component注释这个类。 此外，我们可以使用的@ConfigurationPropertiesScan 注释扫描配置属性类的自定义位置：\n@SpringBootApplication @ConfigurationPropertiesScan(\u0026#34;com.xxx.configurationproperties\u0026#34;) public class EnableConfigurationDemoApplication { public static void main(String[] args) { SpringApplication.run(EnableConfigurationDemoApplication.class, args); } 2、@EnableScheduling 回 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; @EnableScheduling :加在类上方,表示开启定时任务\n@Scheduled(fixedDelay=50) :加在方法上,并设定定时时间（毫秒），可以用cron表达式: @Scheduled(cron = “0 0 2 * * ?”)　//每天凌晨两点执行\n3、@RedisHash 回 # 在实体中需要将某个属性标识为唯一id\n新建存储库接口，并继承CrudRepository接口实现相关方法。\n@Repository public interface TestRepository extends CrudRepository\u0026lt;TestEntity[对象类型], String[键类型]\u0026gt; { } 使用实体：\n@PostMapping(value = \u0026#34;/test/save\u0026#34;) public ResultData testEntity() { TestEntity testEntity = new TestEntity(); testEntity.setPhone(\u0026#34;18803838082\u0026#34;); testEntity.setName(\u0026#34;sun\u0026#34;); testEntity.setTime(60L); testEntityDao.save(testEntity); return ResultData.ok(); } 4、@Entity 回 # 在实体中需要将某个属性标识为唯一id，可以指定表名（默认为类名小写，驼峰加下划线）\n新建存储库接口，并继承CrudRepository接口实现相关方法。\n@Repository public interface TestRepository extends CrudRepository\u0026lt;TestEntity[对象类型], String[主键类型]\u0026gt; { } 6.自定义注解 AOP+反射处理注解 # /** * 自定义注解，用于标识某个方法需要进行功能字段自动填充处理 */ @Target(ElementType.METHOD) // 指明注解只能加在方法上 @Retention(RetentionPolicy.RUNTIME)\t// 注解保留期限 SOURCE \u0026lt; CLASS \u0026lt; RUNTIME public @interface AutoFill { // 数据库操作类型 : UPDATE INSERT OperationType value(); } /** * 自定义切面类，实现公共字段自动填充处理逻辑 */ @Aspect @Component @Slf4j public class AutoFillAspect { /** * 切入点 */ @Pointcut(\u0026#34;execution(* com.sky.mapper.*.*(..)) \u0026amp;\u0026amp; @annotation(com.sky.annnotation.AutoFill)\u0026#34;)// 切入点表达式 返回值类型任意 包名.任意类.任意方法(任意参数) 并且 含有指定注解 public void autoFillPointCut(){} /** * 前置通知，在通知中进行公共字段的赋值 */ @Before(\u0026#34;autoFillPointCut()\u0026#34;) public void autoFill(JoinPoint joinPoint){ log.info(\u0026#34;开始进行公共字段自动填充..{}\u0026#34;, joinPoint); // 获取当前被拦截到的方法上的操作类型 MethodSignature signature = (MethodSignature) joinPoint.getSignature(); // 方法签名对象 AutoFill autoFill = signature.getMethod().getAnnotation(AutoFill.class); // 获取方法上的注解对象 OperationType operationType = autoFill.value(); // 获得数据库操作类型 // 获取被拦截方法的参数--实体对象 Object[] args = joinPoint.getArgs(); if(args == null || args.length == 0){ return; } Object entity = args[0]; // 准备赋值的数据 LocalDateTime now = LocalDateTime.now(); Long currentId = BaseContext.getCurrentId(); // 为公共字段赋值--反射 if(operationType == OperationType.INSERT){ try { Method setCreateTime = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_CREATE_TIME, LocalDateTime.class); Method setCreateUser = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_CREATE_USER, Long.class); // 通过反射赋值 setCreateTime.invoke(entity, now); setCreateUser.invoke(entity, currentId); } catch (Exception e) { e.printStackTrace(); } } try { Method setUpdateTime = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_UPDATE_TIME, LocalDateTime.class); Method setUpdateUser = entity.getClass().getDeclaredMethod(AutoFillConstant.SET_UPDATE_USER, Long.class); setUpdateTime.invoke(entity, now); setUpdateUser.invoke(entity, currentId); } catch (Exception e) { e.printStackTrace(); } } } SpringBoot原理 # 1. 启动Tomcat、接收请求 # 目标是让下面的请求路径可以访问到\n@RestController public class UserController { @GetMapping(\u0026#34;/test\u0026#34;) public String test() { return \u0026#34;test pass! hello to springboot!\u0026#34;; } } @TestSpringBootApplication //@ComponentScan(\u0026#34;com.tcmatj.user\u0026#34;) // 重要--添加到了注解内 public class MyApplication { public static void main(String[] args) { TestSpringApplication.run(MyApplication.class); } } 用到的依赖：\nspring-webmvc 5.3.26 tomcat-embed-core 9.0.65 启动类注解\n@ComponentScan\t// 默认扫描 run 方法传入类所在包路径 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @ComponentScan\t// 默认扫描 run 方法传入类所在包路径 public @interface TestSpringBootApplication { } 最基本的 run 方法：\n要启动 Tomcat 并使得请求可以接收到，则需要使用 spring-mvc 的 dispatcherServlet 根据请求路径在 Bean 对象中找到并执行 Controller 对应的方法，返回结果 dispatcherSerlet 创建需要用到 Bean 容器。因此先需要创建容器。对启动类注册 register、刷新 refresh public class TestSpringApplication { public static void run(Class clazz) { // 创建 spring 容器（得有我们写的 Bean） AnnotationConfigWebApplicationContext applicationContext = new AnnotationConfigWebApplicationContext(); applicationContext.register(clazz); applicationContext.refresh(); // 启动 Tomcat startTomcat(applicationContext); } private static void startTomcat(WebApplicationContext webApplicationContext) { Tomcat tomcat = new Tomcat(); Server server = tomcat.getServer(); Service service = server.findService(\u0026#34;Tomcat\u0026#34;); Connector connector = new Connector(); connector.setPort(8081); StandardEngine engine = new StandardEngine(); engine.setDefaultHost(\u0026#34;localhost\u0026#34;); StandardHost host = new StandardHost(); host.setName(\u0026#34;localhost\u0026#34;); String contextPath = \u0026#34;\u0026#34;; StandardContext context = new StandardContext(); context.setPath(contextPath); context.addLifecycleListener(new Tomcat.FixContextListener()); host.addChild(context); engine.addChild(host); service.setContainer(engine); service.addConnector(connector); // spring-mvc 将任意的请求交给 dispatcherServlet 进行处理 // dispatcherServlet 根据请求路径在 Bean 对象中找到并执行 Controller 对应的方法，返回结果 // 传入 WebApplicationContext ：spring 容器 tomcat.addServlet(contextPath, \u0026#34;dispatcher\u0026#34;, new DispatcherServlet(webApplicationContext)); context.addServletMappingDecoded(\u0026#34;/*\u0026#34;, \u0026#34;dispatcher\u0026#34;); try { tomcat.start(); } catch (LifecycleException e) { e.printStackTrace(); } } 2. 实现切换Servlet容器 # @Conditional 条件注解 # @Conditional(JettyCondition.class) // 条件注解 需要传入一个实现了 Condition 接口的类，有一个唯一的方法 matches\npublic class JettyCondition implements Condition { /** * 为 true 则条件满足 * @param context * @param metadata * @return */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) { // 判断有没有 Jetty 依赖 try { Class\u0026lt;?\u0026gt; aClass = context.getClassLoader().loadClass(\u0026#34;org.eclipse.jetty.server.Server\u0026#34;); return true; } catch (ClassNotFoundException e) { return false; } } } 实现 # 目标是可以在 maven 中排除 Tomcat 启用 Jetty\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;springboot\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;org.apache.tomcat.embed\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;tomcat-embed-core\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.eclipse.jetty\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jetty-server\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;9.4.48.v20220622\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; Jetty——给非默认的同类依赖加上 optional true\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.eclipse.jetty\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jetty-server\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;9.4.48.v20220622\u0026lt;/version\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; 写一个 WebServer 接口\npublic interface WebServer { public void start(WebApplicationContext webApplicationContext); } 实现 TomcatWebServer、JettyWebServer 类\npublic class TomcatWebServer implements WebServer{ @Override public void start(WebApplicationContext webApplicationContext) { System.out.println(\u0026#34;启动tomcat\u0026#34;); Tomcat tomcat = new Tomcat(); Server server = tomcat.getServer(); Service service = server.findService(\u0026#34;Tomcat\u0026#34;); Connector connector = new Connector(); connector.setPort(8081); StandardEngine engine = new StandardEngine(); engine.setDefaultHost(\u0026#34;localhost\u0026#34;); StandardHost host = new StandardHost(); host.setName(\u0026#34;localhost\u0026#34;); String contextPath = \u0026#34;\u0026#34;; StandardContext context = new StandardContext(); context.setPath(contextPath); context.addLifecycleListener(new Tomcat.FixContextListener()); host.addChild(context); engine.addChild(host); service.setContainer(engine); service.addConnector(connector); // spring-mvc 将任意的请求交给 dispatcherServlet 进行处理 // dispatcherServlet 根据请求路径在 Bean 对象中找到并执行 Controller 对应的方法，返回结果 // 传入 WebApplicationContext ：spring 容器 tomcat.addServlet(contextPath, \u0026#34;dispatcher\u0026#34;, new DispatcherServlet(webApplicationContext)); context.addServletMappingDecoded(\u0026#34;/*\u0026#34;, \u0026#34;dispatcher\u0026#34;); try { tomcat.start(); } catch (LifecycleException e) { e.printStackTrace(); } } } 装配\n手动装配【在用户的启动配置类内加上】 // 使用配置类的方式注入Bean @Bean public JettyWebServer jettyWebServer() { return new JettyWebServer(); } @Bean public TomcatWebServer tomcatWebServer() { return new TomcatWebServer(); } 实现 WebServerAutoConfiguration 自动配置类【手动实现麻烦、且不现实】 @Configuration public class WebServerAutoConfiguration { @Bean @Conditional(JettyCondition.class) // 条件注解 public JettyWebServer jettyWebServer() { return new JettyWebServer(); } @Bean @Conditional(TomcatCondition.class) public TomcatWebServer tomcatWebServer() { return new TomcatWebServer(); } } 此时启动仍然扫描不到这两个Bean，因为使用类不会扫描到 springboot 中的配置类。在用户配置类加上 @Import 注解即可：\n@Import(WebServerAutoConfiguration.class) // 不导入会导致无法扫描到这个类，导致无法注入 webServer Bean对象 spring自动配置\n@Import(AutoConfigurationImportSelector.class) // 在 EnableAutoConfiguration 注解【SpringBootApplication 注解上使用到的】上有这个注解 // AutoConfigurationImportSelector 核心方法： protected AutoConfigurationEntry getAutoConfigurationEntry(AnnotationMetadata annotationMetadata) { if (!isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } AnnotationAttributes attributes = getAttributes(annotationMetadata); // 查找候选配置类、去重、去除排除配置 List\u0026lt;String\u0026gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); configurations = removeDuplicates(configurations); Set\u0026lt;String\u0026gt; exclusions = getExclusions(annotationMetadata, attributes); checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); // 过滤掉未使用到的配置，使用到了spring-autoconfigure-metadata.properties 文件可以省略一次类加载 // 【对应配置类的类加载】，只需要判断关键类【如org.springframework.data.redis.core.RedisOperations】 // 是否存在，再决定是否过滤。另外开了一个线程处理上半部分。 configurations = getConfigurationClassFilter().filter(configurations); fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationEntry(configurations, exclusions); } 3. 动态代理 # 3.1 静态代理 # // 接口类 public interface Human { void eat(); void sleep(); void drinkMilk(); } // 目标类 public class Me implements Human{ @Override public void eat() { System.out.println(\u0026#34;吃饭\u0026#34;); } @Override public void sleep() { System.out.println(\u0026#34;睡觉\u0026#34;); } @Override public void drinkMilk() { System.out.println(\u0026#34;喝牛奶\u0026#34;); } } // 代理类 public class YuanTong implements Human{ private Me me; public YuanTong() { me = new Me(); } @Override public void eat() { me.eat(); } @Override public void sleep() { me.sleep(); } @Override public void drinkMilk() { this.lineUp(); me.drinkMilk(); } private void lineUp() { System.out.println(\u0026#34;送牛奶\u0026#34;); } } // 实际使用： public static void main(String[] args) { Human human = new YuanTong(); human.eat(); human.sleep(); human.drinkMilk(); } 3.2 动态代理 # Spring 提供JDK、CGLib两种方式生成代理对象。默认代理对象生成策略：若是目标类接口，则使用 JDK 动态代理，否则使用 CGLib 动态代理\n3.2.1 JDK 动态代理 # 实现 InvocationHandler 接口\nProxy.newProxyInstance 获取代理类 三个参数分别是 类加载器、委托类的接口类型、代理类 invok 方法三个参数：代理类实例 [例：com.tcmatj.proxy.Me@4141d797]、委托类的方法对象 [public abstract void com.tcmatj.proxy.Human.eat()]、委托类的方法参数 invoke 方法内为什么不使用第一个参数进行执行回调。在客户端使用getProxyInstance时，JDK 会返回一个 proxy 的实例，实例内有InvokecationHandler 对象及动态继承下来的目标 。客户端调用了目标方法，有如下操作：首先 JDK 先查找 proxy 实例内的 handler 对象 然后执行 handler 内的 invoke 方法。根据 public Object invoke 这个方法第一个参数 proxy 就是对应着 proxy 实例。如果在 invoke 内使用 method.invoke(proxy,args) ,会出现这样一条方法链，目标方法→invoke→目标方法invoke…，最终导致堆栈溢出。 public class YuanTongHandle implements InvocationHandler { private Object proxyTarget; public Object getProxyInstance(Object target) { this.proxyTarget = target; // 获取代理类 return Proxy.newProxyInstance(proxyTarget.getClass().getClassLoader(), proxyTarget.getClass().getInterfaces(), this); } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object invoke = null; if (\u0026#34;drinkMilk\u0026#34;.equals(method.getName())){ System.out.println(\u0026#34;跑腿\u0026#34;); invoke = method.invoke(proxyTarget, args); System.out.println(\u0026#34;end\u0026#34;); } else { invoke = method.invoke(proxyTarget, args); } return invoke; } } // 使用动态代理 Human human1 = (Human) new YuanTongHandle().getProxyInstance(new Me()); human1.eat(); human1.sleep(); human1.drinkMilk(); 3.2.2 CGLib 动代理 # Code Generation Library，高性能的代码生成库，可以在运行期间拓展类、实现接口。底层通过字节码处理框架ASM实现，通过转换字节码生成新的类\n区别：JDK 只能为接口创建代理实例，对于类，只能由CGLib创建动态代理实现 3.2.3 # 从SpringBoot从2.0开始，SpringBoot将Spring-aop默认设置成了Cglib动态代理实现。\n难道是出于性能考虑？：实际上在jdk1.8中，jdk的动态代理并不比cglib动态代理性能差，甚至有可能优于cglib性能。\n基于jdk的动态代理特殊情况下可能导致ClassCastException，使用jdk的动态代理可能无法通过原始的实现类来注入。\n@Autowired private ChatMsgService chatMsgService; @Autowired private ChatMsgServiceImpl chatMsgService; // 通过实现类注入，JDK会报错 // Caused by: org.springframework.beans.factory.BeanNotOfRequiredTypeException: Bean named \u0026#39;chatMsgServiceImpl\u0026#39; is expected to be of type \u0026#39;com.bruce.integration.service.impl.ChatMsgServiceImpl\u0026#39; but was actually of type \u0026#39;jdk.proxy2.$Proxy64\u0026#39; 原因其实也很简单：无论jdk还是cglib都是通过动态生成Class的方式来实现。但是jdk生成的class是实现了接口，和接口的原先实现类没有直接关系，而cglib生成的class是继承了整个实现类，可以强制转化给实现类。\n如果在SpringBoot中想让Spring针对接口使用jdk的动态代理可以使用如下配置：\nspring.aop.proxy-target-class=false 4. 事务 # @EnableTransactionManagement 应用于配置类上，表示启用Spring的事务管理功能。它会自动为使用@Transactional注解的方法创建代理，并在方法调用时执行事务管理\n@Transactional 将该注解应用于方法或类上，表示该方法或类需要进行事务管理。使用propagation属性来指定事务传播行为。\n方法：只能用在 public 方法\n类：表示对该类中所有的 public 方法都生效\n属性：\npropagation 事务传播行为 isolation 事务隔离级别 timeout 事务超时时间，默认 -1 readOnly 是否为只读事务，默认false rollbackFor 指定促发事务回滚的异常类型 避免同一个类中调用 @Transactional 注解的方法，这样会导致事务失效\u0026ndash;SpringAOP自调用问题\n被 @Transactional 注解的方法所在的类必须被 Spring 管理，否则不生效\n@Transactional 的工作机制是基于 AOP 实现的，AOP 又是使用动态代理实现的。如果目标对象实现了接口，默认情况下会采用 JDK 的动态代理，如果目标对象没有实现了接口，会使用 CGLIB 动态代理。createAopProxy() 方法 决定了是使用 JDK 还是 Cglib 来做动态代理。\n如果一个类或者一个类中的 public 方法上被标注@Transactional 注解的话，Spring 容器就会在启动的时候为其创建一个代理类，在调用被@Transactional 注解的 public 方法的时候，实际调用的是，TransactionInterceptor 类中的 invoke()方法。这个方法的作用就是在目标方法之前开启事务，方法执行过程中如果遇到异常的时候回滚事务，方法调用完成之后提交事务。\n4.1 事务传播行为（多个事务操作嵌套调用时的行为规则）、 # 枚举类 Propagation\nPropagation.REQUIRED（默认行为）：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新事务\nREQUIRES_NEW：无论当前是否存在事务，都会创建一个新的事务。如果外部存在事务，外部事务将挂起，并在内部事务完成后恢复；如果外部没有事务，则内部事务将独立运行\n事务A 调用事务 B，事务 A 回滚不会使事务 B 回滚。事务 B 回滚会导致事务 A 回滚 NESTED：如果当前存在事务，则在嵌套事务中运行；如果当前没有事务，则创建一个新事务\n事务A 调用事务 B，事务 B 回滚不会使事务 A 回滚。事务 A 回滚会导致事务 B 回滚 不常用：\nSUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务方式运行\nMANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常\nNOT_SUPPORTED：以非事务方式运行，并挂起当前存在的事务（如果有的话）\nNEVER：以非事务方式运行，如果当前存在事务，则抛出异常\n4.2 事务隔离级别 # 枚举类 Isolation\nIsolation.DEFAULT（默认级别） 使用后端数据库默认的隔离级别。MySQL的可重复读，Oracle的读已提交 READ_UNCOMMITTED 读未提交。脏读、幻读、不可重复读 READ_COMMITTED 读已提交。幻读、不可重复读 REPEATABLE_READ 可重复读。幻读 SERIALIZABLE 可串行化。 4.3 Spring事务管理 # 4.3.1 编程式事务管理 # 通过 TransactionTemplate 或 TransactionManager 手动管理事务\n@Autowired private TransactionTemplate transactionTemplate; public void testTransaction() { transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) { try { // .... 业务代码 } catch (Exception e){ //回滚 transactionStatus.setRollbackOnly(); } } }); } @Autowired private PlatformTransactionManager transactionManager; public void testTransaction() { TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition()); try { // .... 业务代码 transactionManager.commit(status); } catch (Exception e) { transactionManager.rollback(status); } } 4.3.2 声明式事务管理 # 通过AOP实现\n@Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something B b = new B(); C c = new C(); b.bMethod(); c.cMethod(); } 4.3.3 事务管理接口 # PlatformTransactionManager 会根据 TransactionDefinition 的定义比如事务超时时间、隔离级别、传播行为等来进行事务管理 ，而 TransactionStatus 接口则提供了一些方法来获取事务相应的状态比如是否新事务、是否可以回滚等等。\n4.3.3.1 事务管理器 PlatformTransactionManager # 事务策略核心。为各个平台都提供了对应的事务管理器\nJDBC：DataSourceTransactionManager\nHibernate：HibernateTransactionManager\nJPA：JpaTransactionManager\npackage org.springframework.transaction; import org.springframework.lang.Nullable; public interface PlatformTransactionManager { //获得事务 TransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException; //提交事务 void commit(TransactionStatus var1) throws TransactionException; //回滚事务 void rollback(TransactionStatus var1) throws TransactionException; } 4.3.3.2 事务定义信息 TransactionDefinition # 事务隔离级别、传播行为、超时、只读、回滚规则\npackage org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; int TIMEOUT_DEFAULT = -1; // 返回事务的传播行为，默认值为 REQUIRED。 int getPropagationBehavior(); //返回事务的隔离级别，默认值是 DEFAULT int getIsolationLevel(); // 返回事务的超时时间，单位秒，默认值为-1(取决于底层或没有超时时间)。如果超过该时间限制但事务还没有完成，则自动回滚事务。 int getTimeout(); // 返回是否为只读事务，默认值为 false boolean isReadOnly(); @Nullable String getName(); } 只读事务，可以指定事务类型为 readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。\n为什么数据查询操作还要启用事务支持呢？\nMySQL 默认对每一个新建立的连接都启用了autocommit模式。在该模式下，每一个发送到 MySQL 服务器的sql语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务。如果不加Transactional，每条sql会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。\n如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询 SQL 必须保证整体的读一致性，否则，在前条 SQL 查询之后，后条 SQL 查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持\n回滚规则：默认情况下，遇到 RuntimeExecption、Error 才会回滚。可以指定回滚的异常类型：\n@Transactional(rollbackFor= MyExecption.class) 4.3.3.3 事务运行状态 TransactionStatus # 记录事务的状态 该接口定义了一组方法,用来获取或判断事务的相应状态信息。\npublic interface TransactionStatus{ boolean isNewTransaction(); // 是否是新的事务 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成 } 5. SpringAOP自调用问题 # AOP：Aspect-oriented programming,面向切面编程，一种解决问题的思想，将一些重复性的编码问题通过切面来实现。\nSpring AOP：算是一种简单的AOP的落地实现方式，它主要提供在Spring容器内的一种AOP实现方式，脱离了Spring就不work了。Spring AOP并不是一套完整的AOP解决方案。SpringAOP默认使用代理模式实现的，也就是JDK Proxy/CGLib。\nAspectJ：是一套完整的AOP解决方案，在编译器织入切面到目标类\n自调用同类的方法【使用了使用AOP的注解】时，注解会失效，因为调用的不是代理对象，而是本身这个对象。\n方法一：注入代理bean到自己\n@Autowired @Lazy\t// 解决循环依赖的问题 private AsyncMethod asyncMethod; public void testAsync() { System.out.println(Thread.currentThread().getName()); // 调用注入的bean asyncMethod.testAsnc3(); } @Async public void testAsnc3() { System.out.println(Thread.currentThread().getName()); System.out.println(\u0026#34;async3\u0026#34;); } 方法二：AopContext.currentProxy() 获取当前代理对象\nAopContext 通过 ThreadLocal 来实现（不能跨线程）：\nprivate static final ThreadLocal\u0026lt;Object\u0026gt; currentProxy = new NamedThreadLocal\u0026lt;Object\u0026gt;(\u0026quot;Current AOP proxy\u0026quot;);\n首先需要配置@EnableAspectJAutoProxy(exposeProxy = true) // 暴露当前代理对象到当前线程绑定\npublic void testAsync() { System.out.println(Thread.currentThread().getName()); System.out.println(\u0026#34;async1\u0026#34;); ((AsyncMethod)AopContext.currentProxy()).testAsnc2(); } @Async public void testAsnc2() { System.out.println(Thread.currentThread().getName()); System.out.println(\u0026#34;async2\u0026#34;); } 方法三：直接使用AspectJ\n既然自调用的问题是由于SpringAOP由代理模式实现引起的，那就不使用代理模式不就解决了吗\n切换为代理模式\n@EnableAsync(mode = AdviceMode.ASPECTJ) 添加aspectj织入包依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectjweaver\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.8.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-instrument --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-instrument\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.5.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 使用\npublic void testAsync() { System.out.println(Thread.currentThread().getName()); System.out.println(\u0026#34;async1\u0026#34;); testAsnc2(); } /** * 测试ASPECTJ调用 */ @Async private void testAsnc2() { System.out.println(Thread.currentThread().getName()); System.out.println(\u0026#34;async2\u0026#34;); } 启动方式：AspectJ是编译器将切面织入到目标class的，启动的使用需要加上java agent的参数\n-Dserver.port=1000 -javaagent:${classpath}\\spring-instrument-4.2.5.RELEASE.jar -javaagent:${classpath}\\aspectjweaver-1.8.8.jar 6. @Lazy注解解决循环依赖 # 当两个或多个Bean之间存在循环依赖时，可以使用@Lazy注解延迟初始化其中一个Bean，从而打破循环依赖。\n具体原理：\n当Spring容器启动时，会解析并创建所有的Bean定义，但不会立即初始化Bean实例。 当遇到循环依赖时，Spring会创建一个代理对象来代替其中一个Bean的实例。 通过@Lazy注解，可以指定某个Bean在第一次被使用时才进行初始化，而不是在容器启动时立即初始化。 当另一个Bean依赖于被@Lazy注解修饰的Bean时，Spring会返回一个代理对象，而不是实际的Bean实例。 当代理对象被调用时，Spring会检查是否需要初始化被@Lazy注解修饰的Bean。 如果需要初始化，Spring会解析并创建被@Lazy注解修饰的Bean，并将其注入到代理对象中。 这样，循环依赖问题得到了解决，每个Bean都能够正确地获取到对方的实例。 需要注意的是，@Lazy注解只能解决单例Bean之间的循环依赖问题，因为单例Bean在容器启动时就会被创建。对于原型（prototype）作用域的Bean，@Lazy注解不起作用，因为原型Bean每次获取时都会创建一个新的实例\n7. IOC、AOP # IoC（Inverse of Control:控制反转）是一种设计思想或者说是某种模式。这个设计思想就是 将原本在程序中手动创建对象的控制权交给第三方比如 IoC 容器。 对于 Spring 框架来说， IoC 容器实际上就是个 Map（key，value）,Map 中存放的是各种对象。IoC 最常见以及最合理的实现方式叫做依赖注入（Dependency Injection，简称 DI）\nAOP（Aspect Oriented Programming）即面向切面编程，AOP 是 OOP（面向对象编程）的一种延续，二者互补，并不对立。\nAOP 的目的是将横切关注点（如日志记录、事务管理、权限控制、接口限流、接口幂等等）从核心业务逻辑中分离出来，通过动态代理、字节码操作等技术，实现代码的复用和解耦，提高代码的可维护性和可扩展性。OOP 的目的是将业务逻辑按照对象的属性和行为进行封装，通过类、对象、继承、多态等概念，实现代码的模块化和层次化（也能实现代码的复用），提高代码的可读性和可维护性\n横切关注点（cross-cutting concerns） ：多个类或对象中的公共行为（如日志记录、事务管理、权限控制、接口限流、接口幂等等）。\n切面（Aspect）：对横切关注点进行封装的类，一个切面是一个类。切面可以定义多个通知，用来实现具体的功能。\n连接点（JoinPoint）：连接点是方法调用或者方法执行时的某个特定时刻（如方法调用、异常抛出等）。\n通知（Advice）：通知就是切面在某个连接点要执行的操作。通知有五种类型，分别是前置通知（Before）、后置通知（After）、返回通知（AfterReturning）、异常通知（AfterThrowing）和环绕通知（Around）。前四种通知都是在目标方法的前后执行，而环绕通知可以控制目标方法的执行过程。\n切点（Pointcut）：一个切点是一个表达式，它用来匹配哪些连接点需要被切面所增强。切点可以通过注解、正则表达式、逻辑运算等方式来定义。比如 execution(* com.xyz.service..*(..))匹配 com.xyz.service 包及其子包下的类或接口。\n织入（Weaving）：织入是将切面和目标对象连接起来的过程，也就是将通知应用到切点匹配的连接点上。常见的织入时机有两种，分别是编译期织入（AspectJ）和运行期织入（AspectJ）。\nAOP应用场景：\n日志记录：自定义日志记录注解，利用 AOP，一行代码即可实现日志记录。\n性能统计：利用 AOP 在目标方法的执行前后统计方法的执行时间，方便优化和分析。\n事务管理：@Transactional 注解可以让 Spring 为我们进行事务管理比如回滚异常操作，免去了重复的事务管理逻辑。@Transactional注解就是基于 AOP 实现的。\n权限控制：利用 AOP 在目标方法执行前判断用户是否具备所需要的权限，如果具备，就执行目标方法，否则就不执行。例如，SpringSecurity 利用@PreAuthorize 注解一行代码即可自定义权限校验。\n接口限流：利用 AOP 在目标方法执行前通过具体的限流算法和实现对请求进行限流处理。\n缓存管理：利用 AOP 在目标方法执行前后进行缓存的读取和更新\n8. @Resource 注入静态变量导致空指针异常 # @Resource private static LeadsStatisticBaseServiceImpl leadsStatisticBaseService; public static List\u0026lt;Long\u0026gt; getAllAuthCategoryId(BaseRequest request, Long categoryId) { List\u0026lt;Long\u0026gt; allAuthCategoryId; LeadsStatisticAssembler assembler = new LeadsStatisticAssembler(); if (categoryId == null) { // 空表示差出所以有权限的类目id allAuthCategoryId = leadsStatisticBaseService.getAllAuthCategoryId(request); } else { // 否则表示查出所给的类目 allAuthCategoryId = new ArrayList\u0026lt;\u0026gt;(); allAuthCategoryId.add(categoryId); } return allAuthCategoryId; } 静态变量、类变量，并不是对象的属性，而是一个类的属性；所以静态方法是属于整个类（class）的，普通方法才是属于实体对象（也就是New出来的对象）的，spring注入是在容器中实例化对象，所以不能使用静态方法。\n而使用静态变量、类变量扩大了静态方法的使用范围。在静态方法中注入依赖在spring框架中是不推荐使用的，依赖注入的主要目的,是让容器去产生一个对象的实例,然后在整个生命周期中使用他们，同时也让测试工作更加容易。\n一旦你使用静态方法,就不再需要去产生这个类的实例,这会让测试变得更加困难，同时你也不能为一个给定的类，依靠注入方式去产生多个具有不同的依赖环境的实例，这种static field是隐含共享的，并且是一种global全局状态，spring同样不推荐这样去做。\n将@Autowire加到构造方法上 @Autowired public StatisticUtil(LeadsStatisticBaseServiceImpl leadsStatisticBaseService) { StatisticUtil.leadsStatisticBaseService = leadsStatisticBaseService; } 使用set注入的方式 @Autowired public void setLeadsStatisticBaseService(LeadsStatisticBaseServiceImpl leadsStatisticBaseService) { StatisticUtil.leadsStatisticBaseService = leadsStatisticBaseService; } 用@PostConstruct注解 9. SPI机制 # SPI（Service Provider Interface），是 JDK 内置的一种 服务提供发现机制，可以用来启用框架扩展和替换组件，主要是被框架的开发人员使用，比如 java.sql.Driver 接口，其他不同厂商可以针对同一接口做出不同的实现，MySQL 和 PostgreSQL 都有不同的实现提供给用户，而 Java 的 SPI 机制可以为某个接口寻找服务实现。Java 中 SPI 机制主要思想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是 解耦\n当服务的提供者提供了一种接口的实现之后，需要在 classpath 下的 META-INF/services/ 目录里创建一个以服务接口命名的文件，这个文件里的内容就是这个接口的具体的实现类。当其他的程序需要这个服务的时候，就可以通过查找这个 jar 包（一般都是以 jar 包做依赖）的 META-INF/services/ 中的配置文件，配置文件中有接口的具体实现类名，可以根据这个类名进行加载实例化，就可以使用该服务了。JDK 中查找服务的实现的工具类是：java.util.ServiceLoader\n先定义好接口\npublic interface Search { public List\u0026lt;String\u0026gt; searchDoc(String keyword); } 文件搜索实现\npublic class FileSearch implements Search{ @Override public List\u0026lt;String\u0026gt; searchDoc(String keyword) { System.out.println(\u0026#34;文件搜索 \u0026#34;+keyword); return null; } } 数据库搜索实现\npublic class DatabaseSearch implements Search{ @Override public List\u0026lt;String\u0026gt; searchDoc(String keyword) { System.out.println(\u0026#34;数据搜索 \u0026#34;+keyword); return null; } } 接下来可以在resources下新建 META-INF/services/ 目录，然后新建接口全限定名的文件：com.cainiao.ys.spi.learn.Search，里面加上我们需要用到的实现类com.cainiao.ys.spi.learn.FileSearch 测试方法\npublic class TestCase { public static void main(String[] args) { ServiceLoader\u0026lt;Search\u0026gt; s = ServiceLoader.load(Search.class); Iterator\u0026lt;Search\u0026gt; iterator = s.iterator(); while (iterator.hasNext()) { Search search = iterator.next(); search.searchDoc(\u0026#34;hello world\u0026#34;); } } } 可以看到输出结果：文件搜索 hello world 如果在 com.cainiao.ys.spi.learn.Search 文件里写上两个实现类，那最后的输出结果就是两行了。这就是因为 ServiceLoader.load(Search.class) 在加载某接口时，会去 META-INF/services 下找接口的全限定名文件，再根据里面的内容加载相应的实现类。\n10. 事件机制 # ApplicationContext 事件机制是观察者设计模式的实现，通过 ApplicationEvent 类和 ApplicationListener 接口，可以实现 ApplicationContext 事件处理。如果容器中有一个 ApplicationListener Bean，每当 ApplicationContext 发布 ApplicationEvent 时，ApplicationListener Bean 将自动被触发。\n下面两个文件放到 DDD 的 infrastructure 层\nApplicationEvent：容器事件，必须由ApplicationContext发布 ApplicationListener：监听器，可由容器中的任何监听器Bean担任 自定义的事件类 CustomEvent 继承自 ApplicationEvent：\nimport org.springframework.context.ApplicationEvent; public class CustomEvent extends ApplicationEvent { private String message; public CustomEvent(Object source, String message) { // source 代表事件的来源或触发者 super(source); // 完成对父类Object的构造方法的调用。确保事件对象在创建时完成必要的初始化工作 this.message = message; } public String getMessage() { return message; } } 事件发布者 CustomEventPublisher，用于发布事件：\nimport org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.ApplicationEventPublisher; import org.springframework.stereotype.Component; @Component public class CustomEventPublisher { @Autowired private ApplicationEventPublisher applicationEventPublisher; public void publish(String message) { CustomEvent customEvent = new CustomEvent(this, message); applicationEventPublisher.publishEvent(customEvent); } } 事件监听器 CustomEventListener，用于监听并处理事件：\n// 注解方式 @Component public class CustomEventListener { @EventListener public void handleCustomEvent(CustomEvent event) { // 监听到事件后的处理逻辑 System.out.println(\u0026#34;Received custom event - \u0026#34; + event.getMessage()); } } // 实现接口方式 public class CustomEventListener implements ApplicationListener\u0026lt;CustomEvent\u0026gt; { @Override public void onApplicationEvent(CustomEvent event) { // 监听到事件后的处理逻辑 System.out.println(\u0026#34;Received custom event - \u0026#34; + event.getMessage()); } } 使用：\n方法一：\n得物：impl 调用：AppContextUtil.getApplicationContext().publishEvent(examinePassEvent);\n实际上是调 ApplicationContext.publishEvent()方法，需要线有一个上下文容器，再调 publishEvent\n方法二：\n使用监听器调 publish 方法\n@Service public class MyService { @Autowired private CustomEventPublisher customEventPublisher; public void doSomethingAndPublishEvent() { // 执行一些操作 // 操作完成后发布事件 customEventPublisher.publish(\u0026#34;Something happened\u0026#34;); } } ","externalUrl":null,"permalink":"/docs/java/springboot/","section":"Docs","summary":"Spring Boot # 简化 Spring 框架开发 [toc] 1. 创建项目 # 勾选 Web --\u0026gt; Spring Web 2. 注解 # @SpringBootApplication 标注","title":"Spring Boot","type":"docs"},{"content":" 1. Stream 流式编程 # Stream 对象是一种一次性使用的对象，它只能被消费一次。一旦对 Stream 执行了终止操作（如收集结果、遍历元素），Stream 就会被关闭，后续无法再使用\n1.1 创建流 # 从集合创建\nStream\u0026lt;Integer\u0026gt; stream = listname.stream(); 从数组创建\nStream\u0026lt;Integer\u0026gt; stream = Arrays.stream(intsName); Stream.of() 创建\nStream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 3, 4); Stream.builder() 创建\nStream.Builder\u0026lt;Integer\u0026gt; builder = Stream.builder(); builder.add(1); ... Stream\u0026lt;Integer\u0026gt; stream = builder.build(); 从I/O资源创建\n生成器创建\nStream\u0026lt;Integer\u0026gt; stream = Stream.generate(() -\u0026gt; 0); // 创建一个无限流，每个元素都是 0 Stream\u0026lt;Integer\u0026gt; stream = Stream.iterate(0, n -\u0026gt; n + 1); // 创建一个无限流，从 0 开始递增 1.2 常用操作 # 过滤 filter() # filter() 方法接受一个 Predicate 函数作为参数，用于过滤 Stream 中的元素。只有满足 Predicate 条件的元素会被保留下来\nStream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 3, 4, 5); Stream\u0026lt;Integer\u0026gt; filteredStream = stream.filter(n -\u0026gt; n % 2 == 0); // 过滤出偶数 映射 map() # map() 方法接受一个 Function 函数作为参数，用于对 Stream 中的元素进行映射转换。对每个元素应用函数后的结果会构成一个新的 Stream\nStream\u0026lt;String\u0026gt; stream = Stream.of(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;); Stream\u0026lt;Integer\u0026gt; mappedStream = stream.map(s -\u0026gt; s.length()); // 映射为单词长度 扁平映射 flatMap() # 类似于 map() 方法，不同之处在于它可以将每个元素映射为一个流，并将所有流连接成一个流。主要用于解决嵌套集合的情况。\nList\u0026lt;List\u0026lt;Integer\u0026gt;\u0026gt; nestedList = Arrays.asList( Arrays.asList(1, 2), Arrays.asList(3, 4), Arrays.asList(5, 6) ); Stream\u0026lt;Integer\u0026gt; flattenedStream = nestedList.stream().flatMap(List::stream); // 扁平化为一个流 截断 limit(n) # limit() 方法可以限制 Stream 的大小，只保留前 n 个元素\nStream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 3, 4, 5); Stream\u0026lt;Integer\u0026gt; limitedStream = stream.limit(3); // 只保留前 3 个元素 跳过 skip(n) # skip() 方法可以跳过前 n 个元素，返回剩下的元素组成的新 Stream\ntream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 3, 4, 5); Stream\u0026lt;Integer\u0026gt; skippedStream = stream.skip(2); // 跳过前 2 个元素 排序 sorted() # sorted() 方法用于对 Stream 中的元素进行排序，默认是自然顺序排序。还可以提供自定义的 Comparator 参数来指定排序规则。\nStream\u0026lt;Integer\u0026gt; stream = Stream.of(5, 2, 4, 1, 3); Stream\u0026lt;Integer\u0026gt; sortedStream = stream.sorted(); // 自然顺序排序 去重 distinct() # 根据元素的 equals() 和 hashCode() 方法来判断是否重复\nStream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 2, 3, 3, 3); Stream\u0026lt;Integer\u0026gt; distinctStream = stream.distinct(); // 去重 遍历 forEach() # 接受一个Consumer函数作为参数，对流中的每个元素执行该函数。它没有返回值，因此无法将操作结果传递给后续操作。\nList\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;); names.stream().forEach(System.out::println); peek() # 接受一个Consumer函数作为参数，对流中的每个元素执行该函数。与forEach不同的是，peek方法会返回一个新的流，该流中的元素和原始流中的元素相同。\nList\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;); List\u0026lt;String\u0026gt; upperCaseNames = names.stream() .map(String::toUpperCase) .peek(System.out::println) .collect(Collectors.toList()); 汇总 collect() # 用于将 Stream 中的元素收集到结果容器中，如 List、Set、Map 等。可以使用预定义的 Collectors 类提供的工厂方法来创建收集器，也可以自定义收集器。\nStream\u0026lt;String\u0026gt; stream = Stream.of(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;); List\u0026lt;String\u0026gt; collectedList = stream.collect(Collectors.toList()); // 收集为 List List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;); String joinedNames = names.stream() .collect(Collectors.joining(\u0026#34;, \u0026#34;)); System.out.println(joinedNames); // 输出结果: Alice, Bob, Charlie 归约 reduce() # reduce() 方法用于将 Stream 中的元素依次进行二元操作，得到一个最终的结果。它接受一个初始值和一个 BinaryOperator 函数作为参数。\nStream\u0026lt;Integer\u0026gt; stream = Stream.of(1, 2, 3, 4, 5); Optional\u0026lt;Integer\u0026gt; sum = stream.reduce((a, b) -\u0026gt; a + b); // 将元素依次相加。对所有元素求和 统计 summaryStatistics() # summaryStatistics() 方法可以从 Stream 中获取一些常用的统计信息，如元素个数、最小值、最大值、总和和平均值。\nIntStream stream = IntStream.of(1, 2, 3, 4, 5); IntSummaryStatistics stats = stream.summaryStatistics(); System.out.println(\u0026#34;Count: \u0026#34; + stats.getCount()); System.out.println(\u0026#34;Min: \u0026#34; + stats.getMin()); System.out.println(\u0026#34;Max: \u0026#34; + stats.getMax()); System.out.println(\u0026#34;Sum: \u0026#34; + stats.getSum()); System.out.println(\u0026#34;Average: \u0026#34; + stats.getAverage()); 匹配 allMatch() 、anyMatch() 、noneMatch() # List\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1, 2, 3, 4, 5); boolean allEven = numbers.stream().allMatch(n -\u0026gt; n % 2 == 0); // false boolean hasEven = numbers.stream().anyMatch(n -\u0026gt; n % 2 == 0); // True boolean noneNegative = numbers.stream().noneMatch(n -\u0026gt; n \u0026lt; 0); // True 查找 findFirst() 、findAny() # List\u0026lt;String\u0026gt; names = Arrays.asList(\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;); Optional\u0026lt;String\u0026gt; first = names.stream().findFirst(); first.ifPresent(System.out::println); // 输出结果: Alice // ifPresent 判断是否包含元素 // findAny 方法用于返回流中的任意一个符合条件的元素 List\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1, 2, 3, 4, 5); Optional\u0026lt;Integer\u0026gt; any = numbers.stream() .filter(n -\u0026gt; n % 2 == 0) .findAny(); any.ifPresent(System.out::println); // 输出结果: 2 或 4（取决于并行处理的结果） 统计 count() 、 max() 、 min() # count 方法用于返回流中元素的数量。它返回一个 long 类型的值\nmax 方法用于返回流中的最大值。它返回一个 Optional 对象\nmin 方法用于返回流中的最小值。它返回一个 Optional 对象\nList\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1, 2, 3, 4, 5); long count = numbers.stream().count(); System.out.println(count); // 输出结果: 5 Optional\u0026lt;Integer\u0026gt; max = numbers.stream().max(Integer::compareTo); max.ifPresent(System.out::println); // 输出结果: 5 Optional\u0026lt;Integer\u0026gt; min = numbers.stream().min(Integer::compareTo); min.ifPresent(System.out::println); // 输出结果: 1 1.2 并行流 parallel() 、 parallelStream() # 在传统的顺序流中，所有的操作都是在单个线程上按照顺序执行的。而并行流则会将流的元素分成多个小块，并在多个线程上并行处理这些小块，最后将结果合并起来。这样可以充分利用多核处理器的优势，加快数据处理的速度。\n创建并行流 # List\u0026lt;Integer\u0026gt; numbers = Arrays.asList(1, 2, 3, 4, 5); numbers.stream().parallel().forEach(System.out::println); Stream\u0026lt;Integer\u0026gt; parallelStream = numbers.parallelStream(); numbers.parallelStream() .map(n -\u0026gt; compute(n)) // 在多个线程上并行处理计算 .forEach(System.out::println); 1.3 Optional 类 # 1.3.1 创建 # String name = \u0026#34;John\u0026#34;; Optional\u0026lt;String\u0026gt; optionalName = Optional.of(name); // 创建包含非null值的Optional对象 Optional\u0026lt;String\u0026gt; optionalMaybeNull = Optional.ofNullable(maybeNullValue); // 创建可能为null的Optional对象 1.3.2 获取对象的值 # String value = optionalName.orElse(\u0026#34;Default\u0026#34;); // 如果optionalName为空，则返回默认值 String value = optionalName.orElseGet(() -\u0026gt; \u0026#34;Default\u0026#34;); // 如果optionalName为空，则通过lambda表达式返回默认值 1.3.3 map # 对 Optional 对象中的值进行转换，如果 Optional 对象为空，则 map 方法不进行任何操作，直接返回一个空的 Optional 对象，不需要担心空指针异常。\nOptional\u0026lt;String\u0026gt; upperCaseName = optionalName.map(String::toUpperCase); 1.3.4 用法 # // comp.getResult().getChampion().getName() 此类链式调用 Optional.ofNullable(comp) .map(Competition::getResult) .map(CompResult::getChampion) .map(User::getName) .orElseThrow(()-\u0026gt;new IllegalArgumentException(\u0026#34;The value of param comp isn\u0026#39;t available.\u0026#34;)); 2. IO 流 # 输入流：读入内存\n输出流：从内存写出\n2.1 抽象类 # 字节流：InputStream 字节输入流 OutputStream 字节输出流\n字符流：Reader 字符输入流 Writer 字符输出流\n所有的流都实现了Closeable 接口，都是可关闭的，流是一个管道，是内存和硬盘之间的通道，用完后要关闭，不然会占用资源\n所有的流都实现了Flushable 接口，都是可刷新的，刷新表示将未输出的数据强行输出完毕。\n2.2 常用流 # 2.2.1 文件流 # FileInputStream # 可以传入文件路径或File文件类构造\nint read() 读取一个字节，返回ASCII码；文件末尾返回 -1 int read(byte[] b) 读取b数组长度字节到b中，返回读取的字节个数；文件末尾返回 -1 int read(byte[] b, int off, int len) int available() 返回文件有效的字节数 long skip(long n) 跳过 n 字节 void close() 关闭流 FileOutputStream # 构造函数：\n传入文件名称 文件名称 + boolean append true表示在文件尾部追加 传入文件类 文件类 + true 追加 方法：\nvoid write(int b) 指定字节写入文件 void write(byte[] b) 将 b.length 个字节写入文件 void write(byte[] b, int off, int len) void flush() 刷新输出流，强制写出所有缓冲的输出字节 void close() 关闭 FileReader # 只能读取普通文本，可以传入文件名或文件类\nint read() 读取一个字符，返回ASCII码；文件末尾返回 -1 int read(char[] c) 读取c数组长度字符到c中，返回读取的字符个数；文件末尾返回 -1 int read(char[] c, int off, int len) long skip(long n) 跳过 n 字符 void close() 关闭流 FileWriter # 与上类似，多了个是否追加\nvoid write(int b) 指定字符写入文件 void write(char[] c) 将 c.length 个字节写入文件 void write(char[] c, int off, int len) void write(String s) 将字符串写入文件 void flush() 刷新输出流，强制写出所有缓冲的输出字节 void close() 关闭 2.2.2 缓冲流 # 构造需要传入 Reader 对象或 Write 对象\nBufferedInputStream\nBufferedOutputStream\nBufferedReader\nString readLine() 读取文件一行，末尾 null BufferedWriter\n2.2.3 转换流（字节流转字符流）返回 Reader 对象、Writer对象 # InputStreamReader\nOutputStreamWriter\n2.2.4 标准输出流 # PrintWriter\nPrintStream 默认输出到控制台\n传入File 或 OutputStream 或 文件地址\nprintln(x[类型不定]) 输出 x 换行 print(x) 输出 x void flush() void close() 使用System.setOut(PrintStream p) 可以改变标准输出流（sout）的输出方向 2.2.5 对象流 - 序列化/反序列化 # ObjectInputStream 反序列化-传入InputStream 对象\nreadObject() ObjectOutputStream 序列化对象-传入OutputStream对象\nwriteObjects(s) 序列化对象 s (已经实现了Serializable 接口) flush() close() 2.2.6 数据流 # DataInputStream 数据字节输入流\n传入 InuptStream 对象\nboolean readBoolean() 从文件读取 boolean 字节数据 readByte() / readChar() / readDouble() / readFloat() / readInt() / readLong() / readShort() DataOutputStream 数据字节输出流。写的文件只能使用 DataInputStream 去读，读的时候需要直到写入的顺序\n传入 OutputStream 对象\nvoid writeBoolean(boolean v) 将 v 写入文件 void flush() 2.2.7 文件类 File # boolean delete() 删除文件/文件夹 boolean exists() 判断文件/文件夹是否存在 File getAbsoluteFile() 获取绝对路径 String getName() String getParent() 获取父文件（夹） File getParentFile() String getPath() 获取路径 boolean isDirectory() isFile() isHidden() long lastModified() long length() 文件大小；文件夹的文件个数 String[] list() 文件夹的名称列表 File[] listFiles() 文件夹的文件列表 boolean mkdir() 创建文件/文件夹 boolean mkdirs() 创建多重文件夹 ","externalUrl":null,"permalink":"/docs/java/stream%E6%B5%81%E4%B8%8Eio%E6%B5%81/","section":"Docs","summary":"1. Stream 流式编程 # Stream 对象是一种一次性使用的对象，它只能被消费一次","title":"Stream、IO流","type":"docs"},{"content":"Blowfish 支持基于 Hugo 的所有分类方法。同时，当前的标签预览页也支持展示自定义内容。\n在这里可以为每个分类添加额外的描述信息。查看下面的高级标签页面，了解更多。\n","externalUrl":null,"permalink":"/tags/","section":"标签","summary":"Blowfish 支持基于 Hugo 的所有分类方法。同时，当前的标签预览页也支持展示","title":"标签","type":"tags"},{"content":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页添加自定义内容，这部分内容会显示在顶部。\u0026#x1f680;\n你也可以用这些内容来定义 Hugo 的元数据，比如标题和描述。这些内容可以被用来增强 SEO 或其他目的。\n","externalUrl":null,"permalink":"/tags/advanced/","section":"标签","summary":"这是高级标记。类似其他 Blowfish 中的其他列表页面，你可以在分类列表页","title":"高级","type":"tags"},{"content":"","externalUrl":null,"permalink":"/series/%E5%B7%A5%E5%85%B7/","section":"Series","summary":"","title":"工具","type":"series"},{"content":" tcmatj@gmail.com 2216685752@qq.com\n","externalUrl":null,"permalink":"/","section":"欢迎来到 Blowfish! 🎉","summary":"tcmatj@gmail.com 2216685752@qq.com","title":"欢迎来到 Blowfish! 🎉","type":"page"},{"content":" 1、数据处理 # 1、缺失值 # is_NaN = iris_df_NaN.isna() # 判断 Pandas 数据帧是否为缺失值，是便用 True 占位，否便用 False 占 not_NaN = iris_df_NaN.notna() 删除缺失行/列 # df.dropna(axis = 0, how = \u0026#39;any\u0026#39;) 中 axis = 0 为按行删除，设置 axis = 1 表示按列删除。how = \u0026lsquo;any\u0026rsquo;时，表示某行或列只要有一个缺失值，就删除该行或列；当 how =\u0026lsquo;all\u0026rsquo;，表示该行或列全部都为缺失值时，才删除该行或列\n插值 # from sklearn.impute import SimpleImputer si = SimpleImputer(strategy=\u0026#39;median\u0026#39;)\t# 中位数插补缺失值 # impute training data X_NaN_median = si.fit_transform(X_NaN) 插补方法：\u0026lsquo;mean\u0026rsquo;、\u0026lsquo;median\u0026rsquo;、\u0026lsquo;most_frequent\u0026rsquo;、\u0026lsquo;constant\u0026rsquo;\nKNN 插值 # from sklearn.impute import KNNImputer knni = KNNImputer(n_neighbors=5) X_NaN_kNN = knni.fit_transform(X_NaN) 多变量插值 # from sklearn.impute import IterativeImputer from sklearn.ensemble import RandomForestRegressor rf_imp = IterativeImputer(estimator=RandomForestRegressor(random_state=0), max_iter=20) X_NaN_RF = rf_imp.fit_transform(X_NaN) 2、离群值 # 百分位 # q1, q50, q99 = np.percentile(X_df[feature_names[num]], [1,50,99]) 直方图百分位 # feature_names = [\u0026#39;Sepal length, $X_1$\u0026#39;,\u0026#39;Sepal width, $X_2$\u0026#39;, \u0026#39;Petal length, $X_3$\u0026#39;,\u0026#39;Petal width, $X_4$\u0026#39;] num = 0 fig, axes = plt.subplots(2,2) for i in [0,1]: for j in [0,1]: sns.histplot(data=X_df, x = feature_names[num], binwidth = 0.2, ax = axes[i][j]) axes[i][j].set_xlim([0,8]); axes[0][0].set_ylim([0,40]) q1, q50, q99 = np.percentile(X_df[feature_names[num]], [1,50,99])# 75,50,25可用于箱型图 axes[i][j].axvline(x=q1, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=q50, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=q99, color = \u0026#39;r\u0026#39;) num = num + 1 KDE+RUG百分位 # feature_names = [\u0026#39;Sepal length, $X_1$\u0026#39;,\u0026#39;Sepal width, $X_2$\u0026#39;, \u0026#39;Petal length, $X_3$\u0026#39;,\u0026#39;Petal width, $X_4$\u0026#39;] num = 0 fig, axes = plt.subplots(2,2) for i in [0,1]: for j in [0,1]: sns.kdeplot(data=X_df, x = feature_names[num], ax = axes[i][j], fill = True) sns.rugplot(data=X_df, x = feature_names[num], ax = axes[i][j], color = \u0026#39;k\u0026#39;, height=.05)\t# height=.05小短条的高度 q1, q50, q99 = np.percentile(X_df[feature_names[num]], [1,50,99]) axes[i][j].axvline(x=q1, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=q50, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=q99, color = \u0026#39;r\u0026#39;) num = num + 1 QQ图 # from scipy import stats import pylabimport pylab stats.probplot(values, dist=\u0026#34;norm\u0026#34;, plot=pylab) 箱型图、蜂群图 # fig, ax = plt.subplots() sns.boxplot(data=X_df, palette=\u0026#34;Set3\u0026#34;, orient=\u0026#34;h\u0026#34;)# orient=\u0026#34;h\u0026#34; 水平显示 print(X_df.describe())# 打印描述统计信息（计数、均值、标准差、最小值、25% 分位数、50% 分位数、75% 分位数和最大值） sns.swarmplot(data=X_df, linewidth=0.25, orient=\u0026#34;h\u0026#34;, color=\u0026#34;.5\u0026#34;) z 分数 # 若任何数据点与均值的偏差绝对值大于三倍标准差，则可以判定数据点为离群点。\n也就是任何数据点的 Z 分数绝对值大于 3，即 z 分数大于 3 或小于−3，可以判定数据点为离群点\ndf_zscore = (X_df - X_df.mean())/X_df.std() # 直方图可视化 num = 0 fig, axes = plt.subplots(2,2) for i in [0,1]: for j in [0,1]: sns.histplot(data=df_zscore, x = feature_names[num], binwidth = 0.2, ax = axes[i][j]) axes[i][j].set_xlim([-4,4]); axes[i][j].set_ylim([0,40]) axes[i][j].axvline(x=3, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=2, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=-3, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=-2, color = \u0026#39;r\u0026#39;) axes[i][j].axvline(x=0, color = \u0026#39;r\u0026#39;) num = num + 1 3、数据转换 # 中心化\u0026ndash;去均值 # X_demean = X_df.sub(X_df.mean()) 标准化\u0026ndash;z分数 # Z_score = (X_df - X_df.mean()) /X_df.std() 归一化 # X_normalized = (X_df - X_df.min()) /(X_df.max() - X_df.min()) 其他 # 数据先去均值，然后再除以 X_max - X_min 数据先去均值，然后再除以 箱型图的四分位间距 IQR = Q_3 - Q_1 广义幂转换 （Box-Cox）、Yeo-Johnson 转换 # 对非正态分布数据，通过一系列参数 λ 的取值，将数据的概率密度函数进行幂函数变换，使得变换后的数据更加接近正态分布\nYeo-Johnson 转换 可以处理负数值\n# 原始数据：指数分布 original_X = np.random.exponential(size=1000) # Box-Cox 变换 new_X, fitted_lambda = stats.boxcox(original_X) # Yeo-Johnson 转换 new_X, fitted_lambda = stats.yeojohnson(original_X) 经验累积分布函数 # 不需要对数据进行任何假设或参数估计，适用于任何类型的数据分布，包括连续型和离散型数据。通过将原始数据转换为概率分布函数，可以更好地理解数据的 分布情况，并与理论分布进行比较，从而判断数据是否符合某种分布模型。\n经过 ECDF 转换，特征的样本数据都变成了 [0, 1] 区间的数据\n# 直方图 fig, ax = plt.subplots() sns.histplot(data=X_df, palette = \u0026#34;viridis\u0026#34;,fill = True, binwidth = 0.15,\t# 直方图的箱宽 element=\u0026#34;step\u0026#34;,\t# 绘制直方图的线条 stat=\u0026#34;density\u0026#34;,\t# 绘制密度估计 cumulative=False, # 不绘制累积分布函数 common_norm=False)\t# 每个直方图使用自己的归一化 # 累积分布函数 fig, ax = plt.subplots() sns.histplot(data=X_df, palette = \u0026#34;viridis\u0026#34;,fill = False, binwidth = 0.15,element=\u0026#34;step\u0026#34;,stat=\u0026#34;density\u0026#34;, cumulative=True, common_norm=False) # 经验累积分布函数图 fig, ax = plt.subplots() sns.ecdfplot(data=X_df, palette = \u0026#34;viridis\u0026#34;) 4、插值 # 一维插值、二维插值 # from scipy.interpolate import interp1d,interp2d # 拟合函数 f_prev = interp1d(x_known, y_known, kind = \u0026#39;linear\u0026#39;) # 对x预测 f_prev(x) f_interp = interp2d(xx1_data, xx2_data, yy_data, kind=\u0026#39;linear\u0026#39;)\t# \u0026#39;linear\u0026#39;, \u0026#39;cubic\u0026#39; kind ： [\u0026lsquo;previous\u0026rsquo;, \u0026rsquo;next\u0026rsquo;, \u0026rsquo;nearest\u0026rsquo;, \u0026rsquo;linear\u0026rsquo;, \u0026lsquo;cubic\u0026rsquo;]\n​\t向前常数插值、向后常数插值、最邻近常数插值、线性插值、三次样条插值\n拉格朗日插值 # from scipy.interpolate import lagrange from numpy.polynomial.polynomial import Polynomial poly = lagrange(x_known, y_known)\t# 计算拉格朗日插值多项式 print(Polynomial(poly).coef)\t# 多项式的系数 # 对x预测 poly(x) 随着已知点数量 n 不断增大，拉格朗日插值函数多项式函数次数不断提高，插值多项式的插值逼近效果未必好。插值多项式 (红色曲线) 区间边缘处出现振荡问题，这一现象叫做龙格现象\n二维插值 # 当数据并不是规整的网格数据，而是不规则的散点时可以用 scipy.interpolate.griddata() 完成二维插值\nX_scatter = np.random.uniform(-1,1,(25,2)) yy_scatter = surface(X_scatter[:,0],X_scatter[:,1]) x1_grid = np.linspace(-1, 1, 100) x2_grid = np.linspace(-1, 1, 100) xx1_grid, xx2_grid = np.meshgrid(x1_grid, x2_grid) from scipy.interpolate import griddata yy_interp_2D = griddata(X_scatter, yy_scatter, (xx1_grid, xx2_grid), method=\u0026#39;nearest\u0026#39;) 方法：\u0026rsquo;nearest\u0026rsquo;,\u0026rsquo;linear\u0026rsquo;, \u0026lsquo;cubic\u0026rsquo;\n​\t最邻近、线性、三次样条\n其他二维插值 # import matplotlib.pyplot as plt import numpy as np methods = [\u0026#39;none\u0026#39;, \u0026#39;nearest\u0026#39;, \u0026#39;bilinear\u0026#39;, \u0026#39;bicubic\u0026#39;, \u0026#39;spline16\u0026#39;, \u0026#39;spline36\u0026#39;, \u0026#39;hanning\u0026#39;, \u0026#39;hamming\u0026#39;, \u0026#39;hermite\u0026#39;, \u0026#39;kaiser\u0026#39;, \u0026#39;quadric\u0026#39;, \u0026#39;catrom\u0026#39;, \u0026#39;gaussian\u0026#39;, \u0026#39;bessel\u0026#39;, \u0026#39;mitchell\u0026#39;, \u0026#39;sinc\u0026#39;, \u0026#39;lanczos\u0026#39;, \u0026#39;blackman\u0026#39;] def surface(x1,x2): v = (x1 + x2)*np.exp(-2*(x1**2 + x2**2)) return v x1_data = np.linspace(-1, 1, 5) x2_data = np.linspace(-1, 1, 5) xx1_data, xx2_data = np.meshgrid(x1_data, x2_data) yy_data = surface(xx1_data,xx2_data) fig, axs = plt.subplots(nrows=3, ncols=6, figsize=(9, 6), subplot_kw={\u0026#39;xticks\u0026#39;: [], \u0026#39;yticks\u0026#39;: []}) for ax, interp_method in zip(axs.flat, methods): ax.imshow(yy_data, interpolation=interp_method, cmap=\u0026#39;RdBu_r\u0026#39;) ax.set_title(str(interp_method)) plt.tight_layout() plt.show() 2、可视化 # 成对特征图 # sns.pairplot(iris_df, hue=\u0026#39;species\u0026#39;, palette = \u0026#34;bright\u0026#34;) 热图 # h = sns.heatmap(data, cmap=\u0026#39;RdYlBu_r\u0026#39;, annot=True, # 设置显示数值 xticklabels=list(X_df.columns),\t# 设置 X 轴上的标签 cbar_kws={\u0026#34;orientation\u0026#34;: \u0026#34;vertical\u0026#34;},\t# 设置色的属性，设置了 orientatio 为 vertical 表示垂直方向 vmin=-1, vmax=9)\t# vmin 和 vmax 设置颜色映射的最小值和最大值 h.set_aspect(\u0026#34;equal\u0026#34;) # 设置热力图的纵横比为相等 核密度图 # fig, ax = plt.subplots() sns.kdeplot(data=X,fill=True, # 填充曲线下方的区域 common_norm=False, # 每个曲线使用自己的归一化 alpha=.3, linewidth=1, palette = \u0026#34;viridis\u0026#34;)\t# 设置颜色方案 plt.title(\u0026#39;Distribution of X columns\u0026#39;)\t# 标题 sns.displot( data=diamonds, x=\u0026#34;carat\u0026#34;, hue=\u0026#34;cut\u0026#34;, kind=\u0026#34;kde\u0026#34;, height=6, multiple=\u0026#34;fill\u0026#34;, clip=(0, None), palette=\u0026#34;ch:rot=-.25,hue=1,light=.75\u0026#34; ) 小提琴图 # fig, ax = plt.subplots() sns.violinplot(data=X_df, palette=\u0026#34;Set3\u0026#34;, # 设置颜色方案 bw=.2,\t# 设置内核密度估计的带宽 cut=1, # 设置小提琴图的形状剪切程度 linewidth=0.25, inner=\u0026#34;points\u0026#34;,# 设置内部图形的类型， \u0026#34;points\u0026#34; 表示在小提琴图内部绘制数据点 orient=\u0026#34;v\u0026#34;)\t# 设置图的方向为垂直方向 ax.grid(linestyle=\u0026#39;--\u0026#39;, linewidth=0.25, color=[0.5,0.5,0.5])\t# 添加灰色网格线 平行坐标图 # fig, ax = plt.subplots() pd.plotting.parallel_coordinates(iris_sns, \u0026#39;species\u0026#39;, colormap=plt.get_cmap(\u0026#34;Set2\u0026#34;)) 缺失值热图 # import missingno as msno msno.matrix(iris_df_NaN)\t# 每条白带代表缺失值 三维线框图 # x1_data = np.linspace(-1, 1, 5) x2_data = np.linspace(-1, 1, 5) xx1_data, xx2_data = np.meshgrid(x1_data, x2_data) def surface(x1,x2): v = (x1 + x2)*np.exp(-2*(x1**2 + x2**2)) return v yy_data = surface(xx1_data,xx2_data) fig = plt.figure() ax = plt.axes(projection =\u0026#34;3d\u0026#34;) ax.scatter(xx1_data, xx2_data, yy_data, marker = \u0026#39;x\u0026#39;, c = \u0026#39;k\u0026#39; ax.plot_wireframe(xx1_data, xx2_data, yy_data) 二维颜色图 # x1_grid = np.linspace(-1.1, 1.1, 23) x2_grid = np.linspace(-1.1, 1.1, 23) xx1_grid, xx2_grid = np.meshgrid(x1_grid, x2_grid) # 定义颜色图设置 lims = dict(cmap=\u0026#39;RdBu_r\u0026#39;, vmin=-0.4, vmax=0.4) # 绘制二维颜色图 plt.pcolormesh(xx1_grid, xx2_grid, yy_interp_2D, shading=\u0026#39;auto\u0026#39;, **lims) plt.scatter(xx1_data.ravel(),xx2_data.ravel(), marker = \u0026#39;x\u0026#39;, c = \u0026#39;k\u0026#39;) 双变量关系图 # sns.jointplot(data=y_x_df, x=\u0026#34;SP500\u0026#34;, y=\u0026#34;AAPL\u0026#34;, # 指定 x 轴的数据列为 \u0026#34;SP500\u0026#34; y 轴的数据列为 \u0026#34;AAPL\u0026#34; kind = \u0026#39;scatter\u0026#39;, # 绘制的图表类型为散点图 hue = \u0026#39;species\u0026#39;,\t# 分类 palette = \u0026#34;viridis\u0026#34;\t# 颜色 xlim = [-0.15,0.15],ylim = [-0.15,0.15]) sns.jointplot(data=y_x_df, x=\u0026#34;SP500\u0026#34;, y=\u0026#34;AAPL\u0026#34;, kind=\u0026#34;kde\u0026#34;, cmap = \u0026#39;Blues\u0026#39;, fill = True, # 核密度估计图 xlim = [-0.15,0.15],ylim = [-0.15,0.15]) 两个标准差向量的箭头图 # 两个标准差向量的箭头。夹角越小，说明因变量向量 y 和自变量向量 x 越相 近。也就是说，夹角越小，自变量向量 x 能更充分解释因变量向量 y。\nRHO = y_x_df.corr() Angles = np.arccos(RHO)*180/np.pi def draw_vector(vector,RBG): array = np.array([[0, 0, vector[0], vector[1]]]) X, Y, U, V = zip(*array) plt.quiver(X, Y, U, V,angles=\u0026#39;xy\u0026#39;, scale_units=\u0026#39;xy\u0026#39;,scale=1,color = RBG) angle = Angles[\u0026#39;AAPL\u0026#39;][\u0026#39;SP500\u0026#39;]*np.pi/180 vols = np.sqrt(np.diag(SIGMA)) v_1_x = vols[1] v_1_y = 0 v_2_x = vols[0]*np.cos(angle) v_2_y = vols[0]*np.sin(angle) draw_vector([v_1_x,v_1_y],np.array([0,112,192])/255) draw_vector([v_2_x,v_2_y],np.array([255,0,0])/255) plt.ylabel(\u0026#39;$y, TSLA$\u0026#39;) plt.xlabel(\u0026#39;$x, S\u0026amp;P500$\u0026#39;) plt.axis(\u0026#39;scaled\u0026#39;) plt.axhline(y=0, color=\u0026#39;k\u0026#39;) plt.axvline(x=0, color=\u0026#39;k\u0026#39;) ax.set_xlim([-0.01, 0.03]) ax.set_ylim([-0.01, 0.03]) ax.grid(linestyle=\u0026#39;--\u0026#39;, linewidth=0.25, color=[0.5,0.5,0.5]) 等高线图 # # 绘制二维等高线图 contour_h = ax.contourf(x, y, z,levels = np.linspace(0,1,15), cmap=\u0026#39;RdYlBu_r\u0026#39;) # 绘制线框图 ax.plot_wireframe(x, y, z, rstride=4, cstride=4, color = [0.7,0.7,0.7], linewidth = 0.25) # 绘制三维等高线图 contour_h = ax.contour3D(x, y, z, levels = np.linspace(0,1,15), cmap=\u0026#39;RdYlBu_r\u0026#39;) 聚类热图 # import seaborn as sns g = sns.clustermap(corr_P, cmap=\u0026#34;coolwarm\u0026#34;, annot=True) # clustermap 函数绘制相关性矩阵 corr_P 的聚类热图。 # cmap=\u0026#34;coolwarm\u0026#34; 设置颜色映射为 \u0026#34;coolwarm\u0026#34;，annot=True 表示在热图上显示数值标签 g.ax_row_dendrogram.remove() # 去除热图左侧的聚类树状图 3、回归 # 1、线性回归 # import statsmodels.api as sm # 特征增加一列 1 X_df = sm.add_constant(x_df) model = sm.OLS(y_df, X_df) results = model.fit() print(results.summary()) 各种统计量：多元线性回归、三个平方和、t检验、多重共线性\n2、岭回归、套索回归、弹性网络回归 # from sklearn.linear_model import Ridge clf = Ridge()\t# Lasso() 求解套索回归问题 # ElasticNet() 求解弹性网络回归问题 # lars_path() 生成 Lasso 回归参数轨迹图 alphas = np.logspace(-4, 2, 200) coefs = [] # 存储每个正则化强度下的系数 errors = [] # 存储每个正则化强度下的均方误差 coeff_df = pd.DataFrame() # 存储每个特征的系数 # Train the model with different regularisation strengths for a in alphas: clf.set_params(alpha=a) clf.fit(X_df, y_df) coefs.append(clf.coef_) errors.append(mean_squared_error(clf.coef_, b)) b_i = clf.coef_\t# 提取系数 b_i，并将其转换为一个包含特征名称的 DataFrame b_X_df tickers[1:]特征名称列表 b_X_df = pd.DataFrame(data=b_i[1:].T, index = tickers[1:], columns=[a]) coeff_df = pd.concat([coeff_df, b_X_df], axis = 1) fig, ax = plt.subplots() # 随着 α 增大，岭回归参数变化 h = sns.lineplot(data=coeff_df.T,markers=False, dashes=False,palette = \u0026#34;husl\u0026#34;) plt.axhline(y=0, color=\u0026#39;k\u0026#39;, linestyle=\u0026#39;--\u0026#39;) # 水平线 h.legend(loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1, 0.5)) plt.tight_layout() ax.set_xscale(\u0026#39;log\u0026#39;) 3、多项式回归 # polynomial_features = PolynomialFeatures(degree=degrees[i], # 用于生成多项式特征的转换器 include_bias=False) # 表示不包括截距列 linear_regression = LinearRegression() # Pipeline 通过命名的步骤（Step）来定义数据处理流程。 pipeline = Pipeline([(\u0026#34;polynomial_features\u0026#34;, polynomial_features), # 第一个步骤被命名为 \u0026#34;polynomial_features\u0026#34;，对应的转换器是 polynomial_features (\u0026#34;linear_regression\u0026#34;, linear_regression)]) # 第二个步骤被命名为 \u0026#34;linear_regression\u0026#34;，对应的模型是 linear_regression pipeline.fit(X[:, np.newaxis], y) # 使用 np.newaxis，可以在需要的位置插入新的维度 X_test = np.linspace(0, 1, 100) # 生成一个包含 100 个均匀分布在 0 到 1 之间的数据点的数组，用作测试数据 plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), color = \u0026#39;r\u0026#39;, label=\u0026#34;Fitted\u0026#34;) plt.scatter(X, y, edgecolor=\u0026#39;b\u0026#39;, s=20, label=\u0026#34;Data\u0026#34;) plt.xlabel(\u0026#34;x\u0026#34;) plt.ylabel(\u0026#34;y\u0026#34;) plt.xlim((0, 1)) plt.ylim((-2, 2)) plt.legend(loc=\u0026#34;best\u0026#34;) 4、逻辑回归 # from scipy.special import expit # sigmid函数 x = np.linspace(-5, 5, 100) f_x = expit(x) from sklearn.linear_model import LogisticRegression clf = LogisticRegression() clf.fit(X, y) y_hat = clf.predict(X_test) y_prob = clf.predict_proba(X_test) # 对测试数据进行概率估计，得到每个样本属于不同类别的概率 # y_prob 是一个二维数组，每一行表示一个样本，每一列表示对应类别的概率 b1 = clf.coef_ # 系数 b0 = clf.intercept_ # 截距项 5、主成分分析 PCA\u0026ndash;特征值分解 # from sklearn.decomposition import PCA pcamodel = PCA(n_components=4) pca = pcamodel.fit_transform(X_df) ############ 特征值分解 X_SIGMA = np.cov(X.T) # 协方差 X_RHO = np.corrcoef(X.T) # 相关系数 Lambda,V_eigen = np.linalg.eig(X_SIGMA) # linalg.eig 函数计算特征值和特征向量 ############ 标准差、方差和均值 X_sigma = np.std (X, axis=0) X_VAR = np.var (X, axis=0) X_mu = np.mean(X, axis=0) pcamodel.components_, # .components_ 获取主成分分析模型中的主成分（principal components） # 主成分是通过对原始数据进行降维得到的一组新的变量，每个主成分都是原始变量的线性组合 ############ V 每一列代表一个主成分，该主成分中每一个元素相当于原始数据特征的系数。图 8所示为不同主成分的系数线图 # V.T @ V = I V = pcamodel.components_.transpose() # 将主成分权重矩阵进行转置 ############ 投影原始数据 X 到 Z Z = X@V Z_SIGMA = np.cov(Z.T) Z_RHO = np.corrcoef(Z.T) Z_sigma = np.std (Z, axis=0) Z_VAR = np.var (Z, axis=0) Z_mu = np.mean(Z, axis=0) ############ 重构 X X_re = np.zeros_like(X)\t# X1 + X2 + X3 + X4 重构 X for i in range(4): z_i = np.array([Z[:,i]]).T v_i = np.array([V[:,i]]).T X_i = z_i@(v_i.transpose()) X_re = X_re + X_i; # 绘制X_i的热图 fig, ax = plt.subplots() ax = sns.heatmap(X_i, cmap=\u0026#39;RdYlBu_r\u0026#39;, xticklabels=list(X_df.columns), cbar_kws={\u0026#34;orientation\u0026#34;: \u0026#34;vertical\u0026#34;}, vmin=-1, vmax=9) plt.title(\u0026#39;X_\u0026#39; + str(i+1)) ########## X1 + X2 来估计原始数据 z_12 = Z[:,0:2] v_12 = V[:,0:2] X_12 = z_12@(v_12.transpose()) ########## 误差 fig, ax = plt.subplots() ax = sns.heatmap(X - X_12, cmap=\u0026#39;RdYlBu_r\u0026#39;, cbar_kws={\u0026#34;orientation\u0026#34;: \u0026#34;vertical\u0026#34;}, vmin=-1, vmax=9) plt.title(\u0026#39;Error, E\u0026#39;) ########## PCA 可视化 from yellowbrick.features import PCA from yellowbrick.style import set_palette set_palette(\u0026#39;pastel\u0026#39;) # 应用柔和的颜色样式 # 二维 fig, ax = plt.subplots() visualizer = PCA(scale=True, proj_features=True) # scale=True 对数据进行缩放 proj_features=True 显示特征投影 visualizer.fit_transform(iris_sns[[ # fit_transform 方法传入要进行 PCA 的数据和目标变量 y，进行模型拟合和数据转换 \u0026#39;sepal_length\u0026#39;, \u0026#39;sepal_width\u0026#39;, \u0026#39;petal_length\u0026#39;,\u0026#39;petal_width\u0026#39;]], y) # visualizer.show() # 显示 PCA 可视化结果 # 三维 fig = plt.figure() ax = plt.axes(projection=\u0026#39;3d\u0026#39;) # 传入了额外的参数 projection=3，表示进行三维的 PCA 分析 visualizer = PCA(scale=True, proj_features=True,projection = 3) visualizer.fit_transform(iris_sns[[ \u0026#39;sepal_length\u0026#39;, \u0026#39;sepal_width\u0026#39;, \u0026#39;petal_length\u0026#39;,\u0026#39;petal_width\u0026#39;]], y) ########## 陡坡图 # pcamodel.explained_variance_\t方差解释值 # pcamodel.explained_variance_ratio_\t方差解释比例 fig, ax1 = plt.subplots() color = \u0026#39;tab:red\u0026#39; ax1.set_xlabel(\u0026#39;Principal component\u0026#39;) ax1.set_ylabel(\u0026#39;Variance explained (%)\u0026#39;, color=color) # 绘制主成分的方差解释比例曲线 plt.plot(range(1,len(pcamodel.explained_variance_ratio_ )+1), np.cumsum(pcamodel.explained_variance_ratio_,),\t# 返回逐项累加后的列表 color=color) ax1.tick_params(axis=\u0026#39;y\u0026#39;, labelcolor=color) ax1.set_ylim([0,1]) ax2 = ax1.twinx()\t# 复用坐标轴 color = \u0026#39;tab:blue\u0026#39; ax2.set_ylabel(\u0026#39;Variance\u0026#39;, color=color) # 绘制方差的柱状图 plt.bar(range(1,len(pcamodel.explained_variance_ )+1),pcamodel.explained_variance_ ) # 设置第二个y轴刻度线和刻度标签的颜色为蓝色 ax2.tick_params(axis=\u0026#39;y\u0026#39;, labelcolor=color) fig.tight_layout() plt.show() 5、PCA\u0026ndash;奇异值分解 # U, S, V = np.linalg.svd(X) # 对给定矩阵 X 进行奇异值分解 # U 是矩阵 X 的左奇异向量矩阵，其列向量是 X 的特征向量 # S 是一个对角矩阵，其对角线上的元素是 X 的奇异值 # V 是矩阵 X 的右奇异向量矩阵，其列向量也是 X 的特征向量 n_components = len(S) # 计算奇异值向量 S 的长度，确定主成分的数量 component_idx = range(1, n_components + 1) # 创建一个范围对象，用于表示主成分的索引 lambda_i = np.square(S)/(X.shape[0] - 1) # 计算每个主成分的方差解释比例 fig, axs = plt.subplots() plt.imshow(X, cmap=\u0026#39;gray\u0026#39;)\t# 原始图像 fig, ax = plt.subplots() ### 绘制原始奇异值的图形 plt.plot(component_idx, S) # 奇异值向量 S 作为 y 轴，绘制奇异值的图形 plt.grid() ax.set_xscale(\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Principal component\u0026#34;) plt.ylabel(\u0026#39;Singular value\u0026#39;) ## 方差解释比例 fig, ax = plt.subplots() plt.plot(component_idx, lambda_i) # 每个主成分的方差解释比例作为 y 轴，绘制方差解释比例的图形 plt.grid() ax.set_xscale(\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Principal component\u0026#34;) plt.ylabel(\u0026#39;Eigen value\u0026#39;) # 计算累积方差解释比例，绘制累积方差解释比例的图形 variance_explained = 100 * np.cumsum(lambda_i) / lambda_i.sum() fig, ax = plt.subplots() plt.plot(component_idx, variance_explained) ax.set_xscale(\u0026#39;log\u0026#39;) plt.xlabel(\u0026#34;Principal component\u0026#34;) plt.grid() plt.ylabel(\u0026#39;Cumulative variance explained (%)\u0026#39;) ## 数据重构 for rank in [1, 2, 4, 8, 16, 32, 64]: # 根据指定的主成分数量 rank，使用奇异值分解的结果来重构原始矩阵 X X_reconstruction = U[:, :rank] * S[:rank] @ V[:rank,:] fig, axs = plt.subplots(1, 2) axs[0].imshow(X_reconstruction, cmap=\u0026#39;gray\u0026#39;) axs[0].set_title(\u0026#39;X_reproduced with \u0026#39; + str(rank) + \u0026#39; PCs\u0026#39;) ## 误差 axs[1].imshow(X - X_reconstruction, cmap=\u0026#39;gray\u0026#39;) axs[1].set_title(\u0026#39;Error\u0026#39;) plt.show() 6、正交回归 # ## 一元正交回归 from scipy.odr import * # Define a function to fit the data with def linear_func(b, x): b0, b1 = b return b1*x + b0 linear_model = Model(linear_func) # Load data to the model data = RealData(x_df.T, y_df.T) # 正交距离回归 odr = ODR(data, linear_model, beta0=[0., 1.]) # 求解回归 out = odr.run() # 使用内置的 pprint 方法打印拟合结果 out.pprint() ## 多元正交回归 SIMGA = X_y_df.cov() Lambda, V = np.linalg.eig(SIMGA) idx = Lambda.argsort()[::-1] # 对特征值进行降序排序的索引 Lambda = Lambda[idx] # 排序后的特征值 V = V[:,idx] # 排序后的特征向量 lambda_min = np.min(Lambda) # 最小特征值 D = len(X_tickers) # 特征数 b_TLS_ = -V[0:D,D]/V[D,D] # TLS估计的系数向量 print(b_TLS_) b0_TLS_ = y_df.mean().values - b_TLS_@X_df.mean().values # TLS估计的截距项 print(b0_TLS_) b_TLS = np.hstack((b0_TLS_,b_TLS_)) # TLS估计的系数向量（包括截距项） labels = [\u0026#39;const\u0026#39;] + X_tickers # 系数标签 b_df_TLS = pd.DataFrame(data=b_TLS.T, index=[labels], columns=[\u0026#39;TLS\u0026#39;]) # 将TLS估计的系数向量转换为DataFrame # TLS预测 # y_hat_TLS = X_test@b_TLS + b0_TLS_ 7、主元回归 # 主元回归，因变量数据 y 完全不参与正交化，即仅仅 X 参与 PCA 分解，\n获得特征值由大到小排列 D 个主元 V = (v1, v2, \u0026hellip;, vD)；这 D 个主元方向 (v1, v2, \u0026hellip;, vD) 两两正交。 选取其中 k (k \u0026lt; D)个特征值较大主元 (v1, v2, \u0026hellip;, vk)，构造超平面； 最后，用最小二乘法将因变量 y 投影在超平面上。 from sklearn.decomposition import PCA import statsmodels.api as sm coeff_df = pd.DataFrame() explained_array = []\t# 解释率 num_PCs = [4,5,6,7,8,9]\t# 主元数量 for num_PC in num_PCs: pcamodel = PCA(n_components=num_PC) pca = pcamodel.fit_transform(X_df) V = pcamodel.components_.transpose() Z_df = X_df@V Z_plus_1_df = sm.add_constant(Z_df) model = sm.OLS(y_df, Z_plus_1_df) p_Z = model.fit().params b_Z = p_Z[1:].T\t# Z 矩阵的系数 b_X = V@b_Z # 各特征的系数 # tickers[1:] 特征列标签名称 b_X_df = pd.DataFrame(data=b_X.T, index = tickers[1:], columns = [\u0026#39;PC1~\u0026#39; + str(num_PC)]) explained = np.sum(pcamodel.explained_variance_ratio_) # 总解释率 explained_array.append(explained) # 解释率数组 coeff_df = pd.concat([coeff_df, b_X_df], axis = 1) # 按列堆叠系数 # 累计已释方差百分比变化情况 fig, ax = plt.subplots() plt.bar(num_PCs, explained_array) # 参与主元回归主元数量对于系数的影响 fig, ax = plt.subplots() h = sns.lineplot(data=coeff_df,markers=True, dashes=False,palette = \u0026#34;husl\u0026#34;) plt.axhline(y=0, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;) h.legend(loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1, 0.5)) plt.tight_layout() # 主元数量对于系数的影响，第二视角 fig, ax = plt.subplots() h = sns.lineplot(data=coeff_df.T,markers=True, dashes=False,palette = \u0026#34;husl\u0026#34;) plt.axhline(y=0, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;-\u0026#39;) h.legend(loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1, 0.5)) plt.tight_layout() plt.show() 8、KNN回归 # from sklearn import neighbors knn_regress = neighbors.KNeighborsRegressor(n_neighbors, weights=\u0026#39;distance\u0026#39;)\t# uniform knn_regress.fit(X_train, y_train) y_predict = knn_regress.predict(Q) 4、分类 # 1、K值临近分类（KNN） # from sklearn import neighbors, datasets # k-NN classification, weight = uniform clf = neighbors.KNeighborsClassifier(k_neighbors) clf.fit(X, y) y_predict = clf.predict(q) 2、最近质心分类 # from sklearn.neighbors import NearestCentroid # 收缩阈值大，每个类别数据质心向样本数据总体质心 µX 靠拢。 # 最近质心分类器--最近质心的类别即为该数据点的类别 clf = NearestCentroid(metric=\u0026#39;euclidean\u0026#39;, shrink_threshold=2) clf.fit(X, y) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) 3、高斯朴素贝叶斯分类 # from sklearn.naive_bayes import GaussianNB gnb = GaussianNB() gnb.fit(X, y) q = np.c_[xx1.ravel(), xx2.ravel()] y_predict = gnb.predict(q) 4、判别分析器 # from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis lda = LinearDiscriminantAnalysis()\t# Linear Discriminant Analysis, LDA一次判别分析 qda = QuadraticDiscriminantAnalysis()\t# Quadratic Discriminant Analysis, QDA二次判别分析 lda.fit(X, y) qda.fit(X, y) q = np.c_[xx1.ravel(), xx2.ravel()] y_predict_LDA = lda.predict(q) y_predict_QDA = qda.predict(q) 5、支持向量机SVM # from sklearn import svm clf = svm.SVC(kernel=\u0026#39;linear\u0026#39;, C=penalty) # 惩罚因子 clf.fit(X, y) # 决策边界的参数，包括权重向量 w 和截距 b。 w = clf.coef_[0] w1 = w[0] w2 = w[1] # 截距 b b = clf.intercept_[0] x1_vec = np.linspace(-5, 5) # decision boundary x2_decision = -w1/w2*x1_vec - b/w2 # upper boundary of margin 边界间隔上界 x2_up = -w1/w2*x1_vec - (b - 1)/w2 # lower boundary of margin 边界间隔下界 x2_down = -w1/w2*x1_vec - (b + 1)/w2 核函数\nfrom sklearn import svm from sklearn.preprocessing import StandardScaler X, y = ds X = StandardScaler().fit_transform(X) # 对 X 进行标准化处理，使得每个特征的均值为 0，标准差为 1 C = 3 # 正则化参数 C 。这个参数控制了分类器的容错能力和决策边界的平滑程度 models = (svm.SVC(kernel=\u0026#39;linear\u0026#39;, C=C), # 线性核函数 svm.SVC(kernel=\u0026#39;poly\u0026#39;, degree=2, gamma=\u0026#39;auto\u0026#39;, C=C), # degree=2 指定多项式核函数的次数为 2 svm.SVC(kernel=\u0026#39;poly\u0026#39;, degree=3, gamma=\u0026#39;auto\u0026#39;, C=C), # 3次核 svm.SVC(kernel=\u0026#39;rbf\u0026#39;, gamma=0.7, C=C), # 高斯核 svm.SVC(kernel=\u0026#39;sigmoid\u0026#39;, gamma=0.5, C=C)) # sigmoid 核 models = (clf.fit(X, y) for clf in models) Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) # 将 xx 和 yy 两个网格中的坐标展平并组合成一个特征矩阵 # 然后传入分类器 clf 的 decision_function 方法中，计算决策函数值 # 得到的是一个一维数组 Z = Z.reshape(xx.shape) 6、决策树 # from sklearn.tree import DecisionTreeClassifier, plot_tree clf = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes).fit(X, y) # clf = DecisionTreeClassifier(max_depth=max_depth).fit(X, y) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # 绘制树结构 plot_tree(clf, filled=True, feature_names=[names[0],names[1]], # 特征名 rounded = True) # 颜色映射 rgb = [[255, 238, 255], # red [219, 238, 244], # blue [228, 228, 228]] # black rgb = np.array(rgb)/255. cmap_light = ListedColormap(rgb) cmap_bold = [[255, 51, 0], [0, 153, 255],[138,138,138]] cmap_bold = np.array(cmap_bold)/255. # 绘制决策区域 plt.contourf(xx, yy, Z, cmap=cmap_light) # 绘制决策边界 plt.contour(xx, yy, Z, levels=[0,1,2], colors=np.array([0, 68, 138])/255.) # 绘制散点 sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],palette=cmap_bold, alpha=1.0, linewidth = 1, edgecolor=[1,1,1]) 7、高斯过程 # from sklearn.gaussian_process import GaussianProcessClassifier from sklearn.gaussian_process.kernels import RBF kernel = 1.0 * RBF([1.0])\t# 高斯过程高斯核函数 gpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y) q = np.c_[xx.ravel(), yy.ravel()] Z = gpc_rbf_isotropic.predict(q) y_predict = Z.reshape(xx.shape) 8、KMeans聚类 # from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=3) kmeans.fit(X_train) Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # 绘制簇心--簇心的垂直平分线是决策边界 centroids = kmeans.cluster_centers_ plt.scatter(centroids[:, 0], centroids[:, 1], marker=\u0026#34;x\u0026#34;, s=100, linewidths=1.5, color=\u0026#34;k\u0026#34;) 手肘法则寻找最合适的分类个数（拐点）—— 关键指标是误差平方和（ SSE） $$ SSE(X|K)=\\sum^K_{k=1}SSE(C_k)=\\sum^K_{k=1}\\sum_{x\\in C_k}\\left\\lVert x-\\mu_k\\right\\rVert ^2\u0026mdash;\\mu_k为簇心 $$\nfrom sklearn.cluster import KMeans distortions = [] for i in range(1, 11): km = KMeans( n_clusters=i, init=\u0026#39;random\u0026#39;, n_init=10, max_iter=300, tol=1e-04, random_state=0) km.fit(X_train) # 训练 distortions.append(km.inertia_)\t# SSE 值 fig, ax = plt.subplots() plt.plot(range(1, 11), distortions, marker=\u0026#39;x\u0026#39;) ax.grid(linestyle=\u0026#39;--\u0026#39;, linewidth=0.25, color=[0.5,0.5,0.5]) plt.xlabel(\u0026#39;Number of clusters\u0026#39;) plt.ylabel(\u0026#39;Distortion\u0026#39;) ax.set_xticks(range(1, 11)) plt.show() 轮廓图和轮廓系数\n轮廓系数 S_i $$ s_i = \\frac{b_i-a_i}{\\max(a_i,b_i)}\u0026mdash; a_i 为簇内不相似度，b_i 为簇间不相似度\\ a_i = \\frac{1}{count(C_k)-1}\\sum_{j\\in C_k,i\\neq j}d_{i,j}\u0026mdash;d_{i,j}为样本 i 和 j 之间距离\\ b_i = \\min\\frac{1}{count(C_m)}\\sum_{j\\in C_m}d_{i,j} $$ 理想的聚类结果是，簇内尽量紧密，簇间尽量远离。轮廓系数平均值越高，说明分类越合理。选择轮廓系数较高，并且轮廓图簇宽度均匀的类数。轮廓图结合肘部法则判断聚类簇数更合适。\nfrom sklearn.cluster import KMeans from yellowbrick.cluster import SilhouetteVisualizer from sklearn.metrics import silhouette_score kmeans = KMeans(n_clusters=n_clusters, random_state=10) cluster_labels = kmeans.fit_predict(X) # 计算轮廓系数 silhouette_avg = silhouette_score(X, cluster_labels) print(\u0026#34;For n_clusters =\u0026#34;, n_clusters, \u0026#34;The average silhouette_score is :\u0026#34;, silhouette_avg) # 轮廓图 visualizer = SilhouetteVisualizer(kmeans, colors=\u0026#39;yellowbrick\u0026#39;) visualizer.fit(X) visualizer.show() 9、高斯混合模型 # from sklearn.mixture import GaussianMixture estimators = {cov_type: GaussianMixture(n_components=3,covariance_type=cov_type, max_iter=20,random_state=0) for cov_type in [\u0026#39;tied\u0026#39;, \u0026#39;spherical\u0026#39;, \u0026#39;diag\u0026#39;, \u0026#39;full\u0026#39;]} # 协方差矩阵类型—tied (平移)、spherical (球面)、diag(对角)和 full (完全) 采用 AIC 和 BIC 选择高斯混合模型分量数量 $$ AIC=2K-2\\ln (L)\\ BIC=K\\ln (n)-2\\ln (L) $$ 其中2K、Kln(n)为惩罚项 L是似然函数值\nAIC和BIC的数值越小越好。\nfrom sklearn.mixture import GaussianMixture df_AIC_BIC = pd.DataFrame() n_components_range = range(1, 7) cv_types = [\u0026#39;spherical\u0026#39;, \u0026#39;tied\u0026#39;, \u0026#39;diag\u0026#39;, \u0026#39;full\u0026#39;] for cv_type in cv_types: for n_components in n_components_range: gmm = mixture.GaussianMixture(n_components=n_components, covariance_type=cv_type) gmm.fit(X) aic_i = gmm.aic(X); # 赤池信息量准则 (Akaike information criterion, AIC) bic_i = gmm.bic(X); # 贝叶斯信息准则 (Bayesian Information Criterion, BIC) log_likelihood = gmm.score(X) dic_i = pd.DataFrame({\u0026#39;Cov_type\u0026#39;: cv_type, \u0026#39;K_cluster\u0026#39;: [n_components], \u0026#39;AIC\u0026#39;: [aic_i], \u0026#39;BIC\u0026#39;: [bic_i]}) df_AIC_BIC = pd.concat([df_AIC_BIC,dic_i]) #%% 可视化 fig, ax = plt.subplots() # AIC for various K and covariance choices g = sns.barplot(x = \u0026#39;K_cluster\u0026#39;, y = \u0026#39;AIC\u0026#39;, hue = \u0026#39;Cov_type\u0026#39;, data = df_AIC_BIC, palette = \u0026#39;viridis\u0026#39;) plt.title(\u0026#39;AIC\u0026#39;) plt.axhline(y=df_AIC_BIC[\u0026#39;AIC\u0026#39;].min(), color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) g.set(ylim=(df_AIC_BIC[\u0026#39;AIC\u0026#39;].min()*0.9, None)) fig, ax = plt.subplots() # BIC for various K and covariance choices g = sns.barplot(x = \u0026#39;K_cluster\u0026#39;, y = \u0026#39;BIC\u0026#39;, hue = \u0026#39;Cov_type\u0026#39;, data = df_AIC_BIC, palette = \u0026#39;viridis\u0026#39;) plt.title(\u0026#39;BIC\u0026#39;) plt.axhline(y=df_AIC_BIC[\u0026#39;BIC\u0026#39;].min(), color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;) g.set(ylim=(df_AIC_BIC[\u0026#39;BIC\u0026#39;].min()*0.9, None)) plt.show() 10、层次聚类 # 层次聚类依据数据之间的距离远近，或者亲近度大小，将样本数据划分为簇。层次聚类可以通过自下而上 (agglomerative) 合并， 或者自上而下 (divisive) 分割来构造分层结构聚类\nfrom sklearn.cluster import AgglomerativeClustering from scipy.cluster.hierarchy import dendrogram,linkage clustering_algorithms = ( (\u0026#39;Single linkage\u0026#39;, \u0026#39;single\u0026#39;), # 最近点距离 (single linkage 或 nearest neighbor)，代号为 \u0026#39;single\u0026#39; (\u0026#39;Average linkage\u0026#39;, \u0026#39;average\u0026#39;), # 平均距离代号为 \u0026#39;average\u0026#39; (\u0026#39;Complete linkage\u0026#39;, \u0026#39;complete\u0026#39;), # 最远点距离代号为 \u0026#39;complete\u0026#39; (\u0026#39;Ward linkage\u0026#39;, \u0026#39;ward\u0026#39;), # Ward’s 簇间距离代号为 \u0026#39;ward\u0026#39; ) for name, method in clustering_algorithms: # 可视化 fig, ax = plt.subplots() # 使用 linkage 函数计算数据集 X 的层次聚类，并将结果可视化为树状图（dendrogram） plt.title(name) dend = dendrogram(linkage(X, method = method)) # 层次聚类\tlinkage 指定链接准则的方法 cluster = AgglomerativeClustering(n_clusters=3, affinity=\u0026#39;euclidean\u0026#39;, linkage=method) cluster.fit(X) # 生成网格 plot_step = 0.02 xx, yy = np.meshgrid(np.arange(4, 8+plot_step, plot_step), np.arange(1.5, 4.5+plot_step, plot_step)) # 预测类型 Z = cluster.fit_predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) fig, ax = plt.subplots() plt.title(name) # 决策区域 plt.contourf(xx, yy, Z, cmap=cmap_light) # 样本数据散点 plt.scatter(x=X[:, 0], y=X[:, 1], color=\u0026#39;r\u0026#39;, alpha=1.0, linewidth = 1, edgecolor=[1,1,1]) # 决策边界 plt.contour(xx, yy, Z, levels=[0,1,2], colors=np.array([0, 68, 138])/255.) ax.set_xticks(np.arange(4, 8.5, 0.5)) ax.set_yticks(np.arange(1.5, 5, 0.5)) ax.set_xlim(4, 8) ax.set_ylim(1.5, 4.5) plt.xlabel(iris.feature_names[0]) plt.ylabel(iris.feature_names[1]) ax.grid(linestyle=\u0026#39;--\u0026#39;, linewidth=0.25, color=[0.5,0.5,0.5]) ax.set_aspect(\u0026#39;equal\u0026#39;) plt.show() 亲密度层次聚类 # from sklearn.metrics.pairwise import rbf_kernel import matplotlib.pyplot as plt import numpy as np import seaborn as sns from sklearn import datasets # load iris data iris = datasets.load_iris() X = iris.data[:, [0,1]] # 成对高斯核亲近度矩阵热图 rbf_X = rbf_kernel(X) fig, ax = plt.subplots() ax = fig.add_subplot(1,2,1) sns.heatmap(rbf_X, cmap=\u0026#34;coolwarm\u0026#34;, square=True, linewidths=.05) # 下三角热图 ax = fig.add_subplot(1,2,2)\t# 添加子图 mask = np.triu(np.ones_like(rbf_X, dtype=bool)) sns.heatmap(rbf_X, cmap=\u0026#34;coolwarm\u0026#34;, mask = mask, square=True, linewidths=.05) # 亲密度层次聚类树形图 g = sns.clustermap(rbf_X, cmap=\u0026#34;coolwarm\u0026#34;) g.ax_row_dendrogram.remove() plt.show() 11、密度聚类 # 密度聚类是一种基于数据点密度的聚类方法，其核心思想是将高密度区域作为聚类中心，并 将低密度区域作为聚类边界。常用的密度聚类算法有 DBSCAN、OPTICS、DENCLUE 等\nDBSCAN 通过设定邻域半径和最小密度等参数，将具有足够密度的数据点聚成一个簇；全称为 Density-Based Spatial Clustering of Applications with Noise，是基于密度的聚类方法\nOPTICS 在 DBSCAN 的基础上，通过建立可达距离图来优化聚类结果；\nDENCLUE 则采用高斯核函数来建模数据点的密度，通过求解梯度的方式来寻找密度峰值，进而进行聚类。\n密度聚类方法对于数据分布的形态没有特殊要求，对于噪声和离群点的鲁棒性较强，具有广泛的应用价值。\nfrom sklearn import cluster from sklearn.preprocessing import StandardScaler # normalize dataset 标准化 通过减去均值然后除以标准差，处理后数据符合标准正态分布 X = StandardScaler().fit_transform(X) for eps in np.array([0.1,0.2,0.4,0.6]): # esp 限定领域范围 指的是以某样本数据点为中心、esp 为半径的区域 # 以空间某点为中心，esp 为半径邻域内包含至少 min_samples 数量的数据点，则称该点核心点 # min_samples 调节 DBSCAN 算法对噪声的容忍度 dbscan = cluster.DBSCAN(eps=eps,min_samples=10) y_pred = dbscan.fit_predict(X) # 创建一个循环迭代器，从循环迭代器中截取长度为 max(y_pred) + 1 的子序列，转为列表 fig, ax = plt.subplots() colors = np.array(list(islice(cycle([\u0026#39;#377eb8\u0026#39;, \u0026#39;#ff7f00\u0026#39;, \u0026#39;#4daf4a\u0026#39;, \u0026#39;#f781bf\u0026#39;, \u0026#39;#a65628\u0026#39;, \u0026#39;#984ea3\u0026#39;, \u0026#39;#999999\u0026#39;, \u0026#39;#e41a1c\u0026#39;, \u0026#39;#dede00\u0026#39;]), int(max(y_pred) + 1)))) # 离群样本的标签为-1，为离群点添加为黑色 colors = np.append(colors, [\u0026#34;#000000\u0026#34;]) plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred]) plt.title(\u0026#39;eps = %0.2f\u0026#39; % eps) plt.xlim(-2.5, 2.5) plt.ylim(-2.5, 2.5) plt.xticks(()) plt.yticks(()) plt.axis(\u0026#39;equal\u0026#39;) plt.show() 12、谱聚类 # 谱聚类 (spectral clustering) 是一种基于图论的聚类算法，其特点是能够处理高维数据和非凸数 据簇，并且对于数据分布的形态没有特殊要求。\n优点是可以在任意维度上进行聚类，并且不会受到噪声的影响。缺点是需要进行谱分解计算，计算量较大。\n具体来说，谱聚类的思路是将样本数据看做是空间节点 (node)，这些节点之间用边 (edge) 连 构成的无向图 (undirected graph)，也叫加权图。无向图中，距离远的数据点，边的权重值低；距离近的数据点，在无向图中，边的权重值高。 用无向图聚类的过程很简单，切断无向图中权重值低的边，得到一系列子图。子图内部节点之间边的权重尽可能高，子图之间边权重尽可能低。\n将节点之间的相似度构成的矩阵称为邻接矩阵，通过对邻接矩阵进行谱分解，得到数据点的特征向量，进而将其映射到低维空间进行聚类。\nfrom sklearn import cluster from sklearn.preprocessing import StandardScaler X = StandardScaler().fit_transform(X) spectral = cluster.SpectralClustering( # 谱聚类算法 n_clusters=2, eigen_solver=\u0026#39;arpack\u0026#39;,\t# 特征值求解的策略，{None, ‘arpack’, ‘lobpcg’, or ‘amg’} affinity=\u0026#34;nearest_neighbors\u0026#34;) y_pred = spectral.fit_predict(X) fig, ax = plt.subplots() colors = np.array(list(islice(cycle([\u0026#39;#377eb8\u0026#39;, \u0026#39;#ff7f00\u0026#39;, \u0026#39;#4daf4a\u0026#39;, \u0026#39;#f781bf\u0026#39;, \u0026#39;#a65628\u0026#39;, \u0026#39;#984ea3\u0026#39;, \u0026#39;#999999\u0026#39;, \u0026#39;#e41a1c\u0026#39;, \u0026#39;#dede00\u0026#39;]), int(max(y_pred) + 1)))) plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred]) plt.xlim(-2.5, 2.5) plt.ylim(-2.5, 2.5) plt.xticks(()) plt.yticks(()) plt.axis(\u0026#39;equal\u0026#39;) plt.show() 5、深度学习 # 1、多层感知机 # import torch from torch import nn from d2l import torch as d2l num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256 dropout1, dropout2 = 0.2, 0.5 num_epochs, lr, batch_size = 10, 0.5, 256 loss = nn.CrossEntropyLoss(reduction=\u0026#39;none\u0026#39;) train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), # 在第一个全连接层之后添加一个dropout层 nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(), # 在第二个全连接层之后添加一个dropout层 nn.Dropout(dropout2), nn.Linear(256, 10)) def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01) net.apply(init_weights) trainer = torch.optim.SGD(net.parameters(), lr=lr) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer) import torch from torch import nn from torch.nn import functional as F class MLP(nn.Module): # 用模型参数声明层。这里，我们声明两个全连接的层 def __init__(self): # 调用MLP的父类Module的构造函数来执行必要的初始化。 # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍） super().__init__() self.hidden = nn.Linear(20, 256) # 隐藏层 self.out = nn.Linear(256, 10) # 输出层 # 定义模型的前向传播，即如何根据输入X返回所需的模型输出 def forward(self, X): # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。 return self.out(F.relu(self.hidden(X))) X = torch.rand(2, 20) net = MLP() print(\u0026#39;--------------------------------------------------\u0026#39;) print(net(X)) 2、LeNet-5 # import torch from torch import nn from d2l import torch as d2l net = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32) for layer in net: X = layer(X) print(layer.__class__.__name__,\u0026#39;output shape: \\t\u0026#39;,X.shape) batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size) #@save def train_ch6(net, train_iter, test_iter, num_epochs, lr, device): \u0026#34;\u0026#34;\u0026#34;用GPU训练模型(在第六章定义)\u0026#34;\u0026#34;\u0026#34; def init_weights(m): if type(m) == nn.Linear or type(m) == nn.Conv2d: nn.init.xavier_uniform_(m.weight) net.apply(init_weights) print(\u0026#39;training on\u0026#39;, device) net.to(device) optimizer = torch.optim.SGD(net.parameters(), lr=lr) loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, xlim=[1, num_epochs], legend=[\u0026#39;train loss\u0026#39;, \u0026#39;train acc\u0026#39;, \u0026#39;test acc\u0026#39;]) timer, num_batches = d2l.Timer(), len(train_iter) for epoch in range(num_epochs): # 训练损失之和，训练准确率之和，样本数 metric = d2l.Accumulator(3) net.train() for i, (X, y) in enumerate(train_iter): timer.start() optimizer.zero_grad() X, y = X.to(device), y.to(device) y_hat = net(X) l = loss(y_hat, y) l.backward() optimizer.step() with torch.no_grad(): metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0]) timer.stop() train_l = metric[0] / metric[2] train_acc = metric[1] / metric[2] if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1: animator.add(epoch + (i + 1) / num_batches, (train_l, train_acc, None)) test_acc = d2l.evaluate_accuracy_gpu(net, test_iter) animator.add(epoch + 1, (None, None, test_acc)) print(f\u0026#39;loss {train_l:.3f}, train acc {train_acc:.3f}, \u0026#39; f\u0026#39;test acc {test_acc:.3f}\u0026#39;) print(f\u0026#39;{metric[2] * num_epochs / timer.sum():.1f} examples/sec \u0026#39; f\u0026#39;on {str(device)}\u0026#39;) lr, num_epochs = 0.9, 20 train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 3、AlexNet # import torch from torch import nn from d2l import torch as d2l AlexNet = nn.Sequential( nn.Conv2d(3, 96, kernel_size=11, padding=4), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), nn.Linear(256 * 26 * 26 , 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 1000) ) net = nn.Sequential( nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(), nn.Linear(6400 , 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), nn.Linear(4096, 10) ) X = torch.tensor(torch.randn((1,1,224,224), dtype=torch.float32)) for layer in net: X = layer(X) print(layer.__class__.__name__ ,\u0026#34;output shape :\\t\u0026#34; , X.shape) batch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size = batch_size, resize=224) lr, num_epochs = 0.01, 10 d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 4、VGG # import torch from torch import nn from d2l import torch as d2l def vgg_block(num_convs, in_channels, out_channels): layers = [] for _ in range(num_convs): layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)) layers.append(nn.ReLU()) in_channels = out_channels layers.append(nn.MaxPool2d(kernel_size=2,stride=2)) return nn.Sequential(*layers) conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512)) def vgg(conv_arch): conv_blks = [] in_channels = 1 # 卷积层部分 for (num_convs, out_channels) in conv_arch: conv_blks.append(vgg_block(num_convs, in_channels, out_channels)) in_channels = out_channels return nn.Sequential( *conv_blks, nn.Flatten(), # 全连接层部分 nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4096, 10)) net = vgg(conv_arch) X = torch.randn(size=(1, 1, 224, 224)) for blk in net: X = blk(X) print(blk.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;,X.shape) ratio = 4 small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch] net = vgg(small_conv_arch) lr, num_epochs, batch_size = 0.05, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 5、NIN # import torch from torch import nn from d2l import torch as d2l def nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride=strides, padding=padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU() ) NiN=nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(kernel_size=3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(kernel_size=3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(kernel_size=3, stride=2), nn.Dropout(0.5), # 10类标签 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()) X = torch.tensor(torch.randn((1,1,224,224),dtype=torch.float32)) for layer in NiN: X = layer(X) print(layer.__class__.__name__, \u0026#34;:\\t\u0026#34;, X.shape) lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224) d2l.train_ch6(NiN, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 6、GoogLeNet # import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l class Inception(nn.Module): # c1--c4是每条路径的输出通道数 def __init__(self, in_channels, c1, c2, c3, c4, **kwargs): super(Inception, self).__init__(**kwargs) # 线路1，单1x1卷积层 self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1) # 线路2，1x1卷积层后接3x3卷积层 self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1) self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1) # 线路3，1x1卷积层后接5x5卷积层 self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1) self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2) # 线路4，3x3最大汇聚层后接1x1卷积层 self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1) def forward(self, x): p1 = F.relu(self.p1_1(x)) p2 = F.relu(self.p2_2(F.relu(self.p2_1(x)))) p3 = F.relu(self.p3_2(F.relu(self.p3_1(x)))) p4 = F.relu(self.p4_2(self.p4_1(x))) # 在通道维度上连结输出 return torch.cat((p1, p2, p3, p4), dim=1) b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(), nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32), Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64), Inception(512, 160, (112, 224), (24, 64), 64), Inception(512, 128, (128, 256), (24, 64), 64), Inception(512, 112, (144, 288), (32, 64), 64), Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128), Inception(832, 384, (192, 384), (48, 128), 128), nn.AdaptiveAvgPool2d((1,1)), nn.Flatten()) net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10)) # X = torch.rand(size=(1, 1, 96, 96)) # for layer in net: # X = layer(X) # print(layer.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;, X.shape) # Sequential output shape: torch.Size([1, 64, 24, 24]) # Sequential output shape: torch.Size([1, 192, 12, 12]) # Sequential output shape: torch.Size([1, 480, 6, 6]) # Sequential output shape: torch.Size([1, 832, 3, 3]) # Sequential output shape: torch.Size([1, 1024]) # Linear output shape: torch.Size([1, 10]) lr, num_epochs, batch_size = 0.1, 10, 128 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) 7、ResNet # import torch from torch import nn from torch.nn import functional as F from d2l import torch as d2l class Residual(nn.Module): #@save def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1): super().__init__() self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides) self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1) if use_1x1conv: self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides) else: self.conv3 = None self.bn1 = nn.BatchNorm2d(num_channels) self.bn2 = nn.BatchNorm2d(num_channels) def forward(self, X): Y = F.relu(self.bn1(self.conv1(X))) Y = self.bn2(self.conv2(Y)) if self.conv3: X = self.conv3(X) Y += X return F.relu(Y) blk = Residual(3,3) X = torch.rand(4, 3, 6, 6) Y = blk(X) print(Y.shape) blk = Residual(3,6, use_1x1conv=True, strides=2) print(blk(X).shape) b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) def resnet_block(input_channels, num_channels, num_residuals,first_block=False): blk = [] for i in range(num_residuals): if i == 0 and not first_block: blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2)) else: blk.append(Residual(num_channels, num_channels)) return blk b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True)) b3 = nn.Sequential(*resnet_block(64, 128, 2)) b4 = nn.Sequential(*resnet_block(128, 256, 2)) b5 = nn.Sequential(*resnet_block(256, 512, 2)) net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1,1)), nn.Flatten(), nn.Linear(512, 10)) # X = torch.rand(size=(1, 1, 224, 224)) # for layer in net: # X = layer(X) # print(layer.__class__.__name__,\u0026#39;output shape:\\t\u0026#39;, X.shape) lr, num_epochs, batch_size = 0.05, 10, 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96) d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu()) ","externalUrl":null,"permalink":"/docs/python/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/","section":"Docs","summary":"1、数据处理 # 1、缺失值 # is_NaN = iris_df_NaN.isna() # 判断 Pandas 数据帧是否为缺失值，","title":"机器学习","type":"docs"},{"content":" 创建性设计模式 # 一、单例模式 # 一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。\n单例模式有⼀个特点就是不允许外部直接创建。构造函数上添加私有属性 作用 # 解决资源冲突 使用类级别锁、分布式锁、单例模式 表示全局唯一类 如配置信息类，在系统中只应保存一份。再如唯一递增 ID 生成器 多线程id生成器案例 # 饿汉式 在类加载的时候，instance 静态实例就已经创建并初始化好了，所以，instance 实例的创建过程是线程安全的。不过，这样的实现方式不支持延迟加载 public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static final IdGenerator instance = new IdGenerator(); private IdGenerator() { } public static IdGenerator getInstance() { return instance; } public long getId() { return id.incrementAndGet(); } } 懒汉式（线程安全-线程不安全的类型只是少了一把锁） 懒汉式相对于饿汉式的优势是支持延迟加载。 并发度很低。量化一下的话，并发度是 1，也就相当于串行操作了。而这个函数是在单例使用期间，一直会被调用。如果这个单例类偶尔会被用到，那这种实现方式还以接受。但是，如果频繁地用到，那频繁加锁、释放锁及并发度低等问题，会导致性能瓶颈，这种实现方式就不可取了。\npublic class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() { } public static synchronized IdGenerator getInstance() { if (instance == null) { instance = new IdGenerator(); } return instance; } public long getId() { return id.incrementAndGet(); } } 双重检测 既支持延迟加载、又支持高并发的单例实现方式。只要 instance 被创建之后，即便再调用 getInstance() 函数也不会再进入到加锁逻辑中了。所以，这种实现方式解决了懒汉式并发度低的问题。 public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private IdGenerator() { } public static IdGenerator getInstance() { if (instance == null) { synchronized(IdGenerator.class) { // 此处为类级别的锁 if (instance == null) { instance = new IdGenerator(); } } } return instance; } public long getId() { return id.incrementAndGet(); } } 网上有人说，这种实现方式有些问题。因为指令重排序，可能会导致 IdGenerator 对象被 new 出来，并且赋值给 instance 之后，还没来得及初始化（执行构函数中的代码逻辑），就被另一个线程使用了。\n要解决这个问题，我们需要给 instance 成员变量加上 volatile 关键字，禁止指令重排序才行。实际上，只有很低版本的 Java 才会有这个问题。我们现在用的高版本的 Java 已经在 JDK 内部实现中解决了这个问题（解决的方法很简单，只要把对象 new 操作和初始化操作设计为原子操作，就自然能禁止重排序）。 4. 静态内部类 当外部类 IdGenerator 被加载的时候，并不会创建SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。insance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。\npublic class IdGenerator { private AtomicLong id = new AtomicLong(0); private IdGenerator() { } private static class SingletonHolder{ private static final IdGenerator instance = new IdGenerator(); } public static IdGenerator getInstance() { return SingletonHolder.instance; } public long getId() { return id.incrementAndGet(); } } 枚举 最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性 public enum IdGenerator { INSTANCE; private AtomicLong id = new AtomicLong(0); public long getId() { return id.incrementAndGet(); } } CAS public class IdGenerator { private AtomicLong id = new AtomicLong(0); private static final AtomicReference\u0026lt;IdGenerator\u0026gt; INSTANCE = new AtomicReference\u0026lt;IdGenerator\u0026gt;(); private static IdGenerator instance; private IdGenerator() { } public static final IdGenerator getInstance() { while (true) { IdGenerator instance = INSTANCE.get(); if (instance != null) { return instance; } INSTANCE.compareAndSet(null, new IdGenerator()); return INSTANCE.get(); } } public long getId() { return id.incrementAndGet(); } } 线程单例、集群单例 # 线程单例：ThreadLocal 或者 ConcurrentHashMap 以 线程id 为 key，单例对象为 value 集群单例：把这个单例对象序列化并存储到外部共享存储区（比如文件）。进程在使用这个单例对象的时候，需要先从外部共享存储区中将它读取到内存，并反序列化成对象，然后再使用，使用完成之后还需要再存储回外部共享存储区。为了保证任何时刻，在进程间都只有一份对象存在，一个进程在获取到对象之后，需要对对象加分布式锁，避免其他进程再将其获取。在进程使用完这个对象之后，还需要显式地将对象从内存中删除，并且释放锁。\npublic class IdGenerator { private AtomicLong id = new AtomicLong(0); private static IdGenerator instance; private static SharedObjectStorage storage = FileSharedObjectStorage(/*入参省略*/); private static DistributedLock lock = new DistributedLock(); private IdGenerator() { } public synchronized static IdGenerator getInstance(){ if (instance == null) { lock.lock(); instance = storage.load(IdGenerator.class); } return instance; } public synchroinzed void freeInstance() { storage.save(this, IdGeneator.class); instance = null; //释放对象 lock.unlock(); } public long getId() { return id.incrementAndGet(); } } // IdGenerator使用举例 IdGenerator idGeneator = IdGenerator.getInstance(); long id = idGenerator.getId(); IdGenerator.freeInstance(); 二、简单工厂模式 # ⼯⼚模式⼜称⼯⼚⽅法模式，是⼀种创建型设计模式，其在⽗类中提供⼀个创建对象的⽅法， 允许⼦类决定实例化对象的类型 发放奖品案例： # 定义发奖接⼝ # 所有的奖品通过实现此接⼝进⾏处理，以保证最终**⼊参出参的统⼀性** 接⼝的⼊参包括:⽤户ID 、奖品ID 、业务ID 以及扩展字段⽤于处理发放实物商品时的收货地址 public interface ICommodity { void sendCommodity(String uId, String commodityId, String bizId, Map\u0026lt;String, String\u0026gt; extMap) throws Exception; } 实现奖品发放接⼝ # // 优惠券 public class CouponCommodityService implements ICommodity { private CouponService couponService = new CouponService(); public void sendCommodity(String uId, String commodityId, String bizId, Map\u0026lt;String, String\u0026gt; extMap) throws Exception { CouponResult couponResult = couponService.sendCoupon(uId, commodityId, bizId); log.info(\u0026#34;请求参数[优惠券] =\u0026gt; uId：{} commodityId：{} bizId：{} extMap：{}\u0026#34;, uId, commodityId, bizId, JSON.toJSON(extMap)); log.info(\u0026#34;测试结果[优惠券]：{}\u0026#34;, JSON.toJSON(couponResult)); if (!\u0026#34;0000\u0026#34;.equals(couponResult.getCode())) { throw new RuntimeException(couponResult.getInfo()); } } } // 实物商品 public class GoodsCommodityService implements ICommodity { private GoodsService goodsService = new GoodsService(); public void sendCommodity(String uId, String commodityId, String bizId, Map\u0026lt;String, String\u0026gt; extMap) throws Exception { DeliverReq deliverReq = new DeliverReq(); deliverReq.setUserName(queryUserName(uId)); deliverReq.setUserPhone(queryUserPhoneNumber(uId)); deliverReq.setSku(commodityId); deliverReq.setOrderId(bizId); deliverReq.setConsigneeUserName(extMap.get(\u0026#34;consigneeUserName\u0026#34;)); deliverReq.setConsigneeUserPhone(extMap.get(\u0026#34;consigneeUserPhone\u0026#34;)); deliverReq.setConsigneeUserAddress(extMap.get(\u0026#34;consigneeUserAddress\u0026#34;)); Boolean isSuccess = goodsService.deliverGoods(deliverReq); log.info(\u0026#34;请求参数[优惠券] =\u0026gt; uId：{} commodityId：{} bizId：{} extMap：{}\u0026#34;, uId, commodityId, bizId, JSON.toJSON(extMap)); log.info(\u0026#34;测试结果[优惠券]：{}\u0026#34;, isSuccess); if (!isSuccess) { throw new RuntimeException(\u0026#34;实物商品发放失败\u0026#34;); } } private String queryUserName(String uId) { return \u0026#34;花花\u0026#34;; } private String queryUserPhoneNumber(String uId) { return \u0026#34;15200101232\u0026#34;; } } // 第三⽅兑换卡 public class CardCommodityService implements ICommodity { // 模拟注⼊ private IQiYiCardService iQiYiCardService = new IQiYiCardService(); public void sendCommodity(String uId, String commodityId, String bizId, Map\u0026lt;String, String\u0026gt; extMap) throws Exception { String mobile = queryUserMobile(uId); iQiYiCardService.grantToken(mobile, bizId); logger.info(\u0026#34;请求参数[爱奇艺兑换卡] =\u0026gt; uId：{} commodityId：{} bizId：{} extMap：{}\u0026#34;, uId, commodityId, bizId, JSON.toJSON(extMap)); logger.info(\u0026#34;测试结果[爱奇艺兑换卡]：success\u0026#34;); } private String queryUserMobile(String uId) { return \u0026#34;15200101232\u0026#34;; } } 创建商店工厂 # 商店的⼯⼚类，在⾥⾯按照类型实现各种商品的服务。可以⾮常⼲净整洁的处理你的代码，后续新增的商品在这⾥扩展即可\npublic class StoreFactory { public ICommodity getCommodityService(Integer commodityType) { switch(commodityType) { case 1: return new CouponCommodityService(); case 2: return new GoodsCommodityService(); case 3: return new CardCommodityService(); default: } throw new RuntimeException(\u0026#34;不存在的商品服务类型\u0026#34;); } } 三、抽象工厂模式 # 抽象⼯⼚模式与⼯⼚⽅法模式虽然主要意图都是为了解决，接⼝选择问题。但在实现上，抽象⼯⼚是⼀ 个中⼼⼯⼚，创建其他⼯⼚的模式。 如： 不同系统内的回⻋换⾏此类 兼容问题\nUnix系统⾥，每⾏结尾只有 \u0026lt;换⾏\u0026gt;，即 \\n ； Windows系统⾥⾯，每⾏结尾是 \u0026lt;换⾏\u0026gt;\u0026lt;回⻋\u0026gt;，即 \\n\\r ； Mac系统⾥，每⾏结尾是 \u0026lt;回⻋\u0026gt; Redis 缓存案例 # 同时存在三套方案，需要兼容。单体Redis、集群EGM、集群IIR。里面对功能一样的方法，有可能名称不同\n1. 定义适配接口 # 主要作⽤是让所有集群的提供⽅，能在统⼀的⽅法名称下进⾏操作。也⽅⾯后续的拓展\npublic interface ICacheAdapter { String get(String key); void set(String key, String value); void set(String key, String value, long timeout, TimeUnit timeUnit); void del(String key); } 2. 实现 # // 单体 public class CacheServiceImpl implements CacheService { private RedisUtils redisUtils = new RedisUtils(); public String get(String key) { return redisUtils.get(key); } public void set(String key, String value) { redisUtils.set(key, value); } public void set(String key, String value, long timeout, TimeUnit timeUnit) { redisUtils.set(key, value, timeout, timeUnit); } public void del(String key) { redisUtils.del(key); } } // 集群EGM public class EGMCacheAdapter implements ICacheAdapter { private EGM egm = new EGM(); public String get(String key) { return egm.gain(key); } public void set(String key, String value) { egm.set(key, value); } public void set(String key, String value, long timeout, TimeUnit timeUnit) { egm.setEx(key, value, timeout, timeUnit); } public void del(String key) { egm.delete(key); } } // 集群IIR public class IIRCacheAdapter implements ICacheAdapter { private IIR iir = new IIR(); public String get(String key) { return iir.get(key); } public void set(String key, String value) { iir.set(key, value); } public void set(String key, String value, long timeout, TimeUnit timeUnit) { iir.setExpire(key, value, timeout, timeUnit); } public void del(String key) { iir.del(key); } } 3.抽象代理 # JDKProxy类 作⽤：完成代理类，同时对于使⽤哪个集群有外部通过⼊参进⾏传递\npublic static \u0026lt;T\u0026gt; T getProxy(Class\u0026lt;T\u0026gt; interfaceClass, ICacheAdapter cacheAdapter) throws Exception { InvocationHandler handler = new JDKInvocationHandler(cacheAdapter); ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); Class\u0026lt;?\u0026gt;[] classes = interfaceClass.getInterfaces(); return (T) Proxy.newProxyInstance(classLoader, new Class[]{classes[0]}, handler); } JDKInvocationHandler 作用：通过穿透进来的集群服务进⾏⽅法操作。在 invoke 中通过使⽤获取⽅法名称反射⽅式，调⽤对应的⽅法功能\npublic class JDKInvocationHandler implements InvocationHandler { private ICacheAdapter cacheAdapter; public JDKInvocationHandler(ICacheAdapter cacheAdapter) { this.cacheAdapter = cacheAdapter; } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { return ICacheAdapter.class.getMethod(method.getName(), ClassLoaderUtils.getClazzByArgs(args)).invoke(cacheAdapter, args); } } 4. 使用 # CacheService proxy_EGM = JDKProxy.getProxy(CacheServiceImpl.class, new EGMCacheAdapter()); CacheService proxy_IIR = JDKProxy.getProxy(CacheServiceImpl.class, new IIRCacheAdapter()); 四、建造者模式 # 建造者模式所完成的内容就是通过将多个简单对象通过⼀步步的组装构建出⼀个复杂对象的过程\n⼀些基本物料不会变，⽽其组合经常变化的时候 ，就可以选择这样的设计模式来构建代码 装修套装案例 # 这些套餐的后⾯是不同的商品的组合。例如；⼀级\u0026amp;⼆级吊顶、多乐⼠涂料、圣象地板、⻢可波罗地砖等等，按照不同的套餐的价格选取不同的品牌组合，最终再按照装修⾯积给出⼀个整体的报价\n1. 物料接口 # public interface Matter { String scene(); // 场景；地板、地砖、涂料、吊顶 String brand(); // 品牌 String model(); // 型号 BigDecimal price(); // 价格 String desc(); // 描述 } 2. 各类物料实现 # // 一级吊顶 public class LevelOneCeiling implements Matter { public String scene() { return \u0026#34;吊顶\u0026#34;; } public String brand() { return \u0026#34;装修公司⾃带\u0026#34;; } public String model() { return \u0026#34;⼀级顶\u0026#34;; } public BigDecimal price() { return new BigDecimal(260); } public String desc() { return \u0026#34;造型只做低⼀级，只有⼀个层次的吊顶，⼀般离顶120-150mm\u0026#34;; } } // ... 更多吊顶 // 多乐士涂料 public class DuluxCoat implements Matter { public String scene() { return \u0026#34;涂料\u0026#34;; } public String brand() { return \u0026#34;多乐⼠(Dulux)\u0026#34;; } public String model() { return \u0026#34;第⼆代\u0026#34;; } public BigDecimal price() { return new BigDecimal(719); } public String desc() { return \u0026#34;多乐⼠是阿克苏诺⻉尔旗下的著名建筑装饰油漆品牌，产品畅销于全球100个国家，每年全球有5000万户家庭使⽤多乐⼠油漆。\u0026#34;; } } // ... 更多涂料 // 德尔地板 public class DerFloor implements Matter { public String scene() { return \u0026#34;地板\u0026#34;; } public String brand() { return \u0026#34;德尔(Der)\u0026#34;; } public String model() { return \u0026#34;A+\u0026#34;; } public BigDecimal price() { return new BigDecimal(119); } public String desc() { return \u0026#34;DER德尔集团是全球领先的专业⽊地板制造商，北京2008年奥运会家装和公装地板供应商\u0026#34;; } } // ... 更多地板 // 马可波罗地砖 public class MarcoPoloTile implements Matter { public String scene() { return \u0026#34;地砖\u0026#34;; } public String brand() { return \u0026#34;⻢可波罗(MARCO POLO)\u0026#34;; } public String model() { return \u0026#34;缺省\u0026#34;; } public BigDecimal price() { return new BigDecimal(140); } public String desc() { return \u0026#34;“⻢可波罗”品牌诞⽣于1996年，作为国内最早品牌化的建陶品牌，以“⽂化陶瓷”占领市场，享有“仿古砖⾄尊”的美誉。\u0026#34;; } } // ... 更多地砖 3. 装修接口 # 装修包的实现中每⼀个⽅法都返回了 this ，也就可以⾮常⽅便的⽤于连续填充各项物料\npublic interface IMenu { IMenu appendCeiling(Matter matter); // 吊顶 IMenu appendCoat(Matter matter); // 涂料 IMenu appendFloor(Matter matter); // 地板 IMenu appendTile(Matter matter); // 地砖 String getDetail(); // 明细 } public class DecorationPackageMenu implements IMenu { private List\u0026lt;Matter\u0026gt; list = new ArrayList\u0026lt;Matter\u0026gt;(); // 装修清单 private BigDecimal price = BigDecimal.ZERO; // 装修价格 private BigDecimal area; // ⾯积 private String grade; // 装修等级；豪华欧式、轻奢⽥园、现代简约 private DecorationPackageMenu() { } public DecorationPackageMenu(Double area, String grade) { this.area = new BigDecimal(area); this.grade = grade; } public IMenu appendCeiling(Matter matter) { list.add(matter); price = price.add(area.multiply(new BigDecimal(\u0026#34;0.2\u0026#34;)).multiply(matter.price())); return this; } public IMenu appendCoat(Matter matter) { list.add(matter); price = price.add(area.multiply(new BigDecimal(\u0026#34;1.4\u0026#34;)).multiply(matter.price())); return this; } public IMenu appendFloor(Matter matter) { list.add(matter); price = price.add(area.multiply(matter.price())); return this; } public IMenu appendTile(Matter matter) { list.add(matter); price = price.add(area.multiply(matter.price())); return this; } public String getDetail() { StringBuilder detail = new StringBuilder(\u0026#34;\\r\\n-------------------- -----------------------------------\\r\\n\u0026#34; + \u0026#34;装修清单\u0026#34; + \u0026#34;\\r\\n\u0026#34; + \u0026#34;套餐等级：\u0026#34; + grade + \u0026#34;\\r\\n\u0026#34; + \u0026#34;套餐价格：\u0026#34; + price.setScale(2, BigDecimal.ROUND_HALF_UP) + \u0026#34; 元\\r\\n\u0026#34; + \u0026#34;房屋⾯积：\u0026#34; + area.doubleValue() + \u0026#34; 平⽶\\r\\n\u0026#34; + \u0026#34;材料清单：\\r\\n\u0026#34;); for (Matter matter: list) { detail.append(matter.scene()).append(\u0026#34;：\u0026#34;) .append(matter.brand()).append( \u0026#34;、\u0026#34;) .append(matter.model()).append(\u0026#34;、平⽶价格：\u0026#34;) .append(matter.price()).append(\u0026#34; 元。\\n\u0026#34;); } return detail.toString(); } } 4. 使用 # public class Builder { public IMenu levelOne(Double area) { return new DecorationPackageMenu(area, \u0026#34;豪华欧式\u0026#34;) .appendCeiling(new LevelTwoCeiling()) // 吊顶，⼆级顶 .appendCoat(new DuluxCoat()) // 涂料，多乐⼠ .appendFloor(new ShengXiangFloor()); // 地板，圣象 } public IMenu levelTwo(Double area){ return new DecorationPackageMenu(area, \u0026#34;轻奢⽥园\u0026#34;) .appendCeiling(new LevelTwoCeiling()) // 吊顶，⼆级顶 .appendCoat(new LiBangCoat()) // 涂料，⽴邦 .appendTile(new MarcoPoloTile()); // 地砖，⻢可波罗 } public IMenu levelThree(Double area){ return new DecorationPackageMenu(area, \u0026#34;现代简约\u0026#34;) .appendCeiling(new LevelOneCeiling()) // 吊顶，⼆级顶 .appendCoat(new LiBangCoat()) // 涂料，⽴邦 .appendTile(new DongPengTile()); // 地砖，东鹏 } } 五、原型模式 # 主要解决的问题就是创建重复对象，⽽这部分对象内容本身⽐较复杂，⽣成过程可能从库或者RPC接⼝中获取数据的耗时较⻓，因此采⽤克隆的⽅式节省时间\n在原型模式中所需要的⾮常重要的⼿段就是克隆，在需要⽤到克隆的类中都需要实现 Cloneable 接⼝ 试卷发放案例 # 上机考试抽题的服务，题目顺序打乱，答案打乱\n// 选择题 public class ChoiceQuestion { private String name; // 题⽬ private Map\u0026lt;String, String\u0026gt; option; // 选项；A、B、C、D private String key; // 答案；B public ChoiceQuestion() { } public ChoiceQuestion(String name, Map\u0026lt;String, String\u0026gt; option, String key) { this.name = name; this.option = option; this.key = key; } } //问答题 public class AnswerQuestion { private String name; // 问题 private String key; // 答案 public AnswerQuestion() { } public AnswerQuestion(String name, String key) { this.name = name; this.key = key; } } 克隆类 # public class QuestionBank implements Cloneable { private String candidate; // 考⽣ private String number; // 考号 private ArrayList\u0026lt;ChoiceQuestion\u0026gt; choiceQuestionList = new ArrayList\u0026lt;ChoiceQuestion\u0026gt;(); private ArrayList\u0026lt;AnswerQuestion\u0026gt; answerQuestionList = new ArrayList\u0026lt;AnswerQuestion\u0026gt;(); public QuestionBank append(ChoiceQuestion choiceQuestion) { choiceQuestionList.add(choiceQuestion); return this; } public QuestionBank append(AnswerQuestion answerQuestion) { answerQuestionList.add(answerQuestion); return this; } @Override public Object clone() throws CloneNotSupportedException { QuestionBank questionBank = (QuestionBank) super.clone(); questionBank.choiceQuestionList = (ArrayList\u0026lt;ChoiceQuestion\u0026gt;) choiceQuestionList.clone(); questionBank.answerQuestionList = (ArrayList\u0026lt;AnswerQuestion\u0026gt;) answerQuestionList.clone(); // 题⽬乱序 Collections.shuffle(questionBank.choiceQuestionList); Collections.shuffle(questionBank.answerQuestionList); // 答案乱序 ArrayList\u0026lt;ChoiceQuestion\u0026gt; choiceQuestionList = questionBank.choiceQuestionList; for (ChoiceQuestion question : choiceQuestionList) { Topic random = TopicRandomUtil.random(question.getOption(), question.getKey()); question.setOption(random.getOption()); question.setKey(random.getKey()); } return questionBank; } public void setCandidate(String candidate) { this.candidate = candidate; } public void setNumber(String number) { this.number = number; } @Override public String toString() { // ... 打印试卷 } } 使用 # public class QuestionBankController { private QuestionBank questionBank = new QuestionBank(); public QuestionBankController() { // ... 往 questionBank 添加试题 } public String createPaper(String candidate, String number) throws CloneNotSupportedException { QuestionBank questionBankClone = (QuestionBank) questionBank.clone(); questionBankClone.setCandidate(candidate); questionBankClone.setNumber(number); return questionBankClone.toString(); } } 结构型设计模式 # 一、适配器模式 # 这个模式就是用来做适配的，它将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类 可以一起工作。对于这个模式，有一个经常被拿来解释它的例子，就是 USB 转接头充当适 配器，把两种不兼容的接口，通过转接变得可以一起工作。\n类适配器 # 基于继承关系实现 // 要转化成的接口 public interface ITarget { void f1(); void f2(); void fc(); } // 一组不兼容ITarget定义的接口 public class Adaptee { public void fa() { // ... } public void fb() { // ... } public void fc() { // ... } } // 将Adaptee转为一组符合ITarget接口定义的接口 public class Adaptor extends Adaptee implements ITarget { public void f1() { super.fa(); } public void f2() { // ... 重新实现 } // fc 不实现，继承来。是跟对象适配器最大的不同点 } 对象适配器 # 基于组合关系实现 public class Adaptor implements ITarget { private Adaptee adaptee; public Adaptor(Adaptee adaptee) { this.adaptee = adaptee; } public void f1() { adaptee.fa(); // 委托给Adaptee } public void f2() { // ... 重新实现 } public void fc() { adaptee.fc(); } } 判断标准 # 标准主要有两个，一个是 Adaptee 接口的个数，另一个是 Adaptee 和 ITarget 的契合程度\n如果 Adaptee 接口并不多，那两种实现方式都可以。 如果 Adaptee 接口很多，而且 Adaptee 和 ITarget 接口定义 大部分都相同，推荐使用 类适配器，因为 Adaptor 复用父类 Adaptee 的接口，比起对象适配器的实现方 式，Adaptor 的代码量要少一些。 如果 Adaptee 接口很多，而且 Adaptee 和 ITarget 接口定义 大部分都不相同，推荐使用 对象适配器，因为组合结构相对于继承更加灵活。 使用场景 # 封装有缺陷的接口设计 统一多个类的接口设计 替换依赖的外部系统 兼容老版本接口 适配不同格式的数据 二、代理模式 # 它在不改变原始类 (或叫被代理类)代码的情况下，通过引入代理类来给原始类附加功能 静态代理：通过继承，重写方法来代理；通过实现一个接口代理\n应用：\n开发一个接口请求的缓存功能，对于某些 接口请求，如果入参相同，在设定的过期时间内，直接返回缓存结果，而不用重新进行逻辑处理。比如，针对获取用户个人信息的需求，我们可以开发两个接口，一个支持缓存，一个支持实时查询对于需要实时数据的需求，我们让其调用实时查询接口，对于不需要实时数据的需求，我们让其调用支持缓存的接口。 可以基于AOP，在切面中完成缓存功能。应用启动时，加载需要支持缓存的接口、缓存策略。当请求到来时，在切面拦截请求，若请求中带有支持缓存的字段（如http://\u0026hellip;?..\u0026amp;cached=true）就从缓存中获取数据直接返回。\n监控、统计、鉴权、限流、事务、幂等、日志 三、桥接模式 # 把事物对象和其具体行为、具体特征分离开来，使它们可以各自独立的变化 使用了组合： 四、装饰器模式 # 装饰模式是在不必改变原类和使用继承的情况下，动态地扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰来包裹真实的对象 需要在运行时动态的给一个对象增加额外的职责时候\n需要给一个现有的类增加职责，但是又不想通过继承的方式来实现的时候（应该优先使用组合而非继承），或者通过继承的方式不现实的时候（可能由于排列组合产生类爆炸的问题）\n装饰器类和原始类继承同样的父类，这样我们可以对原始类 嵌套 多个装饰器类\n装饰器类是对 功能的增强，这也是装饰器模式应用场景的一个重要特点 与代理模式区别：代理模式中，代理类附加的是跟原始类 无关的功能，而在装饰器模式中，装饰器类附加的是跟原始类 相关的增强功能\npublic interface ICoffee { public void makeCoffee(); } public class InitCoffee implements ICoffee{ InitCoffee() { } public void makeCoffee() { System.out.println(\u0026#34;原味咖啡\u0026#34;); } } public class MilkCoffee implements ICoffee{ ICoffee coffee; MilkCoffee(ICoffee coffee) { this.coffee = coffee; } public void makeCoffee() { coffee.makeCoffee(); System.out.println(\u0026#34;加牛奶\u0026#34;); } } public class SuggarCoffee implements ICoffee{ ICoffee coffee; SuggarCoffee(ICoffee coffee) { this.coffee = coffee; } public void makeCoffee() { coffee.makeCoffee(); System.out.println(\u0026#34;加糖\u0026#34;); } } public static void main(String[] args) { ICoffee coffee = new InitCoffee(); ICoffee suggarCoffee = new SuggarCoffee(coffee); ICoffee suggarMilkCoffee = new MilkCoffee(suggarCoffee); ICoffee milkCoffee = new MilkCoffee(coffee); ICoffee milkSuggarCoffee = new SuggarCoffee(milkCoffee); suggarMilkCoffee.makeCoffee(); // 原味咖啡 加糖 加牛奶 System.out.println(\u0026#34;--------\u0026#34;); milkSuggarCoffee.makeCoffee(); // 原味咖啡 加牛奶 加糖 } 五、享元模式 # 享元对象是不可变对象（一旦通过构造函数初始化完成之后，它的状态(对象的成员变量或者属性)就不会再被修改了），不能有set方法 代码结构：通过工厂模式，在工厂类通过一个 Map 缓存已经创建过的享元对象，达到复用的目的\nJava 中的应用\nInteger：Integer.IntegerCache类 缓存 -128 ～ 127 的对象 Long、Short、Byte 也类似 Integer 缓存了 -128 ～ 127 的对象 字符串常量池缓存 String // 享元类：棋子的基本属性 public class ChessPieceUnit { private int id; private String text; private Color color; public ChessPieceUnit(int id, String text, Color color) { this.id = id; this.text = text; this.color = color; } public static enum Color { RED, BLACK } // ...省略其他属性和getter方法... } // 工厂类：缓存 ChessPieceUnit public class ChessPieceUnitFactory { private static final Map\u0026lt;Integer, ChessPieceUnit\u0026gt; pieces = new HashMap\u0026lt;\u0026gt;(); static { pieces.put(1, new ChessPieceUnit(1, \u0026#34;車\u0026#34;, ChessPieceUnit.Color.BLACK)); pieces.put(2, new ChessPieceUnit(2,\u0026#34;馬\u0026#34;, ChessPieceUnit.Color.BLACK)); //...省略摆放其他棋子的代码... } public static ChessPieceUnit getChessPiece(int chessPieceId) { return pieces.get(chessPieceId); } } // 棋子类 public class ChessPiece { private ChessPieceUnit chessPieceUnit; private int positionX; private int positionY; public ChessPiece(ChessPieceUnit unit, int positionX, int positionY) { this.chessPieceUnit = unit; this.positionX = positionX; this.positionY = positionY; } // 省略getter、setter方法 } // 棋局类 public class ChessBoard { private Map\u0026lt;Integer, ChessPiece\u0026gt; chessPieces = new HashMap\u0026lt;\u0026gt;(); public ChessBoard() { init(); } private void init() { chessPieces.put(1, new ChessPiece(ChessPieceUnitFactory.getChessPiece(1), 0,0)); chessPieces.put(1, new ChessPiece(ChessPieceUnitFactory.getChessPiece(2), 1,0)); //...省略摆放其他棋子的代码... } public void move(int chessPieceId, int toPositionX, int toPositionY) { //...省略... } } 六、门面模式 # 为程序库、 框架或其他复杂类提供一个简单的接口 七、组合模式 # 将一组对象组织(Compose)成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端(代码的使用者)可以统一单个对象和组合对象的处理逻辑 与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现\n组合模式，将一组对象组织成树形结构，将单个对象和组合对象都看做树中的节点，以统一处理逻辑，并且它利用树形结构的特点，递归 地处理每个子树，依次简化代码实现 行为设计模式 # 一、责任链模式 # 将请求的发送和接收解耦，让多个接收对象都有机会处理这个请求。将这些接收对象串成一条链，并沿着这条链传递这个请求，直到链上的某个接收对象能够处理它为止 在 GoF 给出的定义中，如果处理器链上的某个处理器能够处理这个请求，那就不会继续往 下传递请求。实际上，职责链模式还有一种变体，那就是请求会被所有的处理器都处理一 遍，不存在中途终止的情况\n审批案例 # A场景审批需要通过一级审批 B场景审批需要通过一级、二级审批 C场景审批需要通过一级、二级、三级审批\npublic abstract class Auth { private Auth next; // 后继 public Auth setNext(Auth next) { this.next= next; } public final void handle() { boolean handled = doHandle(); if (next != null \u0026amp;\u0026amp; !handled) { next.handle(); } } protected abstract boolean doHandle(); } public class AuthA extends Auth { @Override protected boolean doHandle() { boolean handled = false; // ... return handled; // 可以视情况而定返回 } } // AuthB、C均类似 public class AuthLink { private Auth head = null; private Auth tail = null; public void addAuth(Auth auth) { auth.setNext(null); if (head == null) { head = auth; tail = auth; return; } tail.setNext(auth); tail = auth; } public void handle(){ if (head != null) { head.handle(); } } } / 使用举例 public static void main(String[] args) { AuthLink chain = new AuthLink(); chain.addAuth(new AuthA()); chain.addAuth(new AuthB()); chain.handle(); } 二、策略模式 # 策略模式是一种行为设计模式，它定义了一系列算法，将每个算法封装到具有共同接口的独立类中，并使它们可以相互替换 使用时，可以根据配置文件的配置决定使用什么策略（运行时动态确定） 可以在代码中指定使用何种策略（非运行时动态确定，不能发挥策略模式的优势，退化为 面向对象的多态特性 或 基于接口而非实现编程原则）\n不同身份不同折扣案例 # // 策略接口 public interface DiscountStrategy { double calculateDiscount(double originalPrice); } // 普通会员折扣策略 public class RegularMemberDiscount implements DiscountStrategy { @Override public double calculateDiscount(double originalPrice) { // 5%折扣 return originalPrice * 0.95; } } // 白银会员折扣策略 public class SilverMemberDiscount implements DiscountStrategy { @Override public double calculateDiscount(double originalPrice) { // 10%折扣 return originalPrice * 0.9; } } // 黄金会员折扣策略 public class GoldMemberDiscount implements DiscountStrategy { @Override public double calculateDiscount(double originalPrice) { // 20%折扣 return originalPrice * 0.8; } } // 上下文类 DiscountContext，用于持有具体的折扣策略对象，并根据不同的会员等级应用相应的折扣策略 public class DiscountContext { private DiscountStrategy discountStrategy; public void setDiscountStrategy(DiscountStrategy discountStrategy) { this.discountStrategy = discountStrategy; } public double applyDiscount(double originalPrice) { return discountStrategy.calculateDiscount(originalPrice); } } // 或者使用工厂对客户端代码屏蔽创建细节： // 对于无状态的策略类，可以这样： // 相反，如果策略类是有状态的，根据业务场景的需要，我们希望每次从工厂方法中，获得的都是新创建的策略对象，而不是缓存好可共享的策略对象，那我们就需要使用ifelse 或 switch 创建策略 public class DiscountStrategyFactory { private static final Map\u0026lt;OrderType, DiscountStrategy\u0026gt; strategies = new HashMap\u0026lt;\u0026gt;(); static { strategies.put(OrderType.NORMAL, new RegularMemberDiscount()); strategies.put(OrderType.SILVER, new SilverMemberDiscount()); strategies.put(OrderType.GOLD, new GoldMemberDiscount()); } public static DiscountStrategy getDiscountStrategy(OrderType type) { if (type == null) { throw new IllegalArgumentExecption(\u0026#34;类型必须非空\u0026#34;); } if (!strategies.contains(type)) { throw new IllegalArgumentExecption(\u0026#34;类型不存在\u0026#34;); } return strategies.get(type); } } // 策略的使用 public class OrderService { public double discount(Order order) { OrderType type = order.getType(); DiscountStrategy discountStrategy = DiscountStrategyFactory.getDiscountStrategy(type); return discountStrategy.calculateDiscount(order.getPrice()); } } 三、观察者模式/发布订阅模式 # 在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知\nRedis 发布与订阅 Spring 事件机制 public interface Subject { void registerObserver(Observer observer); // 订阅 void removeObserver(Observer observer); // 取消订阅 void notifyObservers(Message message); // 通知 } public interface Observer { void update(Message message); } public class ConcreteSubject implements Subject { private List\u0026lt;Observer\u0026gt; observers = new ArrayList\u0026lt;Observer\u0026gt;(); @Override public void registerObserver(Observer observer) { observers.add(observer); } @Override public void removeObserver(Observer observer) { observers.remove(observer); } @Override public void notifyObservers(Message message) { for (Observer observer : observers) { observer.update(message); } } } public class ConcreteObserverOne implements Observer { @Override public void update(Message message) { //TODO: 获取消息通知，执行自己的逻辑... System.out.println(\u0026#34;ConcreteObserverOne is notified.\u0026#34;); } } public class ConcreteObserverTwo implements Observer { @Override public void update(Message message) { //TODO: 获取消息通知，执行自己的逻辑... System.out.println(\u0026#34;ConcreteObserverTwo is notified.\u0026#34;); } } public class Demo { public static void main(String[] args) { ConcreteSubject subject = new ConcreteSubject(); subject.registerObserver(new ConcreteObserverOne()); subject.registerObserver(new ConcreteObserverTwo()); subject.notifyObservers(new Message()); } } 四、命令模式 # 命令模式将请求（命令）封装为一个对象，这样可以使用不同的请求参数化其他对象（将不同请求依赖注入到其他对象），并且能够支持请求（命令）的排队执行、记录日志、撤销等（附加控制）功能。 命令模式用的最核心的实现手段，是将函数封装成对象。我们知道，C 语言支持函数指针，我们可以把函数当作变量传递来传递去。但是，在大部分编程语言中，函数没法儿作为参数传递给其他函数，也没法儿赋值给变量。借助命令模式，我们可以将函数封装成对象\n当我们把函数封装成对象之后，对象就可以存储下来，方便控制执行。所以，命令模式的主要作用和应用场景，是用来控制命令的执行，比如，异步、延迟、排队执行命令、撤销重做命令、存储命令、给命令记录日志等等，这才是命令模式能发挥独一无二作用的地方。\npublic interface Command { ... // 其他属性、构造函数等 public execute(); } public class Command1 implements Command { public execute(){ // 具体逻辑 } } public class GameApplication { private static final int MAX_HANDLED_REQ_COUNT_PER_LOOP = 100; private Queue\u0026lt;Command\u0026gt; queue = new LinkedList\u0026lt;\u0026gt;(); public void mainloop() { while (true) { List\u0026lt;Request\u0026gt; requests = new ArrayList\u0026lt;\u0026gt;(); //省略从epoll或者select中获取数据，并封装成Request的逻辑， //注意设置超时时间，如果很长时间没有接收到请求，就继续下面的逻辑处理。 for (Request request : requests) { Event event = request.getEvent(); Command command = null; if (event.equals(Event.GOT_DIAMOND)) { command = new GotDiamondCommand(/*数据*/); } else if (event.equals(Event.GOT_STAR)) { command = new GotStartCommand(/*数据*/); } else if (event.equals(Event.HIT_OBSTACLE)) { command = new HitObstacleCommand(/*数据*/); } else if (event.equals(Event.ARCHIVE)) { command = new ArchiveCommand(/*数据*/); } // ...一堆else if... queue.add(command); } int handledCount = 0; while (handledCount \u0026lt; MAX_HANDLED_REQ_COUNT_PER_LOOP) { if (queue.isEmpty()) { break; } Command command = queue.poll(); command.execute(); } } } } 五、模版模式 # 模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤 final 防止子类重写模版方法 protected 强迫子类重写方法 public abstract class AbstractClass { public final void templateMethod() { // ... method1(); // ... method2(); // ... method3(); } protected abstract void method1(); protected abstract void method2(); private void method3(); } 作用：\n复用 java.io.InputStream#read(byte[], int, int) 就是一个模板方法，定义了读取数据的整个流程。并且暴露了一个可以由子类来定制的抽象方法java.io.InputStream#read() java.util.AbstractList#addAll 可以看作模版方法，其中的add需要子类重写 拓展：框架的拓展性 javax.servlet.http.HttpServlet#service(javax.servlet.http.HttpServletRequest, javax.servlet.http.HttpServletResponse) 是一个模板方法，实现了整个HTTP请求的执行流程，doGet() 、doPost()是模版中可以由子类定制的部分。相当于Servlet框架提供了一个拓展点，让框架用户在不用修改框架源码的情况下，将业务代码通过拓展点镶嵌到框架中执行 junit.framework.TestCase#runBare 的setUp() tearDwon()等也是拓展点 六、状态模式 # 状态模式一般用来实现状态机，而状态机常用在游戏、工作流引擎等系统开发中。不过，状态机的实现方式有多种，除了状态模式，比较常用的还有分支逻辑法和查表法 七、迭代器模式 # 解耦容器代码和遍历代码 遍历集合一般有三种方式:for 循环、foreach 循环、迭代器遍历。后两种本质上属于一种，都可以看作迭代器遍历。相对于 for 循环遍历，利用迭代器来遍历有下面三个优势:\n迭代器模式封装集合内部的复杂数据结构，开发者不需要了解如何遍历，直接使用容器提供的迭代器即可; 迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一; 迭代器模式让添加新的遍历算法更加容易，更符合开闭原则。除此之外，因为迭代器都实现自相同的接口，在开发中，基于接口而非实现编程，替换迭代器也变得更加容易。 迭代器也有 fail-fast 机制： ArrayList 中定义一个成员变量 modCount，记录集合被修改的次数，集合每调用一次增加或删除元素的函数，就会给 modCount 加 1。当通过调用集合上的 iterator() 函数来创建迭代器的时候，我们把 modCount 值传递给迭代器的 expectedModCount 成员变量，之后每次调用迭代器上的 hasNext()、next()、currentItem() 函数，我们都会检查集合上的 modCount 是否等于 expectedModCount，也就是看，在创建完迭代器之后，modCount 是否改变过\n如果两个值不相同，那就说明集合存储的元素已经改变了，要么增加了元素，要么删除了元素，之前创建的迭代器已经不能正确运行了，再继续使用就会产生不可预期的结果，所以我们选择 fail-fast 解决方式，抛出运行时异常，结束掉程序，让程序员尽快修复这个因为不正确使用迭代器而产生的 bug\n八、访问者模式 # 它难理解、难实现，应用它会导致代码的可读性、可维护性变差，所以，访问者模式在实际的软件开发中很少被用到，在没有特别必要的情况下，建议你不要使用访问者模式 九、备忘录模式 # 在不违背封装原则的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便之后恢复对象为先前的状态 十一、中介模式 # 十、解释器模式 # ","externalUrl":null,"permalink":"/docs/%E5%85%B6%E4%BB%96/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/","section":"Docs","summary":"创建性设计模式 # 一、单例模式 # 一个类只允许创建一个对象（或者","title":"设计模式","type":"docs"},{"content":" 零、训练 # 1.模型的断点续训练 # 在训练过程中，可能由于某种意外原因如断点等导致训练终止，这时需要重新开始训练。断点续练是在训练过程中每隔一定次数的 epoch 就保存模型的参数和优化器的参数，这样如果意外终止训练了，下次就可以重新加载最新的模型参数和优化器的参数，在这个基础上继续训练。\n下面的代码中，每隔 5 个 epoch 就保存一次，保存的是一个 dict，包括模型参数、优化器的参数、epoch。然后在 epoch 大于 5 时，就break模拟训练意外终止。关键代码如下：\nif (epoch+1) % checkpoint_interval == 0: checkpoint = {\u0026#34;model_state_dict\u0026#34;: net.state_dict(), \u0026#34;optimizer_state_dict\u0026#34;: optimizer.state_dict(), \u0026#34;epoch\u0026#34;: epoch} path_checkpoint = \u0026#34;./checkpoint_{}_epoch.pkl\u0026#34;.format(epoch) torch.save(checkpoint, path_checkpoint) 断点续训练的恢复代码如下：\npath_checkpoint = \u0026#34;./checkpoint_4_epoch.pkl\u0026#34; checkpoint = torch.load(path_checkpoint) net.load_state_dict(checkpoint[\u0026#39;model_state_dict\u0026#39;]) optimizer.load_state_dict(checkpoint[\u0026#39;optimizer_state_dict\u0026#39;]) start_epoch = checkpoint[\u0026#39;epoch\u0026#39;] scheduler.last_epoch = start_epoch 需要注意的是，还要设置scheduler.last_epoch参数为保存的 epoch。模型训练的起始 epoch 也要修改为保存的 epoch。\n2.模型 Finetune # Finetune 步骤如下：\n获取预训练模型的参数\n使用load_state_dict()把参数加载到模型中\n修改输出层\n固定 feature extractor 的参数。这部分通常有 2 种做法：\n固定卷积层的预训练参数。可以设置requires_grad=False或者lr=0\n可以通过params_group给 feature extractor 设置一个较小的学习率\n冻结卷积层 设置 requires_grad=False 这里先冻结所有参数，然后再替换全连接层，相当于冻结了卷积层的参数：\nfor param in resnet18_ft.parameters(): param.requires_grad = False # 首先拿到 fc 层的输入个数 num_ftrs = resnet18_ft.fc.in_features # 然后构造新的 fc 层替换原来的 fc 层 resnet18_ft.fc = nn.Linear(num_ftrs, classes) 3.GPU # 1.使用 GPU 的 tips # PyTorch 模型使用 GPU，可以分为 3 步：\n首先获取 device：device = torch.device(\u0026quot;cuda\u0026quot; if torch.cuda.is_available() else \u0026quot;cpu\u0026quot;) 把模型加载到 device：model.to(device) 在 data_loader 取数据的循环中，把每个 mini-batch 的数据和 label 加载到 device：inputs, labels = inputs.to(device), labels.to(device) 2.数据并行 # torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n功能：可以把数据平均分发到各个 GPU 上，每个 GPU 实际的数据量为 batch_size/GPU 数量，实现并行计算。\n主要参数：\nmodule：需要包装分发的模型 device_ids：可分发的 GPU，默认分发到所有可见可用的 GPU output_device：结果输出设备 需要注意的是：使用 DataParallel 时，device 要指定某个 GPU 为 主 GPU(放到device_ids第一个)，否则会报错：\nRuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2\n这是因为，使用多 GPU 需要有一个主 GPU，来把每个 batch 的数据分发到每个 GPU，并从每个 GPU 收集计算好的结果。如果不指定主 GPU，那么数据就直接分发到每个 GPU，会造成有些数据在某个 GPU，而另一部分数据在其他 GPU，计算出错\ndevice = torch.device(\u0026#34;cuda:0\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) inputs, labels = inputs.to(device), labels.to(device) net.to(device) # training for epoch in range(1): outputs = net(inputs) print(\u0026#34;model outputs.size: {}\u0026#34;.format(outputs.size())) print(\u0026#34;CUDA_VISIBLE_DEVICES :{}\u0026#34;.format(os.environ[\u0026#34;CUDA_VISIBLE_DEVICES\u0026#34;])) print(\u0026#34;device_count :{}\u0026#34;.format(torch.cuda.device_count())) nvidia-smi 命令查看可以 GPU 的利用率，但不能动态刷新显示。如果你想每隔一秒刷新显示 GPU 信息\n可以使用watch -n 1 nvidia-smi\n设置 Dataloader的两个参数：\nnum_workers：默认只使用一个 CPU 读取和处理数据。可以设置为 4、8、16 等参数。但线程数并不是越大越好。因为，多核处理需要把数据分发到每个 CPU，处理完成后需要从多个 CPU 收集数据，这个过程也是需要时间的。如果设置num_workers过大，分发和收集数据等操作占用了太多时间，反而会降低效率。 pin_memory：如果内存较大，建议设置为 True。 设置为 True，表示把数据直接映射到 GPU 的相关内存块上，省掉了一点数据传输时间。 设置为 False，表示从 CPU 传入到缓存 RAM 里面，再给传输到 GPU 上 一、卷积神经网络 # 1.LeNet 、AlexNet架构 # 2.VGG # 3.NIN # 4.GoogLeNet # Inception块：\nGoogLeNet网络：\n5.ResNet # 残差块：\nResNet块：\n左右区别： $$ 一种是应用ReLU非线性函数之前，将输入添加到输出。 另一种通过1 \\times 1卷积调整通道和分辨率。 $$ ResNet18:\n二、循环神经网络 # 0.文本预处理 # 1. 读取数据集 # def read_dataset(): \u0026#34;\u0026#34;\u0026#34;将数据集加载到文本行的列表中\u0026#34;\u0026#34;\u0026#34; with open(\u0026#39;time_machine.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: lines = f.readlines() # 正则表达式 [^A-Za-z]+ 将非字母字符替换为空格,strip()去除首尾的空白字符 return [re.sub(\u0026#39;[^A-Za-z]+\u0026#39;, \u0026#39; \u0026#39;, line).strip().lower() for line in lines] 2. 将文本拆分为单词或字符词元： # lines = read_dataset() def tokenize(lines, token=\u0026#39;word\u0026#39;): \u0026#34;\u0026#34;\u0026#34;将文本行拆分为单词或字符词元\u0026#34;\u0026#34;\u0026#34; if token == \u0026#39;word\u0026#39;: return [line.split() for line in lines] elif token == \u0026#39;char\u0026#39;: return [list(line) for line in lines] else: print(\u0026#39;错误：未知词元类型：\u0026#39; + token) tokens = tokenize(lines) 3. 构建词表 # 词元的类型是字符串，而模型需要的输入是数字，因此这种类型不方便模型使用。构建一个字典，通常也叫做词表（vocabulary），用来将字符串类型的词元映射到从 0 开始的数字索引中。\n先将训练集中的所有文档合并在一起，对它们的唯一词元进行统计，得到的统计结果称之为语料（corpus）。\n然后根据每个唯一词元的出现频率，为其分配一个数字索引。很少出现的词元通常被移除，这可以降低复杂性。\n另外，语料库中不存在或已删除的任何词元都将映射到一个特定的未知词元“\u0026lt;unk\u0026gt;”。\n可以选择增加一个列表，用于保存那些被保留的词元，\n例如：填充词元（“\u0026lt;pad\u0026gt;”）；\n序列开始词元（“\u0026lt;bos\u0026gt;”）；\n序列结束词元（“\u0026lt;eos\u0026gt;”）。\ndef count_corpus(tokens): \u0026#34;\u0026#34;\u0026#34;统计词元的频率\u0026#34;\u0026#34;\u0026#34; # 这里的tokens是1D列表或2D列表 if len(tokens) == 0 or isinstance(tokens[0], list): # 将词元列表展平成一个列表 tokens = [token for line in tokens for token in line] return collections.Counter(tokens) # 快速计算tokens中每个元素出现的次数，并将结果存储在一个字典中 # 其中键是元素，值是该元素出现的次数 class Vocab: \u0026#34;\u0026#34;\u0026#34;文本词表\u0026#34;\u0026#34;\u0026#34; def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): if tokens is None: tokens = [] if reserved_tokens is None: reserved_tokens = [] # 按出现频率排序 counter = count_corpus(tokens) self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\t# 按元素出现次数（键值对中的第二个元素，即x[1]）进行排序 reverse=True) # 未知词元的索引为0 self.idx_to_token = [\u0026#39;\u0026lt;unk\u0026gt;\u0026#39;] + reserved_tokens # 词表初始化 self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)} for token, freq in self._token_freqs: if freq \u0026lt; min_freq: break # 加入词表 if token not in self.token_to_idx: self.idx_to_token.append(token) self.token_to_idx[token] = len(self.idx_to_token) - 1 def __len__(self): return len(self.idx_to_token) def __getitem__(self, tokens): if not isinstance(tokens, (list, tuple)): # 传入的为单个词 # 若存在，返回对应的索引，若不存在，返回unk索引（0） return self.token_to_idx.get(tokens, self.unk) # 多个，递归 return [self.__getitem__(token) for token in tokens] def to_tokens(self, indices): # 获取索引对应的词元 if not isinstance(indices, (list, tuple)): # 单个词元 return self.idx_to_token[indices] # 递归 return [self.idx_to_token[index] for index in indices] @property def unk(self): # 未知词元的索引为0 return 0 @property def token_freqs(self): return self._token_freqs vocab = Vocab(tokens) # 将文本行转换为一个数字索引列表 for i in [0, 10]: print(\u0026#39;文本:\u0026#39;, tokens[i]) print(\u0026#39;索引:\u0026#39;, vocab[tokens[i]]) 4. 整合 # def load_corpus(max_tokens=-1): \u0026#34;\u0026#34;\u0026#34;返回数据集的词元索引列表和词表\u0026#34;\u0026#34;\u0026#34; lines = read_dataset() tokens = tokenize(lines, \u0026#39;char\u0026#39;) vocab = Vocab(tokens) # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，还可能是一个单词 # 因此返回的corpus仅处理为单个列表，而不是使用多词元列表构成的一个列表 corpus = [vocab[token] for line in tokens for token in line] if max_tokens \u0026gt; 0: corpus = corpus[:max_tokens] return corpus, vocab corpus, vocab = load_corpus() len(corpus), len(vocab) 1. 文本训练数据集 # 输出 Y 相较于输入 X 往右移一个词元\n1.随机采样 # def seq_data_iter_random(corpus, batch_size, num_steps): \u0026#34;\u0026#34;\u0026#34;使用随机抽样生成一个小批量子序列\u0026#34;\u0026#34;\u0026#34; # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1 corpus = corpus[random.randint(0, num_steps - 1):] # 减去1，是因为我们需要考虑标签。——子序列个数 num_subseqs = (len(corpus) - 1) // num_steps # 长度为num_steps的子序列的起始索引 initial_indices = list(range(0, num_subseqs * num_steps, num_steps)) # 在随机抽样的迭代过程中， # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻 random.shuffle(initial_indices) def data(pos): # 返回从pos位置开始的长度为num_steps的序列 return corpus[pos: pos + num_steps] # ——批次数 num_batches = num_subseqs // batch_size\tfor i in range(0, batch_size * num_batches, batch_size): # 在这里，initial_indices包含子序列的随机起始索引 initial_indices_per_batch = initial_indices[i: i + batch_size] X = [data(j) for j in initial_indices_per_batch] Y = [data(j + 1) for j in initial_indices_per_batch] yield torch.tensor(X), torch.tensor(Y) ########################## my_seq = list(range(35)) for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5): print(\u0026#39;X: \u0026#39;, X, \u0026#39;\\nY:\u0026#39;, Y) 2.顺序分区 # def seq_data_iter_sequential(corpus, batch_size, num_steps): \u0026#34;\u0026#34;\u0026#34;使用顺序分区生成一个小批量子序列\u0026#34;\u0026#34;\u0026#34; # 从随机偏移量开始划分序列 offset = random.randint(0, num_steps) num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size Xs = torch.tensor(corpus[offset: offset + num_tokens]) Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens]) Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) num_batches = Xs.shape[1] // num_steps for i in range(0, num_steps * num_batches, num_steps): X = Xs[:, i: i + num_steps] Y = Ys[:, i: i + num_steps] yield X, Y ########################## my_seq = list(range(35)) for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5): print(\u0026#39;X: \u0026#39;, X, \u0026#39;\\nY:\u0026#39;, Y) 3. 包装为一个类 # class SeqDataLoader: \u0026#34;\u0026#34;\u0026#34;加载序列数据的迭代器\u0026#34;\u0026#34;\u0026#34; def __init__(self, batch_size, num_steps, use_random_iter, max_tokens): if use_random_iter: self.data_iter_fn = seq_data_iter_random else: self.data_iter_fn = seq_data_iter_sequential self.corpus, self.vocab = load_corpus(max_tokens) self.batch_size, self.num_steps = batch_size, num_steps def __iter__(self): return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps) def load_data(batch_size, num_steps, use_random_iter=False, max_tokens=10000): \u0026#34;\u0026#34;\u0026#34;返回数据集的迭代器和词表\u0026#34;\u0026#34;\u0026#34; data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens) return data_iter, data_iter.vocab 3. 困惑度 # 语言模型应该能让我们更准确地预测下一个词元。\n因此，它应该允许我们在压缩序列时花费更少的比特。\n所以我们可以通过一个序列中所有的 n 个词元的交叉熵损失的平均值来衡量： $$ \\frac{1}{n} \\sum_{t=1}^n -\\log P(x_t \\mid x_{t-1}, \\ldots, x_1) $$ 其中 P 由语言模型给出，x_t 是在时间步 t 从该序列中观察到的实际词元。这使得不同长度的文档的性能具有了可比性。\n由于历史原因，自然语言处理的科学家更喜欢使用一个叫做困惑度（perplexity）的量。\n简而言之，它是上面的指数： $$ \\exp\\left(-\\frac{1}{n} \\sum_{t=1}^n \\log P(x_t \\mid x_{t-1}, \\ldots, x_1)\\right). $$ 困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。\n* 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。\n* 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。\n* 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。 在这种情况下，困惑度等于词表中唯一词元的数量。\n使用困惑度来评估模型。\n4.RNN # $$ 时间步t的隐藏变量：\\mathbf{H}t = \\phi(\\mathbf{X}t \\mathbf{W}{xh} + \\mathbf{H}{t-1} \\mathbf{W}_{hh} + \\mathbf{b}_h).\\ 时间步t的输出：\\mathbf{O}t = \\mathbf{H}t \\mathbf{W}{hq} + \\mathbf{b}q.\\ 循环神经网络的参数包括隐藏层的权重 \\mathbf{W}{xh} \\in \\mathbb{R}^{d \\times h}, \\mathbf{W}{hh} \\in \\mathbb{R}^{h \\times h}和偏置\\mathbf{b}h \\in \\mathbb{R}^{1 \\times h}，\\ 以及输出层的权重\\mathbf{W}{hq} \\in \\mathbb{R}^{h \\times q} 和偏置\\mathbf{b}_q \\in \\mathbb{R}^{1 \\times q}\\ d为输入的独热编码的长度，h为隐藏层的长度，q为输出的长度\\ 对于\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d},输出\\mathbf{O}_t\\in \\mathbb{R}^{n \\times q} $$ 训练过程：\nbatch_size, num_steps = 32, 35 train_iter, vocab = load_data(batch_size, num_steps) num_hiddens = 256 rnn_layer = nn.RNN(len(vocab), num_hiddens) # 初始化隐藏状态 state = torch.zeros((1, batch_size, num_hiddens)) state.shape\t# torch.Size([1, 32, 256])（隐藏层数，批量大小，隐藏单元数） #通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出 #需要强调的是rnn_layer的“输出”（Y）不涉及输出层的计算： #它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。 X = torch.rand(size=(num_steps, batch_size, len(vocab))) Y, state_new = rnn_layer(X, state) # Y是隐状态 Y.shape, state_new.shape # (torch.Size([35, 32, 256]), torch.Size([1, 32, 256])) class RNNModel(nn.Module): \u0026#34;\u0026#34;\u0026#34;循环神经网络模型\u0026#34;\u0026#34;\u0026#34; def __init__(self, rnn_layer, vocab_size, **kwargs): super(RNNModel, self).__init__(**kwargs) self.rnn = rnn_layer self.vocab_size = vocab_size self.num_hiddens = self.rnn.hidden_size # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1 if not self.rnn.bidirectional: self.num_directions = 1 self.linear = nn.Linear(self.num_hiddens, self.vocab_size) else: self.num_directions = 2 self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size) def forward(self, inputs, state): X = F.one_hot(inputs.T.long(), self.vocab_size) X = X.to(torch.float32) Y, state = self.rnn(X, state) # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数) # 它的输出形状是(时间步数*批量大小,词表大小)。 output = self.linear(Y.reshape((-1, Y.shape[-1]))) return output, state def begin_state(self, device, batch_size=1): if not isinstance(self.rnn, nn.LSTM): # nn.GRU以张量作为隐状态 return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device) else: # nn.LSTM以元组作为隐状态 return (torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device), torch.zeros(( self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)) # 训练与预测 device = d2l.try_gpu() net = RNNModel(rnn_layer, vocab_size=len(vocab)) net = net.to(device) d2l.predict_ch8(\u0026#39;time traveller\u0026#39;, 10, net, vocab, device) # 训练 def train_epoch(net, train_iter, loss, updater, device, use_random_iter): \u0026#34;\u0026#34;\u0026#34;训练网络一个迭代周期（定义见第8章）\u0026#34;\u0026#34;\u0026#34; state, timer = None, d2l.Timer() metric = d2l.Accumulator(2) # 训练损失之和,词元数量 for X, Y in train_iter: if state is None or use_random_iter: # 在第一次迭代或使用随机抽样时初始化state state = net.begin_state(batch_size=X.shape[0], device=device) else: if isinstance(net, nn.Module) and not isinstance(state, tuple): # state对于nn.GRU是个张量 state.detach_() else: # state对于nn.LSTM或对于我们从零开始实现的模型是个张量 for s in state: s.detach_() y = Y.T.reshape(-1) X, y = X.to(device), y.to(device) y_hat, state = net(X, state) l = loss(y_hat, y.long()).mean() if isinstance(updater, torch.optim.Optimizer): updater.zero_grad() l.backward() grad_clipping(net, 1) updater.step() else: l.backward() grad_clipping(net, 1) # 因为已经调用了mean函数 updater(batch_size=1) metric.add(l * y.numel(), y.numel()) return math.exp(metric[0] / metric[1]), metric[1] / timer.stop() # 预测 def predict(prefix, num_preds, net, vocab, device): \u0026#34;\u0026#34;\u0026#34;在prefix后面生成新字符\u0026#34;\u0026#34;\u0026#34; state = net.begin_state(batch_size=1, device=device) outputs = [vocab[prefix[0]]] get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) for y in prefix[1:]: # 预热期 _, state = net(get_input(), state) outputs.append(vocab[y]) for _ in range(num_preds): # 预测num_preds步 y, state = net(get_input(), state) outputs.append(int(y.argmax(dim=1).reshape(1))) return \u0026#39;\u0026#39;.join([vocab.idx_to_token[i] for i in outputs]) def train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False): \u0026#34;\u0026#34;\u0026#34;训练模型\u0026#34;\u0026#34;\u0026#34; loss = nn.CrossEntropyLoss() animator = d2l.Animator(xlabel=\u0026#39;epoch\u0026#39;, ylabel=\u0026#39;perplexity\u0026#39;, legend=[\u0026#39;train\u0026#39;], xlim=[10, num_epochs]) # 初始化 if isinstance(net, nn.Module): updater = torch.optim.SGD(net.parameters(), lr) else: updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size) predict = lambda prefix: predict(prefix, 50, net, vocab, device) # 训练和预测 for epoch in range(num_epochs): ppl, speed = train_epoch( net, train_iter, loss, updater, device, use_random_iter) if (epoch + 1) % 10 == 0: print(predict(\u0026#39;time traveller\u0026#39;)) animator.add(epoch + 1, [ppl]) print(f\u0026#39;困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}\u0026#39;) print(predict(\u0026#39;time traveller\u0026#39;)) print(predict(\u0026#39;traveller\u0026#39;)) num_epochs, lr = 500, 1 train(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu()) 5.门控循环单元GRU # 重置门和更新门\n$$ 对于给定的时间步t，假设输入是一个小批量 \\mathbf{X}t \\in \\mathbb{R}^{n \\times d}（样本个数n，输入个数d），\\ 上一个时间步的隐状态是\\mathbf{H}{t-1} \\in \\mathbb{R}^{n \\times h}（隐藏单元个数h）。\\ 那么，重置门\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}和 更新门\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}的计算如下所示：\\\n\\begin{aligned} \\mathbf{R}t = \\sigma(\\mathbf{X}t \\mathbf{W}{xr} + \\mathbf{H}{t-1} \\mathbf{W}{hr} + \\mathbf{b}r),\\ \\mathbf{Z}t = \\sigma(\\mathbf{X}t \\mathbf{W}{xz} + \\mathbf{H}{t-1} \\mathbf{W}{hz} + \\mathbf{b}z), \\end{aligned} \\ 其中\\mathbf{W}{xr}, \\mathbf{W}{xz} \\in \\mathbb{R}^{d \\times h} 和\\mathbf{W}{hr}, \\mathbf{W}{hz} \\in \\mathbb{R}^{h \\times h}是权重参数， \\mathbf{b}_r, \\mathbf{b}_z \\in \\mathbb{R}^{1 \\times h}是偏置参数。\\ 请注意，在求和过程中会触发广播机制，使用sigmoid函数将输入值转换到区间(0, 1)。 $$ 候选隐状态\n$$ 将重置门\\mathbf{R}_t与常规隐状态更新机制集成，得到在时间步t的候选隐状态（candidate hidden state）\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}。\\\n\\tilde{\\mathbf{H}}t = \\tanh(\\mathbf{X}t \\mathbf{W}{xh} + \\left(\\mathbf{R}t \\odot \\mathbf{H}{t-1}\\right) \\mathbf{W}{hh} + \\mathbf{b}_h),\\\n其中\\mathbf{W}{xh} \\in \\mathbb{R}^{d \\times h} 和\\mathbf{W}{hh} \\in \\mathbb{R}^{h \\times h}是权重参数， \\mathbf{b}_h \\in \\mathbb{R}^{1 \\times h}是偏置项， 符号\\odot是Hadamard积（按元素乘积）运算符。\\ 使用tanh非线性激活函数来确保候选隐状态中的值保持在区间(-1, 1)中。\\\n\\mathbf{R}t和\\mathbf{H}{t-1}的元素相乘可以减少以往状态的影响。每当重置门\\mathbf{R}_t中的项接近1时，恢复一个普通的循环神经网络。\\ 对于重置门\\mathbf{R}_t中所有接近0的项，候选隐状态是以\\mathbf{X}_t作为输入的多层感知机的结果。因此，任何预先存在的隐状态都会被重置为默认值。 $$ 隐状态\n$$ 上述的计算结果只是候选隐状态，我们仍然需要结合更新门\\mathbf{Z}_t的效果。\\ 这一步确定新的隐状态\\mathbf{H}t \\in \\mathbb{R}^{n \\times h}在多大程度上来自旧的状态\\mathbf{H}{t-1}和 新的候选状态\\tilde{\\mathbf{H}}_t。\\ 更新门\\mathbf{Z}t仅需要在\\mathbf{H}{t-1}和\\tilde{\\mathbf{H}}_t之间进行按元素的凸组合就可以实现这个目标。\\ 这就得出了门控循环单元的最终更新公式：\\\n\\mathbf{H}_t = \\mathbf{Z}t \\odot \\mathbf{H}{t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.\\\n每当更新门\\mathbf{Z}_t接近1时，模型就倾向只保留旧状态。此时，来自\\mathbf{X}_t的信息基本上被忽略，从而有效地跳过了依赖链条中的时间步t。\\ 相反，当\\mathbf{Z}_t接近0时，新的隐状态\\mathbf{H}_t就会接近候选隐状态\\tilde{\\mathbf{H}}_t。\\ 这些设计可以帮助我们处理循环神经网络中的梯度消失问题，并更好地捕获时间步距离很长的序列的依赖关系。\\ 例如，如果整个子序列的所有时间步的更新门都接近于1，在序列起始时间步的旧隐状态都将很容易保留并传递到序列结束。 $$ 总之，门控循环单元具有以下两个显著特征：\n重置门有助于捕获序列中的短期依赖关系；\n更新门有助于捕获序列中的长期依赖关系。\n6.LSTM # 输入门、忘记门和输出门\n$$ \\begin{flalign} \u0026amp;假设有h个隐藏单元，批量大小为n，输入数为d。\\ \u0026amp;因此，输入为\\mathbf{X}t \\in \\mathbb{R}^{n \\times d}， 前一时间步的隐状态为\\mathbf{H}{t-1} \\in \\mathbb{R}^{n \\times h}。\\ \u0026amp;相应地，时间步t的门被定义如下：\\ \u0026amp;输入门是\\mathbf{I}_t \\in \\mathbb{R}^{n \\times h}，\\ \u0026amp;遗忘门是\\mathbf{F}_t \\in \\mathbb{R}^{n \\times h}，\\ \u0026amp;输出门是\\mathbf{O}_t \\in \\mathbb{R}^{n \\times h}。\\\n\u0026amp;它们的计算方法如下：\\ \\mathbf{I}t \u0026amp;= \\sigma(\\mathbf{X}t \\mathbf{W}{xi} + \\mathbf{H}{t-1} \\mathbf{W}_{hi} + \\mathbf{b}i),\\ \\mathbf{F}t \u0026amp;= \\sigma(\\mathbf{X}t \\mathbf{W}{xf} + \\mathbf{H}{t-1} \\mathbf{W}{hf} + \\mathbf{b}f),\\ \\mathbf{O}t \u0026amp;= \\sigma(\\mathbf{X}t \\mathbf{W}{xo} + \\mathbf{H}{t-1} \\mathbf{W}{ho} + \\mathbf{b}_o),\\\n\u0026amp;其中\\mathbf{W}{xi}, \\mathbf{W}{xf}, \\mathbf{W}{xo} \\in \\mathbb{R}^{d \\times h} 和\\mathbf{W}{hi}, \\mathbf{W}{hf}, \\mathbf{W}{ho} \\in \\mathbb{R}^{h \\times h}是权重参数， \\mathbf{b}_i, \\mathbf{b}_f, \\mathbf{b}_o \\in \\mathbb{R}^{1 \\times h}是偏置参数。 \\end{flalign} $$ 候选记忆元\n$$ \\tilde{\\mathbf{C}}_t \\in \\mathbb{R}^{n \\times h} 它的计算与上面描述的三个门的计算类似， 但是使用\\tanh函数作为激活函数，函数的值范围为(-1, 1)。\\ 下面导出在时间步t处的方程：\\\n\\tilde{\\mathbf{C}}t = \\text{tanh}(\\mathbf{X}t \\mathbf{W}{xc} + \\mathbf{H}{t-1} \\mathbf{W}_{hc} + \\mathbf{b}_c),\\\n其中\\mathbf{W}{xc} \\in \\mathbb{R}^{d \\times h}和\\mathbf{W}{hc} \\in \\mathbb{R}^{h \\times h}是权重参数，\\mathbf{b}_c \\in \\mathbb{R}^{1 \\times h}是偏置参数。 $$\n记忆元\n$$ 在门控循环单元中，有一种机制来控制输入和遗忘（或跳过）。\\ 类似地，在长短期记忆网络中，也有两个门用于这样的目的：\\ 输入门\\mathbf{I}_t控制采用多少来自\\tilde{\\mathbf{C}}_t的新数据，\\ 而遗忘门\\mathbf{F}t控制保留多少过去的记忆元\\mathbf{C}{t-1} \\in \\mathbb{R}^{n \\times h}的内容。\\ 使用按元素乘法，得出：\\\n\\mathbf{C}_t = \\mathbf{F}t \\odot \\mathbf{C}{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t.\\\n如果遗忘门始终为1且输入门始终为0，则过去的记忆元\\mathbf{C}_{t-1}将随时间被保存并传递到当前时间步。\\ 引入这种设计是为了缓解梯度消失问题，并更好地捕获序列中的长距离依赖关系。 $$ 隐状态\n$$ 最后，我们需要定义如何计算隐状态\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}， 这就是输出门发挥作用的地方。\\ 在长短期记忆网络中，它仅仅是记忆元的\\tanh的门控版本。这就确保了\\mathbf{H}_t的值始终在区间(-1, 1)内：\\\n\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t).\\\n只要输出门接近1，我们就能够有效地将所有记忆信息传递给预测部分，\\ 而对于输出门接近0，我们只保留记忆元内的所有信息，而不需要更新隐状态。\\ $$\n7.编码器解码器架构 # 编码器（encoder）：接受一个长度可变的序列作为输入，并将其转换为具有固定形状的编码状态。\n解码器（decoder）：将固定形状的编码状态映射到长度可变的序列。\n这被称为编码器-解码器（encoder-decoder）架构:\n在编码器接口中，我们只指定长度可变的序列作为编码器的输入X。任何继承这个Encoder基类的模型将完成代码实现。\nclass Encoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;编码器-解码器架构的基本编码器接口\u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): super(Encoder, self).__init__(**kwargs) def forward(self, X, *args): raise NotImplementedError 在下面的解码器接口中，新增一个init_state函数，用于将编码器的输出（enc_outputs）转换为编码后的状态。\n注意，此步骤可能需要额外的输入，例如：输入序列的有效长度。为了逐个地生成长度可变的词元序列，解码器在每个时间步都会将输入（例如：在前一时间步生成的词元）和编码后的状态映射成当前时间步的输出词元。\nclass Decoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;编码器-解码器架构的基本解码器接口\u0026#34;\u0026#34;\u0026#34; def __init__(self, **kwargs): super(Decoder, self).__init__(**kwargs) def init_state(self, enc_outputs, *args): raise NotImplementedError def forward(self, X, state): raise NotImplementedError 合并\n“编码器-解码器”架构包含了一个编码器和一个解码器，并且还拥有可选的额外的参数。\n在前向传播中，编码器的输出用于生成编码状态，这个状态又被解码器作为其输入的一部分。\nclass EncoderDecoder(nn.Module): \u0026#34;\u0026#34;\u0026#34;编码器-解码器架构的基类\u0026#34;\u0026#34;\u0026#34; def __init__(self, encoder, decoder, **kwargs): super(EncoderDecoder, self).__init__(**kwargs) self.encoder = encoder self.decoder = decoder def forward(self, enc_X, dec_X, *args): enc_outputs = self.encoder(enc_X, *args) dec_state = self.decoder.init_state(enc_outputs, *args) return self.decoder(dec_X, dec_state) seq2seq\n特定的“\u0026lt;eos\u0026gt;”表示序列结束词元。一旦输出序列生成此词元，模型就会停止预测。\n在循环神经网络解码器的初始化时间步，有两个特定的设计决定：\n首先，特定的“\u0026lt;bos\u0026gt;”表示序列开始词元，它是解码器的输入序列的第一个词元。\n其次，使用循环神经网络编码器最终的隐状态来初始化解码器的隐状态。\n例如，在 Sutskever.Vinyals.Le.2014的设计中，正是基于这种设计将输入序列的编码信息送入到解码器中来生成输出序列的。\n如上图所示，编码器最终的隐状态在每一个时间步都作为解码器的输入序列的一部分。可以允许标签成为原始的输出序列，从源序列词元“\u0026lt;bos\u0026gt;”“Ils”“regardent”“.”到新序列词元“Ils”“regardent”“.”“\u0026lt;eos\u0026gt;”来移动预测的位置。\nclass Seq2SeqEncoder(Encoder): \u0026#34;\u0026#34;\u0026#34;用于序列到序列学习的循环神经网络编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqEncoder, self).__init__(**kwargs) # 嵌入层 self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout) def forward(self, X, *args): # 输出\u0026#39;X\u0026#39;的形状：(batch_size,num_steps,embed_size) X = self.embedding(X) # 在循环神经网络模型中，第一个轴对应于时间步 X = X.permute(1, 0, 2) # 如果未提及状态，则默认为0 output, state = self.rnn(X) # output的形状:(num_steps,batch_size,num_hiddens) # state的形状:(num_layers,batch_size,num_hiddens)如果使用LSTM，将包含记忆单元信息 return output, state class Seq2SeqDecoder(Decoder): \u0026#34;\u0026#34;\u0026#34;用于序列到序列学习的循环神经网络解码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs): super(Seq2SeqDecoder, self).__init__(**kwargs) self.embedding = nn.Embedding(vocab_size, embed_size) self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, *args): return enc_outputs[1] def forward(self, X, state): # 输出\u0026#39;X\u0026#39;的形状：(batch_size,num_steps,embed_size) X = self.embedding(X).permute(1, 0, 2) # 广播context，使其具有与X相同的num_steps context = state[-1].repeat(X.shape[0], 1, 1) X_and_context = torch.cat((X, context), 2) output, state = self.rnn(X_and_context, state) output = self.dense(output).permute(1, 0, 2) # output的形状:(batch_size,num_steps,vocab_size) # state的形状:(num_layers,batch_size,num_hiddens) return output, state 预测：\n三、注意力机制 # 1.注意力机制 # 非参数注意力池化层\n$$ f(x) = \\sum_{i=1}^n \\frac{K(x - x_i)}{\\sum_{j=1}^n K(x - x_j)} y_i $$\n高斯核（Gaussian kernel），其定义为： $$ K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}). $$\n将高斯核代入可以得到：\n$$ \\begin{aligned} f(x) \u0026amp;=\\sum_{i=1}^n \\alpha(x, x_i) y_i\\ \u0026amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}(x - x_i)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}(x - x_j)^2\\right)} y_i \\\u0026amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}(x - x_i)^2\\right) y_i. \\end{aligned} $$\n带参数注意力汇聚 $$ \\begin{aligned}f(x) \u0026amp;= \\sum_{i=1}^n \\alpha(x, x_i) y_i \\\u0026amp;= \\sum_{i=1}^n \\frac{\\exp\\left(-\\frac{1}{2}((x - x_i)w)^2\\right)}{\\sum_{j=1}^n \\exp\\left(-\\frac{1}{2}((x - x_j)w)^2\\right)} y_i \\\u0026amp;= \\sum_{i=1}^n \\mathrm{softmax}\\left(-\\frac{1}{2}((x - x_i)w)^2\\right) y_i.\\end{aligned} $$\n注意力分数\n$$ 用数学语言描述，假设有一个查询\\mathbf{q} \\in \\mathbb{R}^q和 m个 键-值 对(\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}_m)，其中\\mathbf{k}_i \\in \\mathbb{R}^k，\\mathbf{v}_i \\in \\mathbb{R}^v。\\ 注意力汇聚函数f就被表示成值的加权和：\\\nf(\\mathbf{q}, (\\mathbf{k}_1, \\mathbf{v}_1), \\ldots, (\\mathbf{k}_m, \\mathbf{v}m)) = \\sum{i=1}^m \\alpha(\\mathbf{q}, \\mathbf{k}_i) \\mathbf{v}_i \\in \\mathbb{R}^v,\\\n其中查询\\mathbf{q}和键\\mathbf{k}_i的注意力权重（标量） 是通过注意力评分函数a将两个向量映射成标量， 再经过softmax运算得到的：\\\n\\alpha(\\mathbf{q}, \\mathbf{k}_i) = \\mathrm{softmax}(a(\\mathbf{q}, \\mathbf{k}_i)) = \\frac{\\exp(a(\\mathbf{q}, \\mathbf{k}i))}{\\sum{j=1}^m \\exp(a(\\mathbf{q}, \\mathbf{k}_j))} \\in \\mathbb{R}. $$\n2.加性注意力 # $$ a(\\mathbf q, \\mathbf k) = \\mathbf w_v^\\top \\text{tanh}(\\mathbf W_q\\mathbf q + \\mathbf W_k \\mathbf k) \\in \\mathbb{R},\\\n其中可学习的参数是\\mathbf W_q\\in\\mathbb R^{h\\times q}、 \\mathbf W_k\\in\\mathbb R^{h\\times k}和 \\mathbf w_v\\in\\mathbb R^{h}。 $$ 相当于keys、values 合并之后放入隐藏层大小为 h、输出层为 1 的 MLP\ndef sequence_mask(X, valid_len, value=0): \u0026#34;\u0026#34;\u0026#34;在序列中屏蔽不相关的项\u0026#34;\u0026#34;\u0026#34; maxlen = X.size(1) # 创建一个包含从 0 到 maxlen-1 的张量表示序列的位置索引 # [None, :]：通过添加维度，将上述张量从一维变为二维，形状为 (1, maxlen) # 广播，将 valid_len 张量从一维转换为二维，并与上述张量进行逐元素比较 # mask[i, j] 表示序列中位置索引为 j 的元素是否属于有效长度范围内的元素 mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] \u0026lt; valid_len[:, None] X[~mask] = value return X def masked_softmax(X, valid_lens): \u0026#34;\u0026#34;\u0026#34;通过在最后一个轴上掩蔽元素来执行softmax操作\u0026#34;\u0026#34;\u0026#34; # X:3D张量，valid_lens:1D或2D张量 if valid_lens is None: return nn.functional.softmax(X, dim=-1) else: shape = X.shape if valid_lens.dim() == 1: # 第一维上重复 shape[1] 次 valid_lens = torch.repeat_interleave(valid_lens, shape[1]) else: # 转为一维 valid_lens = valid_lens.reshape(-1) # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0 X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6) return nn.functional.softmax(X.reshape(shape), dim=-1) class AdditiveAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34;Additive attention\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs): super(AdditiveAttention, self).__init__(**kwargs) self.W_k = nn.Linear(key_size, num_hiddens, bias=False) self.W_q = nn.Linear(query_size, num_hiddens, bias=False) self.w_v = nn.Linear(num_hiddens, 1, bias=False) self.dropout = nn.Dropout(dropout) def forward(self, queries, keys, values, valid_lens): queries, keys = self.W_q(queries), self.W_k(keys) features = queries.unsqueeze(2) + keys.unsqueeze(1) features = torch.tanh(features) scores = self.w_v(features).squeeze(-1) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) 3.缩放点积注意力 # $$ 评分函数为：\\ a(\\mathbf q, \\mathbf k) = \\mathbf{q}^\\top \\mathbf{k} /\\sqrt{d}.\\ 基于n个查询和m个键－值对计算注意力， 其中查询和键的长度为d，值的长度为v。\\ 查询\\mathbf Q\\in\\mathbb R^{n\\times d}、 键\\mathbf K\\in\\mathbb R^{m\\times d}和 值\\mathbf V\\in\\mathbb R^{m\\times v}的缩放点积注意力是：\\\n\\mathrm{softmax}\\left(\\frac{\\mathbf Q \\mathbf K^\\top }{\\sqrt{d}}\\right) \\mathbf V \\in \\mathbb{R}^{n\\times v}. $$ 除以根号 d 让函数值不过大，使其不受序列长度的影响\nLatex 自带转PDF为SVG：pdftocairo -svg source.pdf # class DotProductAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34;缩放点积注意力\u0026#34;\u0026#34;\u0026#34; def __init__(self, dropout, **kwargs): super(DotProductAttention, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) # queries的形状：(batch_size，查询的个数，d) # keys的形状：(batch_size，“键－值”对的个数，d) # values的形状：(batch_size，“键－值”对的个数，值的维度) # valid_lens的形状:(batch_size，)或者(batch_size，查询的个数) def forward(self, queries, keys, values, valid_lens=None): d = queries.shape[-1] # 设置transpose_b=True为了交换keys的最后两个维度 scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d) self.attention_weights = masked_softmax(scores, valid_lens) return torch.bmm(self.dropout(self.attention_weights), values) 4.多头注意力 # $$ 给定查询\\mathbf{q} \\in \\mathbb{R}^{d_q}、键\\mathbf{k} \\in \\mathbb{R}^{d_k}和 值\\mathbf{v} \\in \\mathbb{R}^{d_v}， 每个注意力头\\mathbf{h}_i（i = 1, \\ldots, h）的计算方法为：\\\n\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},\\\n其中，可学习的参数包括 \\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}、 \\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}和 \\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}， 以及代表注意力汇聚的函数f。 $$ f 可以是加性注意力和缩放点积注意力。多头注意力的输出需要经过另一个线性转换，它对应着 h 个头连结后的结果，因此其可学习参数是 $$ \\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}：\\\n\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\vdots\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}. $$\n在实现过程中通常选择缩放点积注意力作为每一个注意力头。\n为了避免计算代价和参数代价的大幅增长，设定 $$ p_q = p_k = p_v = p_o / H $$ 值得注意的是，如果将查询、键和值的线性变换的输出数量设置为 $$ p_q H = p_k H = p_v H = p_o $$ 则可以并行计算 H 个头。\n代码中，p_o = 隐藏层数 h 。将 KQV 最后一维转为隐藏层数。\n后转为 nH 批次，代表多头的头 H，如下图所示\n放入先前的点积注意力块后得到形状 (nH, SQ, h/H) 的输出\n拼接即：转为形状 (n, q, h) 的输出\n输出经过线性层，得到形状 (n, q, h) 的输出\ndef transpose_qkv(X, num_heads): \u0026#34;\u0026#34;\u0026#34;为了多注意力头的并行计算而变换形状\u0026#34;\u0026#34;\u0026#34; # 输入X的形状:(batch_size，查询或者“键－值”对的个数，num_hiddens) # 输出X的形状:(batch_size，查询或者“键－值”对的个数，num_heads，num_hiddens/num_heads) X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads) X = X.permute(0, 2, 1, 3) # 最终输出的形状:(batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads) return X.reshape(-1, X.shape[2], X.shape[3]) def transpose_output(X, num_heads): \u0026#34;\u0026#34;\u0026#34;逆转transpose_qkv函数的操作\u0026#34;\u0026#34;\u0026#34; X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) X = X.permute(0, 2, 1, 3) return X.reshape(X.shape[0], X.shape[1], -1) class MultiHeadAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34;多头注意力\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs): super(MultiHeadAttention, self).__init__(**kwargs) self.num_heads = num_heads self.attention = DotProductAttention(dropout) self.W_q = nn.Linear(query_size, num_hiddens, bias=bias) self.W_k = nn.Linear(key_size, num_hiddens, bias=bias) self.W_v = nn.Linear(value_size, num_hiddens, bias=bias) self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) def forward(self, queries, keys, values, valid_lens): # queries，keys，values的形状: # (batch_size，查询或者“键－值”对的个数，num_hiddens) # valid_lens　的形状: # (batch_size，)或(batch_size，查询的个数) # 经过变换后，输出的queries，keys，values　的形状: # (batch_size*num_heads，查询或者“键－值”对的个数， # num_hiddens/num_heads) queries = transpose_qkv(self.W_q(queries), self.num_heads) keys = transpose_qkv(self.W_k(keys), self.num_heads) values = transpose_qkv(self.W_v(values), self.num_heads) if valid_lens is not None: # 在轴0，将第一项（标量或者矢量）复制num_heads次， # 然后如此复制第二项，然后诸如此类。 valid_lens = torch.repeat_interleave( valid_lens, repeats=self.num_heads, dim=0) # output的形状:(batch_size*num_heads，查询的个数， # num_hiddens/num_heads) output = self.attention(queries, keys, values, valid_lens) # output_concat的形状:(batch_size，查询的个数，num_hiddens) output_concat = transpose_output(output, self.num_heads) return self.W_o(output_concat) 5.自注意力、位置编码 # $$ 给定一个由词元组成的输入序列\\mathbf{x}_1, \\ldots, \\mathbf{x}_n， 其中任意\\mathbf{x}_i \\in \\mathbb{R}^d（1 \\leq i \\leq n）。\\ 该序列的自注意力输出为一个长度相同的序列 \\mathbf{y}_1, \\ldots, \\mathbf{y}_n，其中：\\ \\mathbf{y}_i = f(\\mathbf{x}_i, (\\mathbf{x}_1, \\mathbf{x}_1), \\ldots, (\\mathbf{x}n, \\mathbf{x}n)) \\in \\mathbb{R}^d \\\\ 对长度为n的序列\\mathbf{X}\\in \\mathbb{R}^{n \\times d} ，位置编码\\mathbf{P} \\in \\mathbb{R}^{n \\times d}输出\\mathbf{X} + \\mathbf{P}， 矩阵第i行、第2j列和2j+1列上的元素为：\\ \\begin{aligned} p{i, 2j} \u0026amp;= \\sin\\left(\\frac{i}{10000^{2j/d}}\\right),\\p{i, 2j+1} \u0026amp;= \\cos\\left(\\frac{i}{10000^{2j/d}}\\right).\\end{aligned} $$ 代码上自注意力即多头注意力KQV全部是输入\nclass PositionalEncoding(nn.Module): \u0026#34;\u0026#34;\u0026#34;位置编码\u0026#34;\u0026#34;\u0026#34; def __init__(self, num_hiddens, dropout, max_len=1000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(dropout) # 创建一个足够长的三维P self.P = torch.zeros((1, max_len, num_hiddens)) # 批次为1 # X形状：(max_len, num_hiddens/2) X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow( 10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens) self.P[:, :, 0::2] = torch.sin(X) # 偶数索引赋值 self.P[:, :, 1::2] = torch.cos(X) # 奇数索引赋值 def forward(self, X): X = X + self.P[:, :X.shape[1], :].to(X.device) return self.dropout(X) # 避免对位置过于敏感 #########使用########## encoding_dim, num_steps = 32, 60 pos_encoding = PositionalEncoding(encoding_dim, 1) pos_encoding.eval() X = pos_encoding(torch.zeros((1, num_steps, encoding_dim))) 6.Transformer # 输入编码\n训练\n预测\n编码器\nclass PositionWiseFFN(nn.Module): \u0026#34;\u0026#34;\u0026#34;基于位置的前馈网络\u0026#34;\u0026#34;\u0026#34; def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs): super(PositionWiseFFN, self).__init__(**kwargs) self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens) self.relu = nn.ReLU() self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs) def forward(self, X): return self.dense2(self.relu(self.dense1(X))) class AddNorm(nn.Module): \u0026#34;\u0026#34;\u0026#34;残差连接后进行层规范化\u0026#34;\u0026#34;\u0026#34; def __init__(self, normalized_shape, dropout, **kwargs): super(AddNorm, self).__init__(**kwargs) self.dropout = nn.Dropout(dropout) self.ln = nn.LayerNorm(normalized_shape) def forward(self, X, Y): return self.ln(self.dropout(Y) + X) class EncoderBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34;Transformer编码器块\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs): super(EncoderBlock, self).__init__(**kwargs) self.attention = MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias) self.addnorm1 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm2 = AddNorm(norm_shape, dropout) def forward(self, X, valid_lens): Y = self.addnorm1(X, self.attention(X, X, X, valid_lens)) return self.addnorm2(Y, self.ffn(Y)) class TransformerEncoder(Encoder): \u0026#34;\u0026#34;\u0026#34;Transformer编码器\u0026#34;\u0026#34;\u0026#34; def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs): super(TransformerEncoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens # 将词汇表中的单词表示为连续的向量表示,初始化参数包括：num_embeddings：词汇表的大小 embedding_dim：嵌入向量的维度 self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\u0026#34;block\u0026#34;+str(i), EncoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias)) def forward(self, X, valid_lens, *args): # 因为位置编码值在-1和1之间， # L2正则化=1，如果维度越大，里面的数值越小。 # 因此嵌入值乘以嵌入维度的平方根进行缩放，使得embedding的数值和位置编码数值差不多大小 # 然后再与位置编码相加。 X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) # 可视化使用 self.attention_weights = [None] * len(self.blks) for i, blk in enumerate(self.blks): X = blk(X, valid_lens) self.attention_weights[i] = blk.attention.attention.attention_weights return X 解码器\nclass DecoderBlock(nn.Module): \u0026#34;\u0026#34;\u0026#34;解码器中第i个块\u0026#34;\u0026#34;\u0026#34; def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs): super(DecoderBlock, self).__init__(**kwargs) self.i = i self.attention1 = MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm1 = AddNorm(norm_shape, dropout) self.attention2 = MultiHeadAttention( key_size, query_size, value_size, num_hiddens, num_heads, dropout) self.addnorm2 = AddNorm(norm_shape, dropout) self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens) self.addnorm3 = AddNorm(norm_shape, dropout) def forward(self, X, state): enc_outputs, enc_valid_lens = state[0], state[1] # 训练阶段，输出序列的所有词元都在同一时间处理， # 因此state[2][self.i]初始化为None。 # 预测阶段，输出序列是通过词元一个接着一个解码的， # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示 if state[2][self.i] is None: key_values = X else: key_values = torch.cat((state[2][self.i], X), axis=1) state[2][self.i] = key_values if self.training: batch_size, num_steps, _ = X.shape # 训练时需要将后面的遮掉 # dec_valid_lens的开头:(batch_size,num_steps), 其中每一行是[1,2,...,num_steps] dec_valid_lens = torch.arange(1, num_steps + 1, device=X.device).repeat(batch_size, 1) else: dec_valid_lens = None # 训练时:自注意力，预测时：非自注意力 X2 = self.attention1(X, key_values, key_values, dec_valid_lens) Y = self.addnorm1(X, X2) # 编码器－解码器注意力。 # enc_outputs的开头:(batch_size,num_steps,num_hiddens) Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) Z = self.addnorm2(Y, Y2) return self.addnorm3(Z, self.ffn(Z)), state class TransformerDecoder(AttentionDecoder): def __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, **kwargs): super(TransformerDecoder, self).__init__(**kwargs) self.num_hiddens = num_hiddens self.num_layers = num_layers self.embedding = nn.Embedding(vocab_size, num_hiddens) self.pos_encoding = PositionalEncoding(num_hiddens, dropout) self.blks = nn.Sequential() for i in range(num_layers): self.blks.add_module(\u0026#34;block\u0026#34;+str(i), DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i)) self.dense = nn.Linear(num_hiddens, vocab_size) def init_state(self, enc_outputs, enc_valid_lens, *args): return [enc_outputs, enc_valid_lens, [None] * self.num_layers] def forward(self, X, state): X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens)) # 可视化使用 self._attention_weights = [[None] * len(self.blks) for _ in range (2)] for i, blk in enumerate(self.blks): X, state = blk(X, state) # 解码器自注意力权重 self._attention_weights[0][i] = blk.attention1.attention.attention_weights # “编码器－解码器”自注意力权重 self._attention_weights[1][i] = blk.attention2.attention.attention_weights return self.dense(X), state @property def attention_weights(self): return self._attention_weights ","externalUrl":null,"permalink":"/docs/python/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","section":"Docs","summary":"零、训练 # 1.模型的断点续训练 # 在训练过程中，可能由于某种意","title":"深度学习","type":"docs"},{"content":" Java # 1.堆/优先队列 # Queue\u0026lt;Integer\u0026gt; bigHeap = new PriorityQueue\u0026lt;\u0026gt;((a, b) -\u0026gt; b - a); // 大顶堆 bigHeap.offer(num);\t//入堆 int num = bigHeap.poll();\t//出堆 // 347:给你一个整数数组 nums 和一个整数 k ，请你返回其中出现频率前 k 高的元素 public int[] topKFrequent(int[] nums, int k) { Map\u0026lt;Integer, Integer\u0026gt; occurrences = new HashMap\u0026lt;Integer, Integer\u0026gt;(); for (int num : nums) { occurrences.put(num, occurrences.getOrDefault(num, 0) + 1); } // int[] 的第一个元素代表数组的值，第二个元素代表了该值出现的次数 PriorityQueue\u0026lt;int[]\u0026gt; queue = new PriorityQueue\u0026lt;int[]\u0026gt;(new Comparator\u0026lt;int[]\u0026gt;() { public int compare(int[] m, int[] n) { return m[1] - n[1]; } }); // 维护一个大小为k的小顶堆 for (Map.Entry\u0026lt;Integer, Integer\u0026gt; entry : occurrences.entrySet()) { int num = entry.getKey(), count = entry.getValue(); if (queue.size() == k) { if (queue.peek()[1] \u0026lt; count) { queue.poll(); queue.offer(new int[]{num, count}); } } else { queue.offer(new int[]{num, count}); } } int[] ret = new int[k]; for (int i = 0; i \u0026lt; k; ++i) { ret[i] = queue.poll()[0]; } return ret; } 2.哈希表HashMap # Map\u0026lt;Integer, Integer\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); // 将数组进行统计： for (int num : nums) { map.put(num, map.getOrDefault(num, 0) + 1); } // 键/值对的数量 map.size(); // 获取指定key对应的value map.get(); map.getOrDefault(key,defaultValue)\t//无键值对则返回默认值 // 添加/修改 map.put(1,2); // 删除--键、值（可选） map.remove(1); // 是否存在指定的 Key/value map.containsKey(); map.containsValue(); // 获取全部实体、键或值 map.entrySet();\t//每个的类型都是 Map.Entry\u0026lt;Integer, Integer\u0026gt; map.keySet(); map.values(); 3.String # **正则表达式替换 ** 转义需要使用两个\\\nreplaceAll(\u0026#34;[\\\\[\\\\], ]\u0026#34;,\u0026#34;\u0026#34;) 4.int[]降序排序 # 对int数组进行升序排序后修改为降序\nArrays.sort(satisfaction); for (int i = 0, j = satisfaction.length - 1; i \u0026lt; j; i++, j--) { int temp = satisfaction[i]; satisfaction[i] = satisfaction[j]; satisfaction[j] = temp; } 使用 Comparator\nArrays.sort(intervals, new Comparator\u0026lt;int[]\u0026gt;() { @Override public int compare(int[] o1, int[] o2) { // return (o1[1] \u0026lt; o2[1]) ? -1 : ((o1[1] == o2[1]) ? 0 : 1); return o1[1] - o2[1]; } }); 5.List接口 # Collection接口：\nadd(index, element) - 将元素添加到列表 get(index) - 有助于从列表中随机访问元素 set(index, element) - 更改列表的元素 remove(index) - 从列表中删除一个元素 toArray(new String[size]) - 将列表转换为数组 size() - 返回列表的长度 addAll(Collection\u0026lt;?\u0026gt; c) - 将一个列表的所有元素添加到另一个 removeAll() - 从列表中删除所有元素 clear() - 从列表中删除所有元素（比removeAll()效率更高） isEmpty() - 是否为空 iterator() - 返回迭代器对象，该对象可用于顺序访问列表的元素 contains() - 如果列表包含指定的元素，则返回true List 接口使用：\nList\u0026lt;String\u0026gt; list1 = new ArrayList\u0026lt;\u0026gt;(); indexOf(Object o) 1)ArrayList # 基于数组实现，随机选择快，插入删除节点慢\nforEach() 2)LinkedList # 基于双向链表实现，占用空间更多，随机选择元素更慢，插入删除节点快\npeek()/peekFirst() - 链表头节点，返回头节点或null\nelement()/getFirst() - 头节点，返回头节点\npeekLast() - 链尾节点，返回尾节点或null\npop() - 删除链头节点，返回节点，或报错\npoll()/remove()/pollFirst() - 删除头节点，返回头节点或null\npollLast() - 删除链尾节点，返回尾节点或null\noffer()/add()/offerLast() - 链尾添加节点，返回 True\nofferFirst() - 链头添加节点，返回 True\naddFirst()/push() - 链头添加节点，void函数\npop()\\push()\\peek() 结合当栈使用\npop()\\add()\\peek() 队列\n3)Stack # 继承自Vector\npush() pop() peek() empty() - 是否为空 search(item) - 最后一个出现的项到栈顶的距离 4)Vector # 继承自AbstractList(实现了List、RandomAccess、Cloneable、Serializable接口)\n与Arraylist区别：\n线程安全，使用了 Synchronized 实现线程同步 性能比ArrayList更差（加了锁） 扩容：每次扩容一倍，ArrayList每次扩容50% 同步：所有的方法都是同步的，在不需要保证线程安全时使用ArrayList 6.并查集 # 并查集是一种用于管理元素所属集合的数据结构，实现为一个森林，其中每棵树表示一个集合，树中的节点表示对应集合中的元素。\n顾名思义，并查集支持两种操作：\n合并（Union）：合并两个元素所属集合（合并对应的树） 查询（Find）：查询某个元素所属集合（查询对应的树的根节点），这可以用于判断两个元素是否属于同一集合 并查集在经过修改后可以支持单个元素的删除、移动；使用动态开点线段树还可以实现可持久化并查集\nprivate class UnionFind { private int[] parent; private int count;\t// 可选，联通分量 private int size;\t// 可选，求联通分量的大小。找根节点 i == parent[i] 则 i 所在的分量大小为 size[i] public int getCount() { return count; } public UnionFind(int n) { count = n; parent = new int[n]; size = new int[n]; for (int i = 0; i \u0026lt; n; i++) { parent[i] = i; size[i] = 1; } } public int find(int x) { while (x != parent[x]) { parent[x] = parent[parent[x]];\t// 路径压缩 x = parent[x]; } return x; } public int find(int x) {\t// 更容易理解 if (parent[x] != x) { parent[x] = find(parent[x]); } return parent[x]; } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX == rootY) { return; } parent[rootX] = rootY; count--; size[rootY] += size[rootX];\t// 方向不能反 } } 7.动态规划 # 爬楼梯问题(每次你可以爬 1 或 2 个台阶，你有多少种不同的方法可以爬到楼顶)\n解法一、dfs暴力解法 # public int climbStairs(int n) { return dfs(n); } private int dfs(int n){ if(n == 1 || n == 2){ return n; } return dfs(n - 1) + dfs(n - 2); } 解法二、记忆化搜索 # 使用一个数组保存计算过的解\npublic int climbStairs(int n) { int[] counts = new int[n + 1]; Arrays.fill(counts, -1); return dfs(n, counts); } int dfs(int i, int[] mem){ if(i == 1 || i == 2){ return i;\t// 初始解 } if(mem[i] != -1){ return mem[i];\t// 若存在记录，则直接返回 } return mem[i] = dfs(i-1, mem) + dfs(i-2, mem); } 解法三、动态规划 # 四个解题步骤是：\n定义子问题 写出子问题的递推关系 确定 DP 数组的计算顺序 空间优化（可选） 去除记忆化搜索的递归操作\npublic int climbStairs(int n) { if(n == 1 || n == 2){ return n; } int[] dp = new int[n + 1];\t// dp表， dp[i] 表示状态i对应子问题的解 dp[1] = 1;\t// 初始状态 dp[2] = 2;\t// 初始状态 for(int i = 3; i \u0026lt; n + 1; i++){ dp[i] = dp[i - 1] + dp[i - 2];\t// 状态转移方程 } return dp[n]; } 解法四、动态规划（空间优化） # 由于答案仅和上一个状态和上上个状态相关，只需要存储两个状态即可\npublic int climbStairs(int n) { if(n == 1 || n == 2){ return n; } int i_2 = 1, i_1 = 2;\t// 上上个状态 i-2， 上个状态 i-1 int ans = 0;\t// 结果 for(int i = 3; i \u0026lt; n + 1; i++){ ans = i_1 + i_2;\t// 状态转移方程 i_2 = i_1; i_1 = ans; } return ans; } 8. 单调栈 # 503. 下一个更大元素 II\n求循环数组下一个更大元素\npublic int[] nextGreaterElements(int[] nums) { int n = nums.length; int[] ret = new int[n]; Arrays.fill(ret, -1); Deque\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;Integer\u0026gt;(); for (int i = 0; i \u0026lt; n * 2 - 1; i++) { while (!stack.isEmpty() \u0026amp;\u0026amp; nums[stack.peek()] \u0026lt; nums[i % n]) { ret[stack.pop()] = nums[i % n]; } stack.push(i % n); } return ret; } 739. 每日温度\n求数组中与下一个更大元素的距离\npublic int[] dailyTemperatures(int[] temperatures) { int n = temperatures.length; int[] res = new int[n]; Arrays.fill(res, 0); Deque\u0026lt;Integer\u0026gt; stack = new LinkedList\u0026lt;Integer\u0026gt;(); for(int i = 0; i \u0026lt; n; i++) { while (!stack.isEmpty() \u0026amp;\u0026amp; temperatures[stack.peek()] \u0026lt; temperatures[i]) { int t = stack.pop(); res[t] = i - t; } stack.push(i); } return res; } 9.快速幂 # public int pow(int m, int n) { if (n \u0026lt; 0) { throw new IllegalArgumentException(\u0026#34;指数不能小于 0\u0026#34;); } else if (n == 0) { return 1; } else if (n == 1) { return m; } if ((n \u0026amp; 1) == 1) { return pow(m * m, n / 2) * m; } else { return pow(m * m, n / 2); } } 10.求最大公因数（欧几里得算法/辗转相除法） # public int Gcd(int M, int N) { if (M \u0026lt; 0 || N \u0026lt; 0){ throw new IllegalArgumentException(\u0026#34;输入异常\u0026#34;); } int rem; // 余数 while (N \u0026gt; 0) { rem = M % N; M = N; N = rem; } return M; } 11.堆的实现 # public class Heap { int capacity; int size; int[] elements; public Heap(){ capacity = 11; size = 0; elements = new int[11]; elements[0] = Integer.MIN_VALUE; // 哨兵 } public Heap(int maxLength) { capacity = maxLength + 1; size = 0; elements = new int[maxLength + 1]; elements[0] = Integer.MIN_VALUE; // 哨兵 } private boolean isFull() { return capacity == size + 1; } public void insert(int x) { if (isFull()) { throw new ArrayIndexOutOfBoundsException(\u0026#34;堆已满\u0026#34;); } int i; // 在末尾制造空穴，每次与父节点比较，若小于父节点，空穴上浮 for (i = ++size; elements[i / 2] \u0026gt; x; i /= 2) { elements[i] = elements[i / 2]; } // 找到最终位置 elements[i] = x; } public int delete() { int i, child; int min, last; if(size == 0) { throw new ArrayIndexOutOfBoundsException(\u0026#34;堆空\u0026#34;); } min = elements[1]; last = elements[size--]; // i位置为空穴，每次选择更小的孩子填充空穴，使空穴下滤 // 直到找到地方可以放置last for (i = 1; i * 2 \u0026lt; size; i = child) { child = i * 2; if (child != size \u0026amp;\u0026amp; elements[child + 1] \u0026lt; elements[child]){ child++; } if (last \u0026gt; elements[child]){ elements[i] = elements[child]; } else { // 未到底层就找到i可以放置last break; } } elements[i] = last; return min; } 12.排序 # 12.1 插入排序 ($O(N) - O(N^2), 平均O(N^2)$) # 每次拿到一个元素，往前遍历找到它应该存放的位置，其它位置都往后放\nvoid insertionSort(int[] array) { int n = array.length; for (int i = 1; i \u0026lt; n; i++){ int tmp = array[i]; int j; for (j = i; j \u0026gt;= 1 \u0026amp;\u0026amp; array[j - 1] \u0026gt; tmp; j--) { array[j] = array[j - 1]; } array[j] = tmp; } } 12.2 希尔排序，缩小增量排序（ $希尔增量O(N^2), Hibbard增量O(N^{\\frac{3}{2}})$） # 每趟比较相同间距的元素，直到最后比较相邻元素\n希尔增量 $h_t = N / 2; h_k = h_{k+1} / 2$\nhibbard增量 $1, 3, 5, \u0026hellip; ,2^k - 1$\nvoid shellSort(int[] array) { int n = array.length; // 增量逐渐缩小到1 for (int increment = n / 2; increment \u0026gt; 0; increment /= 2) { // 相当于间距为 increment 组成的数组的插入排序 for (int i = increment; i \u0026lt; n; i++) { int tmp = array[i]; int j; for (j = i; j \u0026gt;= increment \u0026amp;\u0026amp; tmp \u0026lt; array[j - increment]; j -= increment) { array[j] = array[j - increment]; } array[j] = tmp; } } } 12.3 堆排序 ($N\\log(N)-O(N) —— 2N\\log(N) - O(N^2)$) # int leftChild(int i) { return 2 * i + 1; } // 将 i 处的值下沉到合适位置，n为堆大小 void percDown(int[] array, int i, int n){ int child, tmp; for (tmp = array[i]; leftChild(i) \u0026lt; n; i = child) { child = leftChild(i); if (child != n - 1 \u0026amp;\u0026amp; array[child + 1] \u0026gt; array[child]) { child++; } if (tmp \u0026lt; array[child]){ array[i] = array[child]; } else { break; } } array[i] = tmp; } void heapSort(int[] array) { long now = System.currentTimeMillis(); int n = array.length; // 建堆 for (int i = n / 2; i \u0026gt;= 0; i--) { percDown(array, i, n); } for (int i = n - 1; i \u0026gt; 0; i--) { // 将堆顶放到最后 int tmp = array[0]; array[0] = array[i]; array[i] = tmp; percDown(array, 0, i); } System.out.println(System.currentTimeMillis() - now); } 12.4 归并排序($O(N \\log(N))$) # void merge(int[] A, int[] tmp, int left, int right, int rightEnd) { int leftEnd = right - 1; // 左半部分右侧端点 int tmpPos = left; // 临时数组初始指针 int numCounts = rightEnd - left + 1; // 总数 // 2部分数组归并到临时数组 while (left \u0026lt;= leftEnd \u0026amp;\u0026amp; right \u0026lt;= rightEnd) { if (A[left] \u0026lt;= A[right]){ tmp[tmpPos++] = A[left++]; } else { tmp[tmpPos++] = A[right++]; } } while(left \u0026lt;= leftEnd) { tmp[tmpPos++] = A[left++]; } while(right \u0026lt;= rightEnd) { tmp[tmpPos++] = A[right++]; } // 复制覆盖原数组 for (int i = 0; i \u0026lt; numCounts; i++,rightEnd--) { A[rightEnd] = tmp[rightEnd]; } } void mSort(int[] A, int[] tmp, int left, int right) { if (left \u0026lt; right) { int mid = (left + right) / 2; mSort(A, tmp, left, mid); // 前半部分归并 mSort(A, tmp, mid + 1, right); // 后半部分归并 merge(A, tmp, left, mid + 1, right); // 归并两个已排序的数组 } } void mergeSort(int[] A) { long now = System.currentTimeMillis(); int n = A.length; int[] tmp = new int[n]; mSort(A, tmp, 0, n - 1); System.out.println(System.currentTimeMillis() - now); } 12.5 快速排序($O(N \\log(N))——O(N^2)$) # 选取任意元素作为枢纽 将元素划分为两部分，一部分小于枢纽，另一部分大于枢纽 对左右两部分递归地进行快排 void quickSort(int[] A, int low, int high){ int i = low, j = high; if(low \u0026lt; high) { int pivot = i; // 枢纽 while (i \u0026lt; j) { while (i \u0026lt; j \u0026amp;\u0026amp; A[j] \u0026gt;= A[pivot]){ j--; // 从右到左找到第一个小于枢纽的元素 } while (i \u0026lt; j \u0026amp;\u0026amp; A[i] \u0026lt;= A[pivot]){ i++; // 从左到右找到第一个大于枢纽的元素 } if (i \u0026lt; j){ int tmp = A[j]; // 交换 low 和 high 位置的值 A[j] = A[i]; A[i] = tmp; } } int tmp = A[i]; A[i] = A[pivot]; A[pivot] = tmp; // i 位置将数组划分为左右两个部分：因为先移动的 j,最后 i quickSort(A, low, i - 1); // 左部分快排 quickSort(A, i + 1, high); // 右部分快排 } } void quick(int[] A){ quickSort(A, 0, A.length - 1); } 13. 前缀树 # class Trie { private Trie[] children;\t// 孩子节点 private boolean isEnd;\t// 是否存在以该节点结束的单词 public Trie() { children = new Trie[26]; isEnd = false; } public void insert(String word) { Trie node = this; for (int i = 0; i \u0026lt; word.length(); i++) { char ch = word.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { node.children[index] = new Trie(); } node = node.children[index]; } node.isEnd = true; } public boolean search(String word) { Trie node = searchPrefix(word); return node != null \u0026amp;\u0026amp; node.isEnd; } public boolean startsWith(String prefix) { return searchPrefix(prefix) != null; } private Trie searchPrefix(String prefix) { Trie node = this; for (int i = 0; i \u0026lt; prefix.length(); i++) { char ch = prefix.charAt(i); int index = ch - \u0026#39;a\u0026#39;; if (node.children[index] == null) { return null; } node = node.children[index]; } return node; } } 14. 线段树 # 307. 区域和检索 - 数组可修改\npublic class LeetCodeTest { int[] sum; int len; public LeetCodeTest(int[] nums) { len = nums.length; sum = new int[4 * len]; for (int i = 0; i \u0026lt; nums.length; i++) { updateValue(1, 1, len, i + 1, nums[i]); } } /** * 在 cur 管辖的 [l, r] 区间对 idx 位置(从 1 开始)增加 val * 递归结束将该层递归的 cur 位置管辖的区间和进行更新 */ public void updateValue(int cur, int l, int r, int idx, int val) { if (l == r) { // 找到需修改的单子区间\\ sum[cur] = val; return; } int mid = (l + r) / 2; if (idx \u0026lt;= mid) { // 需要在左子区间修改 updateValue(cur * 2, l, mid, idx, val); } else { // 需要在右子区间修改 updateValue(cur * 2 + 1, mid + 1, r, idx, val); } // 修改本级区间和 sum[cur] = sum[cur * 2] + sum[cur * 2 + 1]; } /** * 位置 cur 管理 [l, r] 区间，求 [targetL, targetR] 区间的和 * - 被包括在目标区间，直接返回 * - 被划分为左右区间，分别求和相加 */ public int sumRange(int cur, int l, int r, int targetL, int targetR) { if (targetL \u0026lt;= l \u0026amp;\u0026amp; targetR \u0026gt;= r) { return sum[cur]; // 当前区间为 [L,R] 的子区间 } int sum = 0; int mid = (l + r) / 2; if (targetL \u0026lt;= mid) { sum += sumRange(cur * 2, l, mid, targetL, targetR); } if (targetR \u0026gt; mid) { sum += sumRange(cur * 2 + 1, mid + 1, r, targetL, targetR); } return sum; } public void update(int index, int val) { updateValue(1, 1, len, index + 1, val); } public int sumRange(int left, int right) { return sumRange(1, 1, len, left + 1, right + 1); } public static void main(String[] args) { LeetCodeTest leetCodeTest = new LeetCodeTest(new int[]{1, 3, 5}); System.out.println(leetCodeTest.sumRange(0, 2)); leetCodeTest.update(1, 2); System.out.println(leetCodeTest.sumRange(0, 2)); } } 15. 回溯 # /* 回溯算法框架 */ void backtrack(State state, List\u0026lt;Choice\u0026gt; choices, List\u0026lt;State\u0026gt; res) { // 判断是否为解 if (isSolution(state)) { // 记录解 recordSolution(state, res); // 不再继续搜索 return; } // 遍历所有选择 for (Choice choice : choices) { // 剪枝：判断选择是否合法 if (isValid(state, choice)) { // 尝试：做出选择，更新状态 makeChoice(state, choice); backtrack(state, choices, res); // 回退：撤销选择，恢复到之前的状态 undoChoice(state, choice); } } } 22. 括号生成 数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。\n// 官解 class Solution { public List\u0026lt;String\u0026gt; generateParenthesis(int n) { List\u0026lt;String\u0026gt; ans = new ArrayList\u0026lt;String\u0026gt;(); backtrack(ans, new StringBuilder(), 0, 0, n); return ans; } // open close 记录左右括号的个数 public void backtrack(List\u0026lt;String\u0026gt; ans, StringBuilder cur, int open, int close, int max) { if (cur.length() == max * 2) { ans.add(cur.toString()); return; } if (open \u0026lt; max) {\t// 左括号小于 n 可以继续添加 cur.append(\u0026#39;(\u0026#39;); backtrack(ans, cur, open + 1, close, max); cur.deleteCharAt(cur.length() - 1); // 可以少记录一个 index } if (close \u0026lt; open) { // 右括号 少于 左括号，可以添加 cur.append(\u0026#39;)\u0026#39;); backtrack(ans, cur, open, close + 1, max); cur.deleteCharAt(cur.length() - 1); } } } // 我的解答 class Solution { public List\u0026lt;String\u0026gt; generateParenthesis(int n) { ArrayList\u0026lt;String\u0026gt; res = new ArrayList\u0026lt;\u0026gt;(); char[] choice = {\u0026#39;(\u0026#39;, \u0026#39;)\u0026#39;}; find(res, new StringBuilder(), choice, 0, n); return res; } public void find(ArrayList\u0026lt;String\u0026gt; res, StringBuilder path, char[] choice, int index, int n) { // 判断是否为解 if (isTrue(path, n)) { res.add(path.toString()); return; } if (index == (n \u0026lt;\u0026lt; 1)) return; // 长度不能超过 2 * n // 遍历所有选择 for (char c : choice) { // 剪枝，判断是否合法 if (isValid(path, c)) { // 尝试添加 path.append(c); find(res, path, choice, index + 1, n); // 回退状态 path.deleteCharAt(index); } } } public boolean isValid(StringBuilder path, char c) { if (c == \u0026#39;(\u0026#39;) return true; int len = path.length(); int left = 0; for (int i = 0; i \u0026lt; len; i++) { char cur = path.charAt(i); if (cur == \u0026#39;(\u0026#39;) { left++; } } // 左括号不能比右括号少 return (left \u0026lt;\u0026lt; 1) \u0026gt;= len + 1; } public boolean isTrue(StringBuilder path, int n) { int len = path.length(); if (len != n * 2) return false; char[] stack = new char[2 * n]; int top = -1; for (int i = 0; i \u0026lt; len; i++) { char cur = path.charAt(i); if (cur == \u0026#39;(\u0026#39;) { stack[++top] = \u0026#39;(\u0026#39;; } else if (top != -1){ top--; } else { return false; } } return top == -1; } } 16. Arrays # // 创建空的 n 个List List\u0026lt;Integer\u0026gt;[] res = new ArrayList[n]; Arrays.setAll(res, i -\u0026gt; new ArrayList\u0026lt;\u0026gt;()); // 数组转为List Arrays.asList(res) // 创建数组，赋予初始值 int[] visit = new int[n]; Arrays.fill(visit, -1); 17. 链表归并 # mergeSort每次找到链表的中点，断开，对两边分别进行mergeSort\npublic ListNode mergeSort(ListNode head) { // 当head.next==null时 说明当前链表只有一个元素 无序再排序 if (head==null || head.next==null) { return head; } // 找到中间节点 ListNode mid = findMid(head); // 存储中间节点的下一个结点 ListNode next = mid.next; // 从中间结点断开 分别对两边进行mergeSort mid.next = null; // 返回排序后的头节点 ListNode left = mergeSort(head); ListNode right = mergeSort(next); // 返回合并之后的头节点 return merge(left, right); } findMid找到链表的中点\n// 找到一个链表的中点 public ListNode findMid(ListNode head) { if (head==null) return head; ListNode fast = head.next; // 快指针 每次走2步 ListNode slow = head; // 慢指针 每次走1步 while (fast!=null \u0026amp;\u0026amp; fast.next!=null) { fast = fast.next.next; slow = slow.next; } return slow; } merge 两个升序的链表，将其合并为一个升序的链表\npublic ListNode merge(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(-1); ListNode curr = dummy; while (l1!=null \u0026amp;\u0026amp; l2!=null) { if (l1.val\u0026lt;l2.val) { curr.next = l1; l1 = l1.next; } else { curr.next = l2; l2 = l2.next; } curr = curr.next; } if (l1!=null) { curr.next = l1; } if (l2!=null) { curr.next = l2; } return dummy.next; } 18. LRU缓存 # 设计思路：\n保证 get 时间复杂度为 O(1)，因此需要使用 Map 保证 LRU 的有序，需要使用链表，为了便于删除、插入操作，使用双向链表 链表节点设计: key 为 put 进去的 key, value 为节点 value. prev 前置节点, next 后续节点 Map kv设计: key 为 put 进去的 key, value 为节点 Node get 思路: 从 Map 获取 Node, 若存在则将节点放到表头，返回节点值。否则返回 -1 表示不存在 put 思路: 从 Map 获取 Node, 若不存在表示新增, 判断是否超出容量, 将新增节点放到表头 若存在表示修改, 修改 Node value 后, 将节点重新放到表头 class LRUCache { private class Node { int key, value; Node prev, next; Node (int k, int v) { key = k; value = v; } } Map\u0026lt;Integer, Node\u0026gt; map; // 通过 key 快速获取对应 Node int capacity; // 容量 Node head; // 头节点，便于操作 public LRUCache(int capacity) { this.capacity = capacity; map = new HashMap\u0026lt;\u0026gt;(capacity); head = new Node(0, 0); head.next = head; head.prev = head; } public int get(int key) { Node node = map.get(key); if (node != null) { // 移动到表头 remove(node); push(node); return node.value; } return -1; } public void put(int key, int value) { Node node = map.get(key); if (node == null) { if (map.size() == capacity) { // 达到最大容量，删除最久没使用的节点 Node remove = head.prev; remove(remove); map.remove(remove.key); } node = new Node(key, value); map.put(key, node); } else { // 修改并移动节点到表头 node.value = value; remove(node); } // 添加 node 到头节点 push(node); } // 删除一个节点 private void remove(Node node) { node.prev.next = node.next; node.next.prev = node.prev; node.next = null; node.prev = null; } // 添加节点到表头 private void push(Node node) { node.next = head.next; node.prev = head; head.next.prev = node; head.next = node; } } 19. 0-1背包、完全背包 # 在给定容量下，将物品放入背包，使价值最高\n01和完全的区别是物品能否重复选择 原始 O(n*len) # # items[i][j] 表示从下标为[0-i]的物品里任意取，放进容量为j的背包的最大价值 # 初始化第一行数(防止 i - 1 没有初始值) for j in range(items[0].weight, target + 1): mat[0][j] = items[0].value # 两个 for 交换也行，因为递推用到的都是左上角数据 for i in range(1, len(items) + 1): for j in range(1, target + 1): # 目标容量 if j \u0026lt; items[i].weight: # 容量 j 不足以放下物品 i mat[i][j] = mat[i - 1][j] else: mat[i][j] = max(mat[i - 1][j], # 不取物品 i：最大价值使用左侧的最大价值 # 取物品 i：在剩余容量取最大价值 + 物品 i 价值 mat[i - 1][j - items[i].weight] + items[i].value) 完全背包代码区别：\nmat[i][j - items[i].weight] + items[i].value # 可以多次取物品 i，可以多次使用当前物品 i 对应的状态 优化1 O(2*len) # 把第一维都加上 %2\n优化2 O(len) # 使用一列表示两列（滚动数组），从后往前遍历保证次序。完全背包则无需倒序遍历\n# dp[j] 表示容量为 j 的背包，物品价值的最大值 # 选择：1. 不放物品 i， 放物品 i # dp[j] = max(dp[j], dp[j - items[i].weigh] + items[i].value) dp = [0 for _ range(target + 1)] for i in range(len(items) + 1): # 遍历物品 for j in range(target : items[i] : -1): # 遍历背包容量 dp[j] = max(dp[j], dp[j - items[i].weigh] + items[i].value) Python # 1.堆/优先队列 # # 取反操作 gifts = list(map(lambda x: -x, gifts)) q = [-gift for gift in gifts] heapq.heapify(listA) # 堆内元素堆化--默认小顶堆 heapq.heappop(heap)\t# 出堆 heapq.heappush(heap, 18)\t# 入堆 2.Counter类 # 元素数量统计，返回一个字典，键为元素，值为元素个数，按次数高到低排序\nfrom collections import Counter list1 = [\u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;f\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;f\u0026#34;] dic = Counter(list1)\t# 次数高到低排序 dict(dic)\t# 字典序排序 sorted(dic.items(), key=lambda s: (-s[1]))\t# 次数低到高排序\t返回一个列表 most_common() # 返回一个列表，包含counter中n个最大数目的元素，有着相同数目的将会选择出现早的元素\nelements() # 返回一个迭代器，每个元素重复的次数为它的数目\nupdate() # 从一个可迭代对象（可迭代对象是一个元素序列，而非(key,value)对构成的序列）中或者另一个映射（或counter）中所有元素相加，是数目相加而非替换它们\na = Counter(list1) b = Counter(list2) a.update(b) subtract() # 从一个可迭代对象中或者另一个映射（或counter）中，元素相减，是数目相减而不是替换它们\n3.DFS # class Solution: \u0026#34;\u0026#34;\u0026#34;2316: 无法互相到达的不同点对数目 \u0026#34;\u0026#34;\u0026#34; def countPairs(self, n: int, edges: List[List[int]]) -\u0026gt; int: graph = [[] for _ in range(n)] for x, y in edges: graph[x].append(y) graph[y].append(x) visited = [False] * n def dfs(x: int) -\u0026gt; int: visited[x] = True count = 1 for y in graph[x]: if not visited[y]: count += dfs(y) return count res = 0 for i in range(n): if not visited[i]: count = dfs(i) res += count * (n - count) return res // 2 4.Collections.defaultdict 给不存在的键值对赋默认值 # defaultdict(int)：初始化为 0 defaultdict(float)：初始化为 0.0 defaultdict(str)：初始化为 \u0026quot;\u0026quot; 5.并查集 # class UnionFind: def __init__(self, n): self.parent = [i for i in range(n)] self.count = n def find(self, x): while x!=self.parent[x]: self.parent[x] = self.parent[self.parent[x]] x = self.parent[x] return x def union(self, x, y): rootx = self.find(x) rooty = self.find(y) if rootx == rooty: return self.parent[rootx] = rooty self.count -= 1 6.单调栈 # 伪代码：\nstack\u0026lt;int\u0026gt; st; //此处一般需要给数组最后添加结束标志符，具体下面例题会有详细讲解 for (遍历这个数组) { if (栈空 || 栈顶元素大于等于当前比较元素) { 入栈; } else { while (栈不为空 \u0026amp;\u0026amp; 栈顶元素小于当前元素) { 栈顶元素出栈; 更新结果; } 当前数据入栈; } } 907. 子数组的最小值之和 中等\n给定一个整数数组 arr，找到 min(b) 的总和，其中 b 的范围为 arr 的每个（连续）子数组。由于答案可能很大，因此 返回答案模 10^9 + 7 。\nclass Solution: def sumSubarrayMins(self, arr: List[int]) -\u0026gt; int: n = len(arr) # 左边界 left[i] 为左侧严格小于 arr[i] 的最近元素位置（不存在时为 -1） left, st = [-1] * n, [] for i, x in enumerate(arr): while st and arr[st[-1]] \u0026gt;= x: st.pop() # 移除无用数据 if st: left[i] = st[-1] st.append(i) # 右边界 right[i] 为右侧小于等于 arr[i] 的最近元素位置（不存在时为 n） right, st = [n] * n, [] for i in range(n - 1, -1, -1): while st and arr[st[-1]] \u0026gt; arr[i]: st.pop() # 移除无用数据 if st: right[i] = st[-1] st.append(i) ans = 0 for i, (x, l, r) in enumerate(zip(arr, left, right)): ans += x * (i - l) * (r - i) # 累加贡献 return ans % (10 ** 9 + 7) 7.差分数组 # 对于数组 $a$ ，定义其差分数组（difference array）为 $$ \\begin{eqnarray} d[i]=\\begin{cases} a[0]\u0026amp;,i=0 \\ a[i] - a[i-1] \u0026amp;,i \\neq 0 \\end{cases} \\end{eqnarray} $$\n性质 1：从左到右累加 $d$ 中的元素，可以得到数组 $a$。\n性质 2：如下两个操作是等价的。\n把 $a$的子数组 $a[i]$,$a[i+1]$,⋯ ,$a[j]$ 都加上 $x$。 把 $d[i]$ 增加 $x$，把 $d[j+1]$ 减少 $x$。 1094. 拼车\n车上最初有 capacity 个空座位。车 只能 向一个方向行驶（也就是说，不允许掉头或改变方向）给定整数 capacity 和一个数组 trips , trip[i] = [numPassengersi, fromi, toi] 表示第 i 次旅行有 numPassengersi 乘客，接他们和放他们的位置分别是 fromi 和 toi 。这些位置是从汽车的初始位置向东的公里数。当且仅当你可以在所有给定的行程中接送所有乘客时，返回 true，否则请返回 false\ndef carPooling(self, trips: List[List[int]], capacity: int) -\u0026gt; bool: d = [0] * 1001 for num, from_, to in trips: d[from_] += num d[to] -= num return all(s \u0026lt;= capacity for s in accumulate(d)) ","externalUrl":null,"permalink":"/docs/%E5%85%B6%E4%BB%96/%E7%AE%97%E6%B3%95/","section":"Docs","summary":"Java # 1.堆/优先队列 # Queue\u0026lt;Integer\u0026gt; bigHeap = new PriorityQueue\u0026lt;\u0026gt;((a, b) -\u0026gt; b - a); // 大顶堆 bigHeap.offer(num); //入堆","title":"算法","type":"docs"},{"content":" 基本概念 # 全称是有限状态自动机（Finite-state machine,FSM）。是数学模型，在编译原理中，状态机通常用于词法分析和语法分析。\n状态机的四大概念 # State ，状态。一个状态机至少要包含两个状态 Event ，事件。事件就是执行某个操作的触发条件或者口令。对于自动门，“按下开门按钮”就是一个事件 Action ，动作。事件发生以后要执行动作。“按开门按钮”事件对应动作是“开门”。一个 Action一般对应一个函数 Transition ，变换。也就是从一个状态变化为另一个状态。“开门过程”就是一个变换 记一次学习使用 # 招商状态流转自动机（仅紫色部分）： # 在SSM（Spring StateMachine）的使用中，需要定义如下类：\n状态枚举类 LeadsStatus 事件枚举类 LeadsEvent 状态机配置类 LeadsStateMachine extends StateMachineConfigurerAdapter\u0026lt;LeadsStatus,LeadsEvent\u0026gt; 事件触发作出相应动作的动作类 Action\u0026lt;States extends LeadsStatus, Event extends LeadsEvent\u0026gt; implements Action\u0026lt;States, Event\u0026gt; 状态变换发生时记录日志或其他事务的监视器类 StateListener 持久化类 LeadsStateMachinePersister implements StateMachinePersist\u0026lt;LeadsStatus, LeadsEvent, ClueCommonOperateDO\u0026gt; 持久化配置类 PersistConfig 状态枚举类 # public enum LeadsStatus { UNKNOWN(-1, \u0026#34;未知\u0026#34;), UNCLAIMED(0, \u0026#34;待认领\u0026#34;), COMMUNICATING(1, \u0026#34;洽谈中\u0026#34;), COMMUNICATE_FIELD(2, \u0026#34;洽谈失败\u0026#34;), LEADS_FAILED(3, \u0026#34;无效线索\u0026#34;), WAIT_ENTRY(4, \u0026#34;待入驻\u0026#34;), WAIT_BIDDING(5, \u0026#34;待出价\u0026#34;), BIDDING(6, \u0026#34;已出价\u0026#34;), REJECT(7, \u0026#34;驳回\u0026#34;), ; private int code; private String desc; LeadsStatus(int code, String desc) { this.code = code; this.desc = desc; } public Integer getCode() { return code; } public String getDesc() { return desc; } public static LeadsStatus obtainByCode(Integer code) { for (LeadsStatus value : LeadsStatus.values()) { if (value.getCode().equals(code)) { return value; } } return UNKNOWN; } public static String obtainDescByCode(Integer code) { for (LeadsStatus value : LeadsStatus.values()) { if (value.getCode().equals(code)) { return value.desc; } } return UNKNOWN.getDesc(); } public static Integer obtainCodeByDesc(String reason) { for (LeadsStatus value : LeadsStatus.values()) { if (value.getDesc().equals(reason)) { return value.getCode(); } } return -1; } } 事件枚举类 # public enum LeadsEvent { UNKNOWN(-1, \u0026#34;\u0026#34;), CLAIM(0, \u0026#34;认领\u0026#34;), ALLOT(1, \u0026#34;分配\u0026#34;), COMMUNICATE(2, \u0026#34;洽谈\u0026#34;), // 未使用 TRANSFER(3, \u0026#34;转移\u0026#34;), REJECT(4, \u0026#34;驳回\u0026#34;); private int code; private String desc; LeadsEvent(int code, String desc) { this.code = code; this.desc = desc; } public Integer getCode() { return code; } public String getDesc() { return desc; } public static LeadsEvent obtainByCode(Integer code) { for (LeadsEvent value : LeadsEvent.values()) { if (value.getCode().equals(code)) { return value; } } return UNKNOWN; } public static String obtainDescByCode(Integer code) { for (LeadsEvent value : LeadsEvent.values()) { if (value.getCode().equals(code)) { return value.desc; } } return UNKNOWN.getDesc(); } } 状态机配置类 # 可以通过 @Configuration 与 @EnableStateMachine 或 @EnableStateMachineFactory 进行配置\n@Configuration @EnableStateMachine public class Config1extends StateMachineConfigurerAdapter\u0026lt;String, String\u0026gt; { @Override public void configure(StateMachineStateConfigurer\u0026lt;String, String\u0026gt; states)throws Exception { states .withStates() .initial(\u0026#34;S1\u0026#34;) .state(\u0026#34;S2\u0026#34;); } @Override public void configure(StateMachineTransitionConfigurer\u0026lt;String, String\u0026gt; transitions)throws Exception { transitions .withExternal() .source(\u0026#34;S1\u0026#34;).target(\u0026#34;S2\u0026#34;).event(\u0026#34;E1\u0026#34;).guard(guard1(true)) .and() .withExternal() .source(\u0026#34;S1\u0026#34;).target(\u0026#34;S2\u0026#34;).event(\u0026#34;E2\u0026#34;).guard(guard1(false)) .and() .withExternal() .source(\u0026#34;S1\u0026#34;).target(\u0026#34;S2\u0026#34;).event(\u0026#34;E3\u0026#34;).guard(guard2(true)) .and() .withExternal() .source(\u0026#34;S1\u0026#34;).target(\u0026#34;S2\u0026#34;).event(\u0026#34;E4\u0026#34;).guard(guard2(false)); } } 使用适配器有一个限制，因为它需要通过 Spring@Configuration类和应用程序上下文进行工作。虽然这是一个非常清晰的状态机配置模型，但限制了编译时的配置。如果需要多个构建动态状态机，可以使用构建器模式来构建类似的实例。通过使用字符串作为状态和事件，您可以使用此构建器模式在 Spring 应用程序上下文之外构建完全动态的状态机。 在本次案例中，我们使用工厂模式来创建状态机：\n@Component public class LeadsStateMachineBuilder { private final static String MACHINEID = \u0026#34;leadsStateMachine\u0026#34;; @Resource private UpdateUtils updateUtils; /** * 构建状态机 * * @param beanFactory * @return * @throws Exception */ public StateMachine\u0026lt;LeadsStatus, LeadsEvent\u0026gt; build(BeanFactory beanFactory) throws Exception { StateMachineBuilder.Builder\u0026lt;LeadsStatus, LeadsEvent\u0026gt; builder = StateMachineBuilder.builder(); builder.configureConfiguration() .withConfiguration() .machineId(MACHINEID) .beanFactory(beanFactory); builder.configureStates() .withStates() .initial(LeadsStatus.UNCLAIMED) .choice(LeadsStatus.REJECT) .states(EnumSet.allOf(LeadsStatus.class)); builder.configureTransitions() // 认领事件:待认领 -\u0026gt; 洽谈中 .withExternal().source(LeadsStatus.UNCLAIMED) .target(LeadsStatus.COMMUNICATING) .event(LeadsEvent.CLAIM) // 分配事件:待认领 -\u0026gt; 洽谈中 .and() .withExternal() .source(LeadsStatus.UNCLAIMED) .target(LeadsStatus.COMMUNICATING) .event(LeadsEvent.ALLOT) // 转移事件:洽谈选择 -\u0026gt; 洽谈中 .and() .withExternal() .source(LeadsStatus.COMMUNICATING) .target(LeadsStatus.COMMUNICATING) .event(LeadsEvent.TRANSFER) // 驳回事件：洽谈中 -\u0026gt; 驳回状态 .and() .withExternal() .source(LeadsStatus.COMMUNICATING) .target(LeadsStatus.REJECT) .event(LeadsEvent.REJECT) // 驳回自动选择：驳回状态 -\u0026gt; 状态机终止状态 .and() .withChoice() .source(LeadsStatus.REJECT) .first(LeadsStatus.COMMUNICATE_FIELD, new CommunicateGuard(), new UpdateByReasonTypeAction\u0026lt;\u0026gt;(updateUtils)) .then(LeadsStatus.LEADS_FAILED, new LeadsFailedGuard(), new UpdateByReasonTypeAction\u0026lt;\u0026gt;(updateUtils)) .last(LeadsStatus.COMMUNICATING, new RejectFailedAction\u0026lt;\u0026gt;()); return builder.build(); } } builder.configureStates() .withStates() .initial(Status.INITSTATE) .choice(Status.S0) // 配置 Choice 状态 .states(EnumSet.allOf(Status.class)); \u0026gt;___________________________________________________________________________分割线\u0026lt; // Choice 状态转换 .and() .withChoice() .source(Status.S0) .first(Status.S1, new FirstGuard(), new FirstAction()) .then(Status.S2, new SecondGuard(), new SecondAction()) .last(Status.S3, new LastAction()) 这里需要注意的是，Choice 状态需要配置。\n一旦抵达配置的 Choice 状态（S0），自动机会根据配置的变换自动执行 FirstGuard ，判断为 true则执行 FirstAction 跳转到状态 S1。 否则执行 SecondGuard，判断为 true 则执行 SecondAction 跳转到状态 S2。 这里可以再添加多个 then 句式。 否则执行 LastAction 跳转到状态 S3。 与我们熟知的 if-else if-else 类似。 动作类 # @AllArgsConstructor public class UpdateByReasonTypeAction\u0026lt;States extends LeadsStatus, Event extends LeadsEvent\u0026gt; implements Action\u0026lt;States, Event\u0026gt; { private UpdateUtils updateUtils; @Override public void execute(StateContext\u0026lt;States, Event\u0026gt; context) { String reason = context.getMessage().getHeaders().get(SSMConstants.MESSAGE_KEY_REASON, String.class); updateUtils.update((Message\u0026lt;LeadsEvent\u0026gt;) context.getMessage(), LeadsStatus.obtainCodeByDesc(reason)); } } 更新工具类 # 公用的状态更新操作 这边由于数据库表设计问题，设计了冗余的状态字段，导致需要进行三表联合更新。实际情况下不应该这样去设计，这里有待优化。\n@Slf4j @Component public class UpdateUtils { @Resource private ClueCommonOperateMapper clueCommonOperateMapper; @Resource private BasicInfosV2Mapper basicInfosV2Mapper; @Resource private MerchantEnrollV2Mapper merchantEnrollV2Mapper; public void update(Message\u0026lt;LeadsEvent\u0026gt; message, Integer status) { ClueCommonOperateDO clueCommonOperateDO = message.getHeaders().get(SSMConstants.MESSAGE_KEY_DO, ClueCommonOperateDO.class); if (clueCommonOperateDO != null) { clueCommonOperateDO.setStatus(status); clueCommonOperateDO.setStatusName(LeadsStatus.obtainDescByCode(status)); clueCommonOperateMapper.updateStatus(clueCommonOperateDO); UpdateWrapper\u0026lt;BasicInfosV2DO\u0026gt; updateWrapper = new UpdateWrapper\u0026lt;\u0026gt;(); updateWrapper.eq(\u0026#34;leads_id\u0026#34;, clueCommonOperateDO.getLeadsId()) .set(\u0026#34;status\u0026#34;, status); basicInfosV2Mapper.update(null, updateWrapper); UpdateWrapper\u0026lt;MerchantEnrollV2DO\u0026gt; updateWrapper1 = new UpdateWrapper\u0026lt;\u0026gt;(); updateWrapper1.eq(\u0026#34;leads_id\u0026#34;, clueCommonOperateDO.getLeadsId()) .set(\u0026#34;status\u0026#34;, status); merchantEnrollV2Mapper.update(null, updateWrapper1); } else { log.error(SSMConstants.UN_FINED_DO); } } } 监视器类 # @Slf4j @Component @WithStateMachine(id = \u0026#34;leadsStateMachine\u0026#34;) public class LeadsStateListener { @Resource UpdateUtils updateUtils; @OnTransition(target = \u0026#34;UNCLAIMED\u0026#34;) public void create() { log.info(\u0026#34;初始化：待认领\u0026#34;); } /** * 状态转换:待认领 -\u0026gt; 洽谈中 * * @param message */ @Transactional @OnTransition(source = \u0026#34;UNCLAIMED\u0026#34;, target = \u0026#34;COMMUNICATING\u0026#34;) public void claimTransition(Message\u0026lt;LeadsEvent\u0026gt; message) { log.info(\u0026#34;监听器触发状态转换:待认领 -\u0026gt; 洽谈中\u0026#34;); event(message, message.getPayload().getDesc(), LeadsStatus.COMMUNICATING.getCode()); } /** * 状态转换:洽谈中 -\u0026gt; 洽谈中 * * @param message */ @Transactional @OnTransition(source = \u0026#34;COMMUNICATING\u0026#34;, target = \u0026#34;COMMUNICATING\u0026#34;) public void transferTransition(Message\u0026lt;LeadsEvent\u0026gt; message) { log.info(\u0026#34;监听器触发状态转换:洽谈中 -\u0026gt; 洽谈中\u0026#34;); event(message, LeadsEvent.TRANSFER.getDesc(), LeadsStatus.COMMUNICATING.getCode()); } /** * 公用转换事件——修改数据库 * * @param message 消息体 * @param eventName 事件名称 * @param status 事件代码 */ private void event(Message\u0026lt;LeadsEvent\u0026gt; message, String eventName, int status) { log.info(\u0026#34;{}，状态机反馈信息：{}\u0026#34;, eventName, message.getHeaders()); updateUtils.update(message, status); } } 这里需要注意的点就是，监视器只是监测SSM的状态转变，不论何种事件导致的相同状态转变都会被一个方法识别。 本例状态转换比较少，前期设计时将状态转换事件都使用 Listener 代替了Action。在以后的使用中不推荐。只在监视器进行日志记录，每个事件新建一个 Action 类是比较清晰、易拓展的写法。\n持久化 # @Component public class LeadsStateMachinePersister implements StateMachinePersist\u0026lt;LeadsStatus, LeadsEvent, ClueCommonOperateDO\u0026gt; { @Resource private ClueCommonOperateMapper clueCommonOperateMapper; @Override public void write(StateMachineContext\u0026lt;LeadsStatus, LeadsEvent\u0026gt; stateMachineContext, ClueCommonOperateDO clueCommonOperateDO) { // 不做持久化 } @Override public StateMachineContext\u0026lt;LeadsStatus, LeadsEvent\u0026gt; read(ClueCommonOperateDO clueCommonOperateDO) { ClueCommonOperateDO operateDO = clueCommonOperateMapper.getByLeadsId(clueCommonOperateDO.getLeadsId()); return new DefaultStateMachineContext\u0026lt;\u0026gt;( LeadsStatus.obtainByCode(operateDO.getStatus()), null, null, null, null, \u0026#34;leadsStateMachine\u0026#34;); } } @Configuration public class PersistConfig { @Resource private LeadsStateMachinePersister leadsStateMachinePersister; @Bean(name = \u0026#34;stateMachinePersister\u0026#34;) public DefaultStateMachinePersister\u0026lt;LeadsStatus, LeadsEvent, ClueCommonOperateDO\u0026gt; stateMachinePersister() { return new DefaultStateMachinePersister\u0026lt;\u0026gt;(leadsStateMachinePersister); } } 这里对写操作不做持久化，恢复状态机时直接将取得的对象状态赋给状态机。这是因为状态实际上是存于数据库中，没必要再将状态机存到内存中，取出状态可以随时取到最新的状态，这样可以使状态机每次都可以从上次的状态继续执行，无需每次都新建状态机、维持这个新建的状态机。\n使用 # @Slf4j @Service public class ClueCommonOperateServiceImpl implements ClueCommonOperateService { @Resource private ClueCommonOperateMapper clueCommonOperateMapper; @Resource private EmployeeMapper employeeMapper; /** * 认领公共招商记录 * * @param request * @return */ @Override public String claim(ClueCommonOperateClaimRequest request) { return claimEvent(request); } /** * 批量认领公共招商记录 * * @param request * @return */ @Override @Transactional public String batchClaim(ClueCommonOperateBatchClaimRequest request) { ClueCommonOperateClaimRequest single = new ClueCommonOperateClaimRequest(); BeanUtils.copyProperties(request, single); List\u0026lt;Long\u0026gt; leadsIds = request.getLeadsIds(); for (Long leadsId : leadsIds) { single.setLeadsId(leadsId); String s = claimEvent(single); if (s.matches(\u0026#34;.*失败\u0026#34;)) { // 手动回滚 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); throw new DataErrorException(SSMConstants.CLAIM_BATCH_FAILED); } } return SSMConstants.CLAIM_BATCH; } private String claimEvent(ClueCommonOperateClaimRequest request) { log.info(\u0026#34;尝试认领，{}\u0026#34;, request.getLeadsId()); Long leadsId = request.getLeadsId(); ClueCommonOperateDO clueCommonOperateDO = clueCommonOperateMapper.getByLeadsId(leadsId); if (clueCommonOperateDO != null) { clueCommonOperateDO.setPriority(request.getPriority()); if (!sendEvent(LeadsEvent.CLAIM, clueCommonOperateDO, null)) { log.error(\u0026#34;认领失败：{}\u0026#34;, request.getLeadsId()); return SSMConstants.CLAIM_FAILED; } } return SSMConstants.CLAIM; } /** * 分配公共招商记录 * * @param request * @return */ @Override public String allot(ClueCommonOperateAllotRequest request) { if (employeeMapper.selectStatusById(request.getOperatorId()) \u0026lt; 3) { return SSMConstants.ALLOT_FAILED; } return allotEvent(request); } /** * 批量分配公共招商记录 * * @param request * @return */ @Override @Transactional public String batchAllot(ClueCommonOperateBatchAllotRequest request) { if (employeeMapper.selectStatusById(request.getOperatorId()) \u0026lt; 3) { return SSMConstants.AUTH_FAILED; } ClueCommonOperateAllotRequest single = new ClueCommonOperateAllotRequest(); BeanUtils.copyProperties(request, single); List\u0026lt;Long\u0026gt; leadsIds = request.getLeadsIds(); for (Long leadsId : leadsIds) { single.setLeadsId(leadsId); String s = allotEvent(single); if (s.matches(\u0026#34;.*失败\u0026#34;)) { // 手动回滚 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); throw new DataErrorException(SSMConstants.ALLOT_BATCH_FAILED); } } return SSMConstants.ALLOT_BATCH; } private String allotEvent(ClueCommonOperateAllotRequest request) { log.info(\u0026#34;尝试分配，{}\u0026#34;, request.getLeadsId()); Long leadsId = request.getLeadsId(); ClueCommonOperateDO clueCommonOperateDO = clueCommonOperateMapper.getByLeadsId(leadsId); clueCommonOperateDO.setFlowBd(request.getBindBusinessDeveloper()); clueCommonOperateDO.setPriority(request.getPriority()); if (!sendEvent(LeadsEvent.ALLOT, clueCommonOperateDO, null)) { log.error(\u0026#34;分配失败：{}\u0026#34;, request.getLeadsId()); return SSMConstants.ALLOT_FAILED; } return SSMConstants.ALLOT; } /** * 转移招商记录 * * @param request * @return */ @Override public String transfer(ClueCommonOperateTransferRequest request) { if (employeeMapper.selectStatusById(request.getOperatorId()) \u0026lt; 3) { return SSMConstants.AUTH_FAILED; } return transferEvent(request); } /** * 批量转移招商记录 * * @param request * @return */ @Override @Transactional public String batchTransfer(ClueCommonOperateBatchTransferRequest request) { if (employeeMapper.selectStatusById(request.getOperatorId()) \u0026lt; 3) { return SSMConstants.AUTH_FAILED; } ClueCommonOperateTransferRequest single = new ClueCommonOperateTransferRequest(); BeanUtils.copyProperties(request, single); List\u0026lt;Long\u0026gt; leadsIds = request.getLeadsIds(); for (Long leadsId : leadsIds) { single.setLeadsId(leadsId); String s = transferEvent(single); if (s.matches(\u0026#34;.*失败\u0026#34;)) { // 手动回滚 TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); throw new DataErrorException(SSMConstants.TRANSFER_BATCH_FAILED); } } return SSMConstants.TRANSFER_BATCH; } private String transferEvent(ClueCommonOperateTransferRequest request) { log.info(\u0026#34;尝试转移，{}\u0026#34;, request.getLeadsId()); Long leadsId = request.getLeadsId(); ClueCommonOperateDO clueCommonOperateDO = clueCommonOperateMapper.getByLeadsId(leadsId); clueCommonOperateDO.setFlowBd(request.getTransfereeName()); if (!sendEvent(LeadsEvent.TRANSFER, clueCommonOperateDO, null)) { log.error(\u0026#34;转移失败：{}\u0026#34;, request.getLeadsId()); return SSMConstants.TRANSFER_FAILED; } return SSMConstants.TRANSFER; } /** * 驳回 * * @param request * @return */ @Override public String reject(ClueCommonOperateRejectRequest request) { log.info(\u0026#34;尝试驳回，{}\u0026#34;, request.getLeadsId()); if (employeeMapper.selectStatusById(request.getOperatorId()) \u0026lt; 3) { return SSMConstants.AUTH_FAILED; } Long leadsId = request.getLeadsId(); ClueCommonOperateDO clueCommonOperateDO = clueCommonOperateMapper.getByLeadsId(leadsId); if (!sendEvent(LeadsEvent.REJECT, clueCommonOperateDO, request.getReason())) { log.error(\u0026#34;驳回失败：{}\u0026#34;, request.getLeadsId()); return SSMConstants.REJECTED_FAILED; } boolean failed = LeadsStatus.COMMUNICATING.getCode().equals(clueCommonOperateMapper.getByLeadsId(leadsId).getStatus()); return failed ? SSMConstants.REJECTED : SSMConstants.UNKNOWN_REASON_TYPE; } @Resource private BeanFactory beanFactory; @Resource private LeadsStateMachineBuilder leadsStateMachineBuilder; @Resource(name = \u0026#34;stateMachinePersister\u0026#34;) private StateMachinePersister\u0026lt;LeadsStatus, LeadsEvent, ClueCommonOperateDO\u0026gt; stateMachineMemPersister; /** * 状态转换事件 * synchronized修饰保证这个方法是线程安全的 * * @param changeEvent 事件 * @param operateDO 操作DO * @param reason 驳回原因 * @return */ private synchronized boolean sendEvent(LeadsEvent changeEvent, ClueCommonOperateDO operateDO, String reason) { boolean result = false; StateMachine\u0026lt;LeadsStatus, LeadsEvent\u0026gt; leadsStateMachine = null; try { leadsStateMachine = leadsStateMachineBuilder.build(beanFactory); // 启动状态机 leadsStateMachine.start(); // 尝试恢复状态机状态 stateMachineMemPersister.restore(leadsStateMachine, operateDO); Message message = MessageBuilder.withPayload(changeEvent).setHeader(SSMConstants.MESSAGE_KEY_DO, operateDO) .setHeader(SSMConstants.MESSAGE_KEY_REASON, reason).build(); result = leadsStateMachine.sendEvent(message); // 持久化状态机状态 stateMachineMemPersister.persist(leadsStateMachine, operateDO); } catch (Exception e) { log.error(\u0026#34;操作失败:{}\u0026#34;, e); } finally { if (leadsStateMachine != null) { leadsStateMachine.stop(); } } return result; } } 参考文档 # Spring Statemachine - Reference Documentation\nhttps://blog.shizhuang-inc.com/article/MTIyMDU\nhttps://blog.shizhuang-inc.com/article/MTIwNzc\nhttps://segmentfault.com/u/wanyuecanzhaofei/articles\n11.状态机配置 · Statemachine\n","externalUrl":null,"permalink":"/docs/%E5%85%B6%E4%BB%96/%E7%8A%B6%E6%80%81%E6%9C%BA/","section":"Docs","summary":"基本概念 # 全称是有限状态自动机（Finite-state ma","title":"状态机","type":"docs"}]